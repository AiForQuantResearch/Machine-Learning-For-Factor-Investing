# Support vector machines


**NOTES TO USERS**:   
- notebooks are by nature sequential. Chunks at the end depend on variables defined in the snippets at the beginning: don't forget to proceed in order!   
- only the code is provided. For comments of methods & results, we refer to the book.   
- please report errors!    
- don't forget: models are not optimized and variables are probably not optimally chosen, so *performance is often disappointing*.    

**Step 0**: please make sure the dataset is in your working directory!


The first step is to make sure the appropriate packages are installed.


```{r, message = FALSE, warning = FALSE}
if(!require(tidyverse)){install.packages("tidyverse")}
if(!require(lubridate)){install.packages("lubridate")}
if(!require(e1071)){install.packages("e1071")}
```


Next, we activate the relevant packages and load the data. 


```{r, message = FALSE, warning = FALSE}
library(tidyverse)                      # Activate the data science package
library(lubridate)                      # Activate the date management package
load("data_ml.RData")                   # Load the data
data_ml <- data_ml %>% 
    filter(date > "1999-12-31",         # Keep the date with sufficient data points
           date < "2019-01-01") %>%
    arrange(stock_id, date)             # Order the data
```


We also copy/paste & aggregate some chunks from Chapter 2.


```{r, message = FALSE, warning = FALSE}
stock_ids <- levels(as.factor(data_ml$stock_id)) # A list of all stock_ids
stock_days <- data_ml %>%                        # Compute the number of data points per stock
    group_by(stock_id) %>% summarize(nb = n()) 
stock_ids_short <- stock_ids[which(stock_days$nb == max(stock_days$nb))] # Stocks with full data
returns <- data_ml %>%                           # Compute returns, in matrix format, in 3 steps:
    filter(stock_id %in% stock_ids_short) %>%    # 1. Filtering the data
    dplyr::select(date, stock_id, R1M_Usd) %>%   # 2. Keep returns along with dates & firm names
    spread(key = stock_id, value = R1M_Usd)      # 3. Put in matrix shape 
features <- colnames(data_ml[3:95]) # Keep the feature's column names (hard-coded, beware!)
features_short <- c("Div_Yld", "Eps", "Mkt_Cap_12M_Usd", "Mom_11M_Usd", 
                    "Ocf", "Pb", "Vol1Y_Usd")
data_ml <- data_ml %>% 
    group_by(date) %>%                                   # Group by date
    mutate(R1M_Usd_C = R1M_Usd > median(R1M_Usd),        # Create the categorical labels
           R12M_Usd_C = R1M_Usd > median(R12M_Usd)) %>%
    ungroup() %>%
    mutate_if(is.logical, as.factor)
separation_date <- as.Date("2014-01-15")
training_sample <- filter(data_ml, date < separation_date)
testing_sample <- filter(data_ml, date >= separation_date)
```

And also from Chapter 7 (for data formats).

```{r, message = FALSE, warning = FALSE}
train_features_xgb <- training_sample %>% 
    filter(R1M_Usd < quantile(R1M_Usd, 0.2) | 
               R1M_Usd > quantile(R1M_Usd, 0.8)) %>%            # Extreme values only!
    dplyr::select(features_short) %>% as.matrix()               # Independent variable
train_label_xgb <- training_sample %>%
    filter(R1M_Usd < quantile(R1M_Usd, 0.2) | 
               R1M_Usd > quantile(R1M_Usd, 0.8)) %>%
    dplyr::select(R1M_Usd) %>% as.matrix()                      # Dependent variable
```

The order of above operations matters: we need the categorical variables like R1M_Usd_C to be present in the training & testing samples.

In R the LIBSVM library is exploited in several packages. We use *e1071*.

In the implementation of LIBSVM, the package requires to specify the label and features separately. For this reason, we recycle the variables used for the boosted trees. Moreover, the training being slow, we perform it on a subsample of these sets (first thousand instances). 

```{r, message = FALSE, warning = FALSE}
library(e1071)
fit_svm <- svm(y = train_label_xgb[1:1000],      # Train label
               x = train_features_xgb[1:1000,],  # Training features
               type = "eps-regression",          # SVM task type (see LIBSVM documentation)
               kernel = "radial",                # SVM kernel (or: linear, polynomial, sigmoid)
               epsilon = 0.1,                    # Width of strip for errors
               gamma = 0.5,                      # Constant in the radial kernel 
               cost = 0.1)                       # Slack variable penalisation
test_feat_short <- dplyr::select(testing_sample, features_short)
mean((predict(fit_svm, test_feat_short) - testing_sample$R1M_Usd)^2) # MSE
mean(predict(fit_svm, test_feat_short) * testing_sample$R1M_Usd > 0) # Hit ratio
```

The results are slightly better than those of the boosted trees. All parameters are completely arbitrary, especially the choice of the kernel. We finally turn to a classification example.

```{r, message = FALSE, warning = FALSE}
fit_svm_C <- svm(y = training_sample$R1M_Usd_C[1:1000],   # Train label
               x = training_sample[1:1000,] %>%
                   dplyr::select(features),               # Training features
               type = "C-classification",                 # SVM task type (see LIBSVM doc.)
               kernel = "sigmoid",                        # SVM kernel
               gamma = 0.5,                               # Parameter in the sigmoid kernel 
               coef0 = 0.3,                               # Parameter in the sigmoid kernel 
               cost = 0.2)                                # Slack variable penalisation
mean(predict(fit_svm_C, 
             dplyr::select(testing_sample,features)) == testing_sample$R1M_Usd_C) # Accuracy
```

Both the small training sample and the arbitrariness in our choice of the parameters may explain why the predictive accuracy is so poor. 


## Coding exercises


1. From the simple example shown above, extend SVM models to other kernels and discuss the impact on the fit.   
2. Train a  vanilla SVM model with labels being the 12 month forward (i.e., future) return and evaluate it on the testing sample. Do the same with a simple random forest. Compare. 

