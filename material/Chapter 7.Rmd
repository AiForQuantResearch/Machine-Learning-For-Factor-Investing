# Tree-based methods 


**NOTES TO USERS**:   
- notebooks are by nature sequential. Chunks at the end depend on variables defined in the snippets at the beginning: don't forget to proceed in order!   
- only the code is provided. For comments of methods & results, we refer to the book.   
- please report errors!    
- don't forget: models are not optimized and variables are probably not optimally chosen, so *performance is often disappointing*.    

**Step 0**: please make sure the dataset is in your working directory!


The first step is to make sure the appropriate packages are installed.


```{r, message = FALSE, warning = FALSE}
if(!require(tidyverse)){install.packages("tidyverse")}
if(!require(lubridate)){install.packages("lubridate")}
if(!require(rpart)){install.packages(c("rpart","rpart.plot"))}
if(!require(randomForest)){install.packages("randomForest")}
if(!require(adabag)){install.packages("adabag")}
if(!require(xgboost)){install.packages("xgboost")}
```


Next, we activate the relevant packages and load the data. 


```{r, message = FALSE, warning = FALSE}
library(tidyverse)                      # Activate the data science package
library(lubridate)                      # Activate the date management package
library(rpart)                          # Package for the trees 
library(rpart.plot)                     # Package for tree plots
library(randomForest)                   # Package for random forests
library(adabag)                         # Package for Adaboost
library(xgboost)                        # Package for boosted trees
load("data_ml.RData")                   # Load the data
data_ml <- data_ml %>% 
    filter(date > "1999-12-31",         # Keep the date with sufficient data points
           date < "2019-01-01") %>%
    arrange(stock_id, date)             # Order the data
```


We also copy/paste & aggregate some chunks from Chapter 2.


```{r, message = FALSE, warning = FALSE}
stock_ids <- levels(as.factor(data_ml$stock_id)) # A list of all stock_ids
stock_days <- data_ml %>%                        # Compute the number of data points per stock
    group_by(stock_id) %>% summarize(nb = n()) 
stock_ids_short <- stock_ids[which(stock_days$nb == max(stock_days$nb))] # Stocks with full data
returns <- data_ml %>%                           # Compute returns, in matrix format, in 3 steps:
    filter(stock_id %in% stock_ids_short) %>%    # 1. Filtering the data
    dplyr::select(date, stock_id, R1M_Usd) %>%   # 2. Keep returns along with dates & firm names
    spread(key = stock_id, value = R1M_Usd)      # 3. Put in matrix shape 
features <- colnames(data_ml[3:95]) # Keep the feature's column names (hard-coded, beware!)
features_short <- c("Div_Yld", "Eps", "Mkt_Cap_12M_Usd", "Mom_11M_Usd", 
                    "Ocf", "Pb", "Vol1Y_Usd")
data_ml <- data_ml %>% 
    group_by(date) %>%                                   # Group by date
    mutate(R1M_Usd_C = R1M_Usd > median(R1M_Usd),        # Create the categorical labels
           R12M_Usd_C = R1M_Usd > median(R12M_Usd)) %>%
    ungroup() %>%
    mutate_if(is.logical, as.factor)
separation_date <- as.Date("2014-01-15")
training_sample <- filter(data_ml, date < separation_date)
testing_sample <- filter(data_ml, date >= separation_date)
```

The order of above operations matters: we need the categorical variables like R1M_Usd_C to be present in the training & testing samples.

A first tree.

```{r, message = FALSE, warning = FALSE}
formula <- paste("R1M_Usd ~", paste(features, collapse = " + ")) # Defines the model 
formula <- as.formula(formula)                                   # Forcing formula object
fit_tree <- rpart(formula,
             data = data_ml,     # Data source: full sample
             minbucket = 3500,   # Min nb of obs required in each terminal node (leaf)
             minsplit = 8000,    # Min nb of obs required to continue splitting
             cp = 0.0001,        # Precision: smaller = more leaves
             maxdepth = 3        # Maximum depth (i.e. tree levels)
             ) 
rpart.plot(fit_tree)             # Plot the tree
```


Then: related predictions.


```{r, message = FALSE, warning = FALSE}
predict(fit_tree, data_ml[1:6,]) # Test (prediction) on the first six instances of the sample
```


As a verification of the first splits, we plot the smoothed average of future returns, conditionally on market capitalization, past return and trading volume.


```{r, message = FALSE, warning = FALSE}
data_ml %>% ggplot() +
    stat_smooth(aes(x = Mkt_Cap_3M_Usd, y = R1M_Usd, color = "Market Cap"), se = FALSE) +
    stat_smooth(aes(x = Pb, y = R1M_Usd, color = "Price-to-Book"), se = FALSE) +
    stat_smooth(aes(x = Advt_3M_Usd, y = R1M_Usd, color = "Volume"), se = FALSE) +
    xlab("Predictor") + labs(color = "Characteristic")
```


Finally, we assess the predictive quality of a single tree on the testing set (the tree is grown on the training set). We use a deeper tree, with a maximum depth of five.


```{r, message = FALSE, warning = FALSE}
fit_tree2 <- rpart(formula, 
             data = training_sample,     # Data source: training sample
             minbucket = 1500,           # Min nb of obs required in each terminal node (leaf)
             minsplit = 4000,            # Min nb of obs required to continue splitting
             cp = 0.0001,                # Precision: smaller cp = more leaves
             maxdepth = 5                # Maximum depth (i.e. tree levels)
             ) 
mean((predict(fit_tree2, testing_sample) - testing_sample$R1M_Usd)^2) # MSE
mean(predict(fit_tree2, testing_sample) * testing_sample$R1M_Usd > 0) # Hit ratio
```


Next, we turn to random forests.

The syntax of randomForest follows that of many ML libraries. The full list of options for some random forest implementations is prohibitively large.^[See, e.g., http://docs.h2o.ai/h2o/latest-stable/h2o-r/docs/reference/h2o.randomForest.html] Below, we train a model and exhibit the predictions for the first 5 instances of the testing sample.   

```{r, message = FALSE, warning = FALSE}
library(randomForest) 
set.seed(42)                                # Sets the random seed
fit_RF <- randomForest(formula,             # Same formula as for simple trees!
                 data = training_sample,    # Data source: training sample
                 sampsize = 10000,          # Size of (random) sample for each tree
                 replace = FALSE,           # Is the sampling done with replacement?
                 nodesize = 250,            # Minimum size of terminal cluster
                 ntree = 40,                # Nb of random trees
                 mtry = 30                  # Nb of predictive variables for each tree
    )
predict(fit_RF, testing_sample[1:5,])       # Prediction over the first 5 test instances 
```


We can assess the accuracy of the model.


```{r, message = FALSE, warning = FALSE}
mean((predict(fit_RF, testing_sample) - testing_sample$R1M_Usd)^2) # MSE
mean(predict(fit_RF, testing_sample) * testing_sample$R1M_Usd > 0) # Hit ratio
```

The MSE is smaller than 4% and the hit ratio is close to 54%, which is reasonably above both 50% and 52% thresholds.

Let's see if we can improve the hit ratio by resorting to a classification exercise. We start by training the model on a new formula (the label is R1M_Usd_C).


```{r, message = FALSE, warning = FALSE}
formula_C <- paste("R1M_Usd_C ~", paste(features, collapse = " + ")) # Defines the model 
formula_C <- as.formula(formula_C)                                   # Forcing formula object
fit_RF_C <- randomForest(formula_C,         # New formula! 
                 data = training_sample,    # Data source: training sample
                 sampsize = 20000,          # Size of (random) sample for each tree
                 replace = FALSE,           # Is the sampling done with replacement?
                 nodesize = 250,            # Minimum size of terminal cluster
                 ntree = 40,                # Number of random trees
                 mtry = 30                  # Number of predictive variables for each tree 
    )
```


We can then assess the proportion of correct (binary) guesses.


```{r, message = FALSE, warning = FALSE}
mean(predict(fit_RF_C, testing_sample) == testing_sample$R1M_Usd_C) # Hit ratio
```


The accuracy is disappointing. 


Below, move to the original adaboost classifier. As such, we work with the R1M_Usd_C variable and change the model formula. The computational cost of adaboost is high on large datasets, thus we work with a smaller sample and we only impose three iterations.


```{r, message = FALSE, warning = FALSE}
library(fastAdaboost)                                                     # Adaboost package 
subsample <- (1:52000)*4                                                  # Target small sample
fit_adaboost_C <- adaboost(formula_C,                                     # Model spec.
                         data = data.frame(training_sample[subsample,]),  # Data source
                         nIter = 3)                                       # Number of trees              
```


Finally, we evaluate the performance of the classifier.


```{r, message = FALSE, warning = FALSE, cache = TRUE}
mean(testing_sample$R1M_Usd_C == predict(fit_adaboost_C, testing_sample)$class)
```


Next, we move to boosted trees.

In this section, we train a model using the *XGBoost* library. Other options include *catboost*, *gbm*, *lightgbm*, and *h2o*'s own version of boosted machines. Unlike many other packages, the XGBoost function requires a particular syntax and dedicated formats. The first step is thus to encapsulate the data accordingly.

Moreover, because training times can be long, we shorten the training sample: we retain only the 40% most extreme observations (in terms of label values: top 20% and bottom 20%) and work with the small subset of features. 


```{r xgbdata, message = FALSE, warning = FALSE, size = "footnotesize"}
train_features_xgb <- training_sample %>% 
    filter(R1M_Usd < quantile(R1M_Usd, 0.2) | 
               R1M_Usd > quantile(R1M_Usd, 0.8)) %>%            # Extreme values only!
    dplyr::select(features_short) %>% as.matrix()               # Independent variable
train_label_xgb <- training_sample %>%
    filter(R1M_Usd < quantile(R1M_Usd, 0.2) | 
               R1M_Usd > quantile(R1M_Usd, 0.8)) %>%
    dplyr::select(R1M_Usd) %>% as.matrix()                      # Dependent variable
train_matrix_xgb <- xgb.DMatrix(data = train_features_xgb, 
                                label = train_label_xgb)        # XGB format!
```


The second (optional) step is to determine the monotonicity constraints that we want to impose. For simplicity, we will only enforce three constraints on    

1. market capitalization (negative, because large firms have smaller returns under the size anomaly);   
2. price-to-book ratio (negative, because overvalued firms also have smaller returns under the value anomaly);   
3. past annual returns (positive, because winners outperform losers under the momentum anomaly).  


```{r, xgbmono, message = FALSE, warning = FALSE, size = "footnotesize"}
mono_const <- rep(0, length(features))                   # Initialize the vector
mono_const[which(features == "Mkt_Cap_12M_Usd")] <- (-1) # Decreasing in market cap
mono_const[which(features == "Pb")] <- (-1)              # Decreasing in price-to-book
mono_const[which(features == "Mom_11M_Usd")] <- 1        # Increasing in past return
```


The third step is to train the model on the formatted training data. 


```{r xgbtrain, message = FALSE, warning = FALSE, size = "footnotesize"}
fit_xgb <- xgb.train(data = train_matrix_xgb,     # Data source 
              eta = 0.3,                          # Learning rate
              objective = "reg:linear",           # Objective function
              max_depth = 4,                      # Maximum depth of trees
              lambda = 1,                         # Penalisation of leaf values
              gamma = 0.1,                        # Penalisation of number of leaves
              nrounds = 30,                       # Number of trees used (rather low here)
              monotone_constraints = mono_const,  # Monotonicity constraints
              verbose = 0                         # No comment from the algo 
    )
```

Finally, we evaluate the performance of the model. Note that before that, a proper formatting of the testing sample is required.

```{r xgbperf, message = FALSE, warning = FALSE, size = "footnotesize"}
xgb_test <- testing_sample %>%                                # Test sample => XGB format
    dplyr::select(features_short) %>% 
    as.matrix() 
mean((predict(fit_xgb, xgb_test) - testing_sample$R1M_Usd)^2) # MSE
mean(predict(fit_xgb, xgb_test) * testing_sample$R1M_Usd > 0) # Hit ratio
```

The performance is comparable to those observed for other predictive tools. As a final exercise, we show one implementation of a classification task under XGBoost. Only the label changes. In XGBoost, labels must be coded with integer number, starting at zero exactly. In R, factors are numerically coded as integers numbers starting from one, hence the mapping is simple.

```{r xgbdataC, message = FALSE, warning = FALSE, size = "footnotesize"}
train_label_C <- training_sample %>% 
    filter(R1M_Usd < quantile(R1M_Usd, 0.2) |          # Either low 20% returns 
               R1M_Usd > quantile(R1M_Usd, 0.8)) %>%   # Or top 20% returns
    dplyr::select(R1M_Usd_C)
train_matrix_C <- xgb.DMatrix(data = train_features_xgb, 
                              label = as.numeric(train_label_C == "TRUE")) # XGB format!
```

When working with categories, the loss function is usually the softmax function.

```{r xgbtrainC, message = FALSE, warning = FALSE, size = "footnotesize"}
fit_xgb_C <-  xgb.train(data = train_matrix_C,  # Data source (pipe input)
              eta = 0.8,                        # Learning rate
              objective = "multi:softmax",      # Objective function
              num_class = 2,                    # Number of classes
              max_depth = 4,                    # Maximum depth of trees
              nrounds = 10,                     # Number of trees used
              verbose = 0                       # No warning message 
    )
```

We can then proceed to the assessment of the quality of the model. We adjust the prediction to the value of the true label and count the proportion of accurate forecasts.

```{r xgbperfC, message = FALSE, warning = FALSE, size = "footnotesize"}
mean(predict(fit_xgb_C, xgb_test) + 1 == as.numeric(testing_sample$R1M_Usd_C)) # Hit ratio
```

Finally, one last option is to resort to instance weighting.

```{r instweights, message = FALSE, warning = FALSE, size = "footnotesize", eval = FALSE}
inst_weights <- runif(nrow(train_features_xgb))               # Random weights
inst_weights <- inst_weights / sum(inst_weights)              # Normalization
train_matrix_xgb <- xgb.DMatrix(data = train_features_xgb, 
                                label = train_label_xgb,
                                weight = inst_weights)        # Weights!
```

