# Validation and tuning

**NOTES TO USERS**:   
- notebooks are by nature sequential. Chunks at the end depend on variables defined in the snippets at the beginning: don't forget to proceed in order!   
- only the code is provided. For comments of methods & results, we refer to the book.   
- please report errors!    
- don't forget: models are not optimized and variables are probably not optimally chosen, so *performance is often disappointing*.    

**Step 0**: please make sure the dataset is in your working directory!


The first step is to make sure the required packages are installed.


```{r, message = FALSE, warning = FALSE}
if(!require(tidyverse)){install.packages("tidyverse")}
if(!require(lubridate)){install.packages("lubridate")}
if(!require(caTools)){install.packages("caTools")}
if(!require(randomForest)){install.packages("randomForest")}
if(!require(rpart)){install.packages(c("rpart","rpart.plot"))}
if(!require(glmnet)){install.packages("glmnet")}
if(!require(xgboost)){install.packages("xgboost")}
if(!require(rBayesianOptimization)){install.packages("rBayesianOptimization")}
if(!require(ggpubr)){install.packages("ggpubr")}
```


Next, we activate the relevant packages and load the data. 


```{r, message = FALSE, warning = FALSE}
library(tidyverse)                      # Activate the data science package
library(lubridate)                      # Activate the date management package
load("data_ml.RData")                   # Load the data
data_ml <- data_ml %>% 
    filter(date > "1999-12-31",         # Keep the date with sufficient data points
           date < "2019-01-01") %>%
    arrange(stock_id, date)             # Order the data
```


We also copy/paste & aggregate some chunks from Chapter 2.


```{r, message = FALSE, warning = FALSE}
stock_ids <- levels(as.factor(data_ml$stock_id)) # A list of all stock_ids
stock_days <- data_ml %>%                        # Compute the number of data points per stock
    group_by(stock_id) %>% summarize(nb = n()) 
stock_ids_short <- stock_ids[which(stock_days$nb == max(stock_days$nb))] # Stocks with full data
returns <- data_ml %>%                           # Compute returns, in matrix format, in 3 steps:
    filter(stock_id %in% stock_ids_short) %>%    # 1. Filtering the data
    dplyr::select(date, stock_id, R1M_Usd) %>%   # 2. Keep returns along with dates & firm names
    spread(key = stock_id, value = R1M_Usd)      # 3. Put in matrix shape 
features <- colnames(data_ml[3:95]) # Keep the feature's column names (hard-coded, beware!)
features_short <- c("Div_Yld", "Eps", "Mkt_Cap_12M_Usd", "Mom_11M_Usd", 
                    "Ocf", "Pb", "Vol1Y_Usd")
data_ml <- data_ml %>% 
    group_by(date) %>%                                   # Group by date
    mutate(R1M_Usd_C = R1M_Usd > median(R1M_Usd),        # Create the categorical labels
           R12M_Usd_C = R1M_Usd > median(R12M_Usd)) %>%
    ungroup() %>%
    mutate_if(is.logical, as.factor)
separation_date <- as.Date("2014-01-15")
training_sample <- filter(data_ml, date < separation_date)
testing_sample <- filter(data_ml, date >= separation_date)
```


Below, we use a particular package (*caTools*) to compute a ROC curve for a given set of predictions on the testing sample. 
The curve is of course computed with respect to a dataset & a predictive model. We use a random forest classifier.

```{r, message = FALSE, warning = FALSE}
library(caTools)         # Package for AUC computation
library(randomForest)    # Package for RF classifier
formula_C <- paste("R1M_Usd_C ~", paste(features, collapse = " + ")) # Defines the model 
formula_C <- as.formula(formula_C)                                   # Forcing formula object
fit_RF_C <- randomForest(formula_C,         # Model formula
                 data = training_sample,    # Data source: training sample
                 sampsize = 20000,          # Size of (random) sample for each tree
                 replace = FALSE,           # Is the sampling done with replacement?
                 nodesize = 250,            # Minimum size of terminal cluster
                 ntree = 40,                # Number of random trees
                 mtry = 30                  # Number of predictive variables for each tree 
    )
colAUC(X = predict(fit_RF_C, testing_sample, type = "prob"), 
       y = testing_sample$R1M_Usd_C, 
       plotROC = TRUE)
```




We illustrate the variance-bias tradeoff with the ridge regression. In the example below we recycle the ridge model trained in the chapter on penalized regressions.

```{r, message = FALSE, warning = FALSE}
library(glmnet)
y_penalized <- data_ml$R1M_Usd                              # Dependent variable
x_penalized <- data_ml %>%                                  # Predictors
    dplyr::select(features) %>% as.matrix() 
fit_ridge <- glmnet(x_penalized, y_penalized, alpha = 0)    # alpha = 0: ridge
lambda <- fit_ridge$lambda                                  # Vector of penalisation const
x_penalized_test <- testing_sample %>%                      # Predictors (test set)
    dplyr::select(features) %>% as.matrix()        

ridge_errors <- predict(fit_ridge, x_penalized_test) -                       # Errors from all models
    (rep(testing_sample$R1M_Usd, 100) %>% 
    matrix(ncol = 100, byrow = FALSE))
ridge_bias <- ridge_errors %>% apply(2, mean)                                # Biases
ridge_var <- predict(fit_ridge, x_penalized_test) %>% apply(2, var)          # Variance
tibble(lambda, ridge_bias^2, ridge_var, total = ridge_bias^2+ridge_var) %>%  # Plot
    gather(key = Error_Component, value = Value, -lambda) %>%
    ggplot(aes(x = lambda, y = Value, color = Error_Component)) + geom_line()

```


To further illustrate the variance-bias tradeoff, we build 2 trees: one small and one deep.

```{r, message = FALSE, warning = FALSE}
library(rpart)
library(rpart.plot)
formula <- paste("R1M_Usd ~", paste(features, collapse = " + ")) # Defines the model 
formula <- as.formula(formula)                                   # Forcing formula object
fit_tree_simple <- rpart(formula, 
             data = training_sample,     # Data source: training sample
             cp = 0.0001,                # Precision: smaller = more leaves
             maxdepth = 2                # Maximum depth (i.e. tree levels)
             ) 
rpart.plot(fit_tree_simple)
```

The model only has 4 clusters, which means that the predictions can only take four values. The model is simple!

```{r, message = FALSE, warning = FALSE}
mean(predict(fit_tree_simple, testing_sample) - testing_sample$R1M_Usd) # Bias
var(predict(fit_tree_simple, testing_sample))                           # Variance
```

On average, the error is slightly positive, with an overall overestimation of `r round(mean(predict(fit_tree_simple, testing_sample) - testing_sample$R1M_Usd),4)` . As expected, the variance is very small (`r round(var(predict(fit_tree_simple, testing_sample)),4)`).

For the complex model, we take the boosted tree that was obtained in chapter on trees. The model aggregates 40 trees with a maximum depth of 4, it is thus undoubtedly more complex. The amount of code that generates the model is larger compared to other models notably because of variable pre-processing.

```{r, message = FALSE, warning = FALSE}
library(xgboost)
train_features_xgb <- training_sample %>% 
    filter(R1M_Usd < quantile(R1M_Usd, 0.2) | 
               R1M_Usd > quantile(R1M_Usd, 0.8)) %>%            # Extreme values only!
    dplyr::select(features_short) %>% as.matrix()               # Independent variable
train_label_xgb <- training_sample %>%
    filter(R1M_Usd < quantile(R1M_Usd, 0.2) | 
               R1M_Usd > quantile(R1M_Usd, 0.8)) %>%
    dplyr::select(R1M_Usd) %>% as.matrix()                      # Dependent variable
train_matrix_xgb <- xgb.DMatrix(data = train_features_xgb, 
                                label = train_label_xgb)        # XGB format!
mono_const <- rep(0, length(features))                   # Initialize the vector
mono_const[which(features == "Mkt_Cap_12M_Usd")] <- (-1) # Decreasing in market cap
mono_const[which(features == "Pb")] <- (-1)              # Decreasing in price-to-book
mono_const[which(features == "Mom_11M_Usd")] <- 1        # Increasing in past return
fit_xgb <- xgb.train(data = train_matrix_xgb,     # Data source 
              eta = 0.3,                          # Learning rate
              objective = "reg:linear",           # Objective function
              max_depth = 4,                      # Maximum depth of trees
              lambda = 1,                         # Penalisation of leaf values
              gamma = 0.1,                        # Penalisation of number of leaves
              nrounds = 30,                       # Number of trees used (rather low here)
              monotone_constraints = mono_const,  # Monotonicity constraints
              verbose = 0                         # No comment from the algo 
    )
xgb_test <- testing_sample %>%                                # Test sample => XGB format
    dplyr::select(features_short) %>% 
    as.matrix() 

mean(predict(fit_xgb, xgb_test) - testing_sample$R1M_Usd) # Bias
var(predict(fit_xgb, xgb_test))                           # Variance
```

The bias is indeed smaller compared to that of the simple model, but in exchange, the variance increases sustantially. 


Below, we move towards grid search in hyper-parameter optimization.

```{r, message = FALSE, warning = FALSE,}
eta <- c(0.1, 0.3, 0.5, 0.7, 0.9)         # Values for eta
nrounds <- c(10, 50, 100)                 # Values for nrounds
lambda <- c(0.01, 0.1, 1, 10, 100)        # Values for lambda
pars <- expand.grid(eta, nrounds, lambda) # Exploring all combinations!
head(pars)                                # Let's see the parameters
eta <- pars[,1]
nrounds <- pars[,2]
lambda <- pars[,3]
```

Given the computational cost of grid search, we perform the exploration on the dataset with the small number of features (which we recycle from the chapter on trees). In order to avoid the burden of loops, we resort to the functional programming capabilities of R, via the *purrr* package. This allows us to define a function that will lighten and simplify the code. This function, coded below, takes data and parameter inputs and returns an error metric for the algorithm. We choose the mean squared error to evaluate the impact of hyperparameter values.


```{r, message = FALSE, warning = FALSE}
grid_par <- function(train_matrix, test_features, test_label, eta, nrounds, lambda){
    fit <- train_matrix %>% 
        xgb.train(data = .,                       # Data source (pipe input)
                  eta = eta,                      # Learning rate
                  objective = "reg:linear",       # Objective function
                  max_depth = 5,                  # Maximum depth of trees
                  lambda = lambda,                # Penalisation of leaf values
                  gamma = 0.1,                    # Penalisation of number of leaves
                  nrounds = nrounds,              # Number of trees used
                  verbose = 0                     # No comment from algo
        )
    
    pred <- predict(fit, test_features)           # Preditions based on model & test values
    return(mean((pred-test_label)^2))             # Mean squared error
}
```

The grid_par function can then be processed by the functional programming tool **pmap** that is going to perform the loop on parameter values automatically. 

```{r, message = FALSE, warning = FALSE}
# grid_par(train_matrix_xgb, xgb_test, testing_sample$R1M_Usd, 0.1, 3, 0.1) # Possible test 
grd <- pmap(list(eta, nrounds, lambda),             # Parameters for the grid search
            grid_par,                               # Function on which to apply the search
            train_matrix = train_matrix_xgb,        # Input for function: training data
            test_features = xgb_test,               # Input for function: test features
            test_label = testing_sample$R1M_Usd     # Input for function: test labels (returns) 
)
grd <- data.frame(eta, nrounds, lambda, error = unlist(grd)) # Dataframe with all results
```

Once the squared mean errors have been gathered, it is possible to plot them. We chose to work with 3 parameters on purpose because their influence can be simultaneuously plotted on one graph. 

```{r, message = FALSE, warning = FALSE}
grd$eta <- as.factor(eta)                                  # Params as categories (for plot)
grd %>% ggplot(aes(x = eta, y = error, fill = eta)) +      # Plot!
    geom_bar(stat = "identity") +
    facet_grid(rows = vars(nrounds), cols = vars(lambda)) +
    theme(axis.text.x = element_text(size = 6))
```



There are several packages in R that relate to Bayesian optimization. We work with *rBayesianOptimization*, which is general purpose but also needs more coding involvment. 


Just as for the grid search, we need to code the objective function on which the hyperparameters will be optimized. Under *rBayesianOptimization*, the output has to have a particular form, with a score and a prediction variable. The function will *maximize* the score, hence we will define it as *minus* the mean squared error. 

```{r, message = FALSE, warning = FALSE}
bayes_par_opt <- function(train_matrix = train_matrix_xgb,        # Input for func: train data
            test_features = xgb_test,                             # Input for func: test features
            test_label = testing_sample$R1M_Usd,                  # Input for func: test label
            eta, nrounds, lambda){                                # Input for func params
    fit <- train_matrix %>% 
        xgb.train(data = .,                       # Data source (pipe input)
                  eta = eta,                      # Learning rate
                  objective = "reg:linear",       # Objective function
                  max_depth = 5,                  # Maximum depth of trees
                  lambda = lambda,                # Penalisation of leaf values
                  gamma = 0.1,                    # Penalisation of number of leaves
                  nrounds = round(nrounds),       # Number of trees used
                  verbose = 0                     # No comment from algo
        )

    pred <- predict(fit, test_features)           # Forecast based on fitted model & test values
    list(Score = -mean((pred-test_label)^2),      # Minus RMSE
         Pred = pred)                             # Predictions on test set
}
```

Once the objective function is defined, it can be plugged into the Bayesian optimizer.

```{r, message = FALSE, warning = FALSE}
library(rBayesianOptimization)
bayes_opt <- BayesianOptimization(bayes_par_opt,           # Function to maximize
                     bounds = list(eta = c(0.2, 0.8),      # Bounds for eta
                                   lambda = c(0.5, 15),    # Bounds for lambda
                                   nrounds = c(10, 100)),  # Bounds for nrounds
                     init_points = 10,            # Nb initial points for first estimation
                     n_iter = 24,                 # Nb optimization steps/trials
                     acq = "ei",                  # Acquisition function = expected improvement
                     verbose = FALSE)
bayes_opt$Best_Par
```

Finally, we plot the relationship between the loss (up to the sign) and two hyperparameters. Each point corresponds to a value tested in the optimization. The best values are clearly to the left of the left graph and to the right of the right graph and the pattern is reliably pronounced. 

```{r, message = FALSE, warning = FALSE}
library("ggpubr") # Package for combining plots
plot_rounds <- bayes_opt$History %>% 
    ggplot(aes(x = nrounds, y = Value)) + geom_point() + geom_smooth(method = "lm")
plot_lambda <- bayes_opt$History %>% 
    ggplot(aes(x = lambda, y = Value)) + geom_point() + geom_smooth(method = "lm")
par(mar = c(1,1,1,1))
ggarrange(plot_rounds, plot_lambda, ncol = 2)
```

