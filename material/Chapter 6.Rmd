# Penalized regressions and sparse hedging for minimum variance portfolios 

**NOTES TO USERS**:   
- notebooks are by nature sequential. Chunks at the end depend on variables defined in the snippets at the beginning: don't forget to proceed in order!   
- only the code is provided. For comments of methods & results, we refer to the book.  
- please report errors!   

**Step 0**: please make sure the dataset is in your working directory!


The first step is to make sure the appropriate packages are installed.


```{r, message = FALSE, warning = FALSE}
if(!require(tidyverse)){install.packages("tidyverse")}
if(!require(lubridate)){install.packages("lubridate")}
if(!require(glmnet)){install.packages("glmnet")}
```


Next, we activate the relevant packages and load the data. 


```{r, message = FALSE, warning = FALSE}
library(tidyverse)                      # Activate the data science package
library(lubridate)                      # Activate the date management package
library(glmnet)                         # Package for penalized regressions
load("data_ml.RData")                   # Load the data
data_ml <- data_ml %>% 
    filter(date > "1999-12-31",         # Keep the date with sufficient data points
           date < "2019-01-01") %>%
    arrange(stock_id, date)             # Order the data
```


We also copy/paste some chunks from Chapter 2.


```{r, message = FALSE, warning = FALSE}
stock_ids <- levels(as.factor(data_ml$stock_id)) # A list of all stock_ids
stock_days <- data_ml %>%                        # Compute the number of data points per stock
    group_by(stock_id) %>% summarize(nb = n()) 
stock_ids_short <- stock_ids[which(stock_days$nb == max(stock_days$nb))] # Stocks with full data
returns <- data_ml %>%                           # Compute returns, in matrix format, in 3 steps:
    filter(stock_id %in% stock_ids_short) %>%    # 1. Filtering the data
    dplyr::select(date, stock_id, R1M_Usd) %>%   # 2. Keep returns along with dates & firm names
    spread(key = stock_id, value = R1M_Usd)      # 3. Put in matrix shape 
features <- colnames(data_ml[3:95]) # Keep the feature's column names (hard-coded, beware!)
features_short <- c("Div_Yld", "Eps", "Mkt_Cap_12M_Usd", "Mom_11M_Usd", 
                    "Ocf", "Pb", "Vol1Y_Usd")
separation_date <- as.Date("2014-01-15")
training_sample <- filter(data_ml, date < separation_date)
testing_sample <- filter(data_ml, date >= separation_date)
```


We start with the LASSO. The syntax is slightly different, compared to usual linear models. The illustrations are run on the whole dataset. First, we estimate the coefficients. By default, the function chooses a large array of penalization values so that the results for different penalization intensities ($\lambda$) can be shown immediately.


```{r, message = FALSE, warning = FALSE}
y_penalized <- data_ml$R1M_Usd                              # Dependent variable
x_penalized <- data_ml %>%                                  # Predictors
    dplyr::select(features) %>% as.matrix() 
fit_lasso <- glmnet(x_penalized, y_penalized, alpha = 1)    # Model alpha = 1: LASSO
```

Once the coefficients are computed, they require some wrangling before plotting. Also, there are too many of them, so we only plot a subset of them.

```{r, message = FALSE, warning = FALSE}
lasso_res <- summary(fit_lasso$beta)                        # Extract LASSO coefs
lambda <- fit_lasso$lambda                                  # Values of the penalisation const
lasso_res$Lambda <- lambda[lasso_res$j]                     # Put the labels where they belong
lasso_res$Feature <- features[lasso_res$i] %>% as.factor()  # Add names of variables to output
lasso_res[1:120,] %>%                                       # Take the first 120 estimates
    ggplot(aes(x = Lambda, y = x, color = Feature)) +       # Plot!
    geom_line() + coord_fixed(0.25) + ylab("beta") +        # Change aspect ratio of graph
    theme(legend.text = element_text(size = 7))             # Reduce legend font size
```


The graph plots the evolution of coefficients as the penalization intensity, $\lambda$, increases. 

Next, we turn to ridge regressions. 


```{r, message = FALSE, warning = FALSE}
fit_ridge <- glmnet(x_penalized, y_penalized, alpha = 0)                  # alpha = 0: ridge
ridge_res <- summary(fit_ridge$beta)                                      # Extract ridge coefs
lambda <- fit_ridge$lambda                                                # Penalisation const
ridge_res$Feature <- features[ridge_res$i] %>% as.factor()
ridge_res$Lambda <- lambda[ridge_res$j]                                   # Set labels right
ridge_res %>% 
    filter(Feature %in% levels(droplevels(lasso_res$Feature[1:120]))) %>% # Keep same features 
    ggplot(aes(x = Lambda, y = x, color = Feature)) + ylab("beta") +      # Plot!
    geom_line() + scale_x_log10() + coord_fixed(45) +                     # Aspect ratio 
    theme(legend.text = element_text(size = 7))
```

The convergence to zero is much smoother. We underline that the x-axis (penalization intensities) have a log-scale. This allows to see the early patterns (close to zero, to the left) more clearly. 

By definition, the elasticnet will produce curves that behave like a blend of the two above approaches. The strength of the LASSO is such that a balanced mix of the two penalization is not reached at $\alpha = 1/2$, but rather at a much smaller value (possibly below 0.1).


Below is the code for the backtest of **sparse portfolios**.

```{r, message = FALSE, warning = FALSE}
t_oos <- returns$date[returns$date > separation_date] %>%            # Out-of-sample dates 
    unique() %>%                                                     # Remove duplicates
    as.Date(origin = "1970-01-01")                                   # Transform in date format
Tt <- length(t_oos)                                                  # Nb of dates, avoid T 
nb_port <- 3                                                         # Nb of portfolios/strats.
portf_weights <- array(0, dim = c(Tt, nb_port, ncol(returns) - 1))   # Initial portf. weights
portf_returns <- matrix(0, nrow = Tt, ncol = nb_port)                # Initial portf. returns 
```


Next, because it is the purpose of this section, we isolate the computation of the weights of sparse-hedging portfolios. 


```{r, message = FALSE, warning = FALSE}
weights_sparsehedge <- function(returns, alpha, lambda){  # The parameters are defined here
    w <- 0                                                # Initiate weights
    for(i in 1:ncol(returns)){                            # Loop on the assets
        y <- returns[,i]                                  # Dependent variable
        x <- returns[,-i]                                 # Independent variable
        fit <- glmnet(x,y, family = "gaussian", alpha = alpha, lambda = lambda)
        err <- y-predict(fit, x)                          # Prediction errors
        w[i] <- (1-sum(fit$beta))/var(err)                # Output: weight of asset i
    }
    return(w / sum(w))                                    # Normalisation of weights
}
```


In order to benchmark our strategy, we define a meta-weighting function that embeds three strategies: the EW benchmarks (1), the classical GMV (2) and the sparse-hedging minimum variance (3). For the GMV, since there are much more assets than dates, the covariance matrix is singular. Thus, we have a small heuristic shrinkage term (see the papers of Ledoit & Wolf for details, especially the one from 2004 in JMA). 


```{r, message = FALSE, warning = FALSE}
weights_multi <- function(returns,j, alpha, lambda){
    N <- ncol(returns)
    if(j == 1){                                    # j = 1 => EW
        return(rep(1/N,N))
    }
    if(j == 2){                                    # j = 2 => Minimum Variance
        sigma <- cov(returns) + 0.01 * diag(N)     # Covariance matrix + regularizing term
        w <- solve(sigma) %*% rep(1,N)             # Inverse & multiply
        return(w / sum(w))                         # Normalize
    }
    if(j == 3){                                    # j = 3 => Penalised / elasticnet
        w <- weights_sparsehedge(returns, alpha, lambda)
    }
}
```

Finally, we proceed to the backtesting loop. Given the number of assets, the execution of the loop may take a few minutes. At the end of the loop, we compute the standard deviation of portfolio returns (monthly volatility). This is the key indicator as minimum variance seeks to minimize this particular metric.

```{r, message = FALSE, warning = FALSE}
for(t in 1:length(t_oos)){                                                 # Loop = rebal. dates
    temp_data <- returns %>%                                               # Data for weights
        filter(date < t_oos[t]) %>%                                        # Expand. window
        dplyr::select(-date) %>%
        as.matrix() 
    realised_returns <- returns %>%                                        # OOS returns
        filter(date ==  t_oos[t]) %>% 
        dplyr::select(-date)
    for(j in 1:nb_port){                                                   # Loop over strats
        portf_weights[t,j,] <- weights_multi(temp_data, j, 0.1, 0.1)       # Hard-coded params!
        portf_returns[t,j] <- sum(portf_weights[t,j,] * realised_returns)  # Portf. returns
    }
}
colnames(portf_returns) <- c("EW", "MV", "Sparse") # Colnames
apply(portf_returns, 2, sd)                        # Portfolio volatilities (monthly scale)
```



The code below is dedicated to predictive regressions.


```{r penpredreg, message = FALSE, warning = FALSE, size = "footnotesize"}
y_penalized_train <- training_sample$R1M_Usd                 # Dependent variable
x_penalized_train <- training_sample %>%                     # Predictors
    dplyr::select(features) %>% as.matrix()                  
fit_pen_pred <- glmnet(x_penalized_train, y_penalized_train, # Model
                       alpha = 0.1, lambda = 0.1)
```


We then report two key performance measures: the mean squared error and the hit ratio, which is the proportion of times that the prediction guesses the sign of the return correctly. 


```{r penpredregperf, message = FALSE, warning = FALSE, size = "footnotesize"}
x_penalized_test <- testing_sample %>%                                     # Predictors
    dplyr::select(features) %>% as.matrix()         
mean((predict(fit_pen_pred, x_penalized_test) - testing_sample$R1M_Usd)^2) # MSE
mean(predict(fit_pen_pred, x_penalized_test) * testing_sample$R1M_Usd > 0) # Hit ratio
```


From an investor's standpoint, the MSE (or even the mean absolute error) are hard to interpret because it is complicated to map them mentally into some intuitive financial indicator. In this perspective, the hit ratio is more natural. 

