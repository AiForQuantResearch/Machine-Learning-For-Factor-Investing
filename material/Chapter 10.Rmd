# Bayesian methods


**NOTES TO USERS**:   
- notebooks are by nature sequential. Chunks at the end depend on variables defined in the snippets at the beginning: don't forget to proceed in order!   
- only the code is provided. For comments of methods & results, we refer to the book.   
- please report errors!    
- don't forget: models are not optimized and variables are probably not optimally chosen, so *performance is often disappointing*.    

**Step 0**: please make sure the dataset is in your working directory!


The first step is to make sure the appropriate packages are installed.


```{r, message = FALSE, warning = FALSE}
if(!require(tidyverse)){install.packages("tidyverse")}
if(!require(lubridate)){install.packages("lubridate")}
if(!require(e1071)){install.packages("e1071")}
if(!require(spBayes)){install.packages("spBayes")}
if(!require(naivebayes)){install.packages(c("naivebayes"))}
if(!require(BART)){install.packages("BART")}
```


Next, we activate the relevant packages and load the data. 


```{r, message = FALSE, warning = FALSE}
library(tidyverse)                      # Activate the data science package
library(lubridate)                      # Activate the date management package
load("data_ml.RData")                   # Load the data
data_ml <- data_ml %>% 
    filter(date > "1999-12-31",         # Keep the date with sufficient data points
           date < "2019-01-01") %>%
    arrange(stock_id, date)             # Order the data
```


We also copy/paste & aggregate some chunks from Chapter 2.


```{r, message = FALSE, warning = FALSE}
stock_ids <- levels(as.factor(data_ml$stock_id)) # A list of all stock_ids
stock_days <- data_ml %>%                        # Compute the number of data points per stock
    group_by(stock_id) %>% summarize(nb = n()) 
stock_ids_short <- stock_ids[which(stock_days$nb == max(stock_days$nb))] # Stocks with full data
returns <- data_ml %>%                           # Compute returns, in matrix format, in 3 steps:
    filter(stock_id %in% stock_ids_short) %>%    # 1. Filtering the data
    dplyr::select(date, stock_id, R1M_Usd) %>%   # 2. Keep returns along with dates & firm names
    spread(key = stock_id, value = R1M_Usd)      # 3. Put in matrix shape 
features <- colnames(data_ml[3:95]) # Keep the feature's column names (hard-coded, beware!)
features_short <- c("Div_Yld", "Eps", "Mkt_Cap_12M_Usd", "Mom_11M_Usd", 
                    "Ocf", "Pb", "Vol1Y_Usd")
data_ml <- data_ml %>% 
    group_by(date) %>%                                   # Group by date
    mutate(R1M_Usd_C = R1M_Usd > median(R1M_Usd),        # Create the categorical labels
           R12M_Usd_C = R1M_Usd > median(R12M_Usd)) %>%
    ungroup() %>%
    mutate_if(is.logical, as.factor)
separation_date <- as.Date("2014-01-15")
training_sample <- filter(data_ml, date < separation_date)
testing_sample <- filter(data_ml, date >= separation_date)
```



The code belows pertains to Bayesian regressions.

```{r, message = FALSE, warning = FALSE}
prior_mean <- c(0.01,0.1,0.1)                    # Average value of parameters (prior)
precision_mat <- diag(prior_mean^2) %>% solve()  # Inverse cov matrix of parameters (prior)
fit_lmBayes <- bayesLMConjugate(
    R1M_Usd ~ Mkt_Cap_3M_Usd + Pb,          # Model: size and value
    data = testing_sample,                  # Data source, here, the test sample
    n.samples = 2000,                       # Number of samples used
    beta.prior.mean = prior_mean,           # Avg prior: size & value rewarded & unit beta
    beta.prior.precision = precision_mat,   # Precision matrix
    prior.shape = 0.5,                      # Shape for prior distribution of sigma
    prior.rate = 0.5)                       # Scale for prior distribution fo sigma
```

In the above specification, we must also provide a prior for the constant. By default, we set its average value to 0.01, which corresponds to a 1% average monthly return. Once the model has been estimated, we can plot the distribution of coefficient estimates.

```{r, message = FALSE, warning = FALSE}
fit_lmBayes$p.beta.tauSq.samples[,1:3] %>% as_tibble() %>%
    `colnames<-`(c("Intercept", "Size", "Value")) %>%
    gather(key = coefficient, value = value) %>%
    ggplot(aes(x = value, fill = coefficient)) + geom_histogram(alpha = 0.5)
```


We then move towards naive Bayes classifiers.


```{r, message = FALSE, warning = FALSE}
library(naivebayes)                           # Load package
gauss_features_train <- training_sample %>%   # Build sample
    dplyr::select(features_short) %>% 
    as.matrix() %>%
    `*`(0.999) %>%                            # Features smaller than 1
    + (0.0001) %>%                            # Features larger than 0
    qnorm() %>%                               # Inverse Gaussian cdf
    `colnames<-`(features_short)
fit_NB_gauss <- naive_bayes(x = gauss_features_train,      # Transformed features
                            y = training_sample$R1M_Usd_C) # Label
layout(matrix(c(1,1,2,3,4,5,6,7), 4, 2, byrow = TRUE),     # Organize graphs
       widths=c(0.9,0.45))
par(mar=c(1, 1, 1, 1))
plot(fit_NB_gauss, prob = "conditional")
```


```{r, message = FALSE, warning = FALSE, size = "footnotesize"}
gauss_features_test <- testing_sample %>% 
    dplyr::select(features_short) %>% 
    as.matrix() %>%
    `*`(0.999) %>%
    + (0.0001) %>%
    qnorm() %>%
    `colnames<-`(features_short)
mean(predict(fit_NB_gauss, gauss_features_test) == testing_sample$R1M_Usd_C) # Hit ratio
```



Finally, we end with Bayesian additive trees


There are several R packages that implement BART methods: *BART*, *bartMachine* and an older one (the original), *BayesTree*. The first one is highly efficient, hence we work with it. 

```{r, message = FALSE, warning = FALSE}
library(BART)                                                           # Load package
fit_bart <- gbart(                                                      # Main function
    x.train = dplyr::select(training_sample, features_short) %>%        # Training features
        data.frame(), 
    y.train = dplyr::select(training_sample, R1M_Usd) %>%               # Training label
        as.matrix() ,        
    x.test = dplyr::select(testing_sample, features_short)  %>%         # Testing features
        data.frame(),  
    type = "wbart",                                          # Option: label is continuous
    ntree = 20,                                              # Number of trees in the model 
    nskip = 100,                                             # Size of burn-in sample
    ndpost = 200,                                            # Number of posteriors drawn
    power = 2,                                               # beta in the tree structure prior
    base = 0.95)                                             # alpha in the tree structure prior
```

Once the model is trained,^[In the case of BARTs, the training is consists exactly in the drawing of posterior samples.] we evaluated its performance. We simply compute the hit ratio. The predictions are embedded within the fit variable, under the name '*yhat.test*'.


```{r bart1, message = FALSE, warning = FALSE, , size = "footnotesize"}
mean(fit_bart$yhat.test * testing_sample$R1M_Usd > 0)
```


The data from all sampled trees is available in the *fit_bart* variable. It has nonetheless a complex structure (as is often the case with trees). The simplest information we can extract is the value of $\sigma$ across all 300 simulations.

```{r bartsigplot, fig.cap="Evolution of sigma across BART simulations.", fig.asp=0.7, fig.width=4}
data.frame(simulation = 1:300, sigma = fit_bart$sigma) %>%
    ggplot(aes(x = simulation, y = sigma)) + geom_point(size = 0.7)
```


