# Data preprocessing

**NOTES TO USERS**:   
- notebooks are by nature sequential. Chunks at the end depend on variables defined in the snippets at the beginning: don't forget to proceed in order!   
- only the code is provided. For comments of methods & results, we refer to the book.   
- please report errors!   

**Step 0**: please make sure the dataset is in your working directory!


The first step is to make sure the appropriate packages are installed.


```{r, message = FALSE, warning = FALSE}
if(!require(tidyverse)){install.packages("tidyverse")}
if(!require(lubridate)){install.packages("lubridate")}
```


Next, we activate the relevant packages and load the data. 


```{r, message = FALSE, warning = FALSE}
library(tidyverse)                      # Activate the data science package
library(lubridate)                      # Activate the date management package
load("data_ml.RData")                   # Load the data
data_ml <- data_ml %>% 
    filter(date > "1999-12-31",         # Keep the date with sufficient data points
           date < "2019-01-01") %>%
    arrange(stock_id, date)             # Order the data
```


We also recycle a few variables from Chapter 2: feature names essentially.


```{r, message = FALSE, warning = FALSE}
features <- colnames(data_ml[3:95]) # Keep the feature's column names (hard-coded, beware!)
features_short <- c("Div_Yld", "Eps", "Mkt_Cap_12M_Usd", "Mom_11M_Usd", 
                    "Ocf", "Pb", "Vol1Y_Usd")
```


Below, we show a box-plot that illustrates the distribution of correlations between features and the one month ahead return. The correlations are computed on a date-by-date basis, over the whole cross-section of stocks. 


```{r, message = FALSE, warning = FALSE}
data_ml %>% 
    dplyr::select(c(features_short, "R1M_Usd", "date")) %>%     # Keep few features, label & date
    group_by(date) %>%                                          # Group: dates!
    summarise_all(funs(cor(.,R1M_Usd))) %>%                     # Compute correlations
    dplyr::select(-R1M_Usd) %>%                                 # Remove label
    gather(key = Predictor, value = value, -date) %>%           # Put in tidy format
    ggplot(aes(x = Predictor, y = value, color = Predictor)) +  # Plot
    geom_boxplot(outlier.colour = "black") + coord_flip() 
```


Below, we plot two illustrations of the smoothed **conditional average** when the dependent variable is the one month ahead return. Sometimes, this function is called the **regression function**.


```{r, message = FALSE, warning = FALSE}
data_ml %>%                                                      # From dataset:
  ggplot(aes(y = R1M_Usd)) +                                     # Plot
  geom_smooth(aes(x = Mkt_Cap_12M_Usd, color = "Market Cap")) +  # Cond. Exp. Mkt_cap
  geom_smooth(aes(x = Vol1Y_Usd, color = "Volatility")) +        # Cond. Exp. Vol
  scale_color_manual(values=c("#F87E1F", "#0570EA")) +           # Change color
  labs(color = "Predictor") + xlab(element_blank())
```


Below, we build the histogram of autocorrelations, computed stock by stock and feature by feature. It takes a bit of time.


```{r, message = FALSE, warning = FALSE}
autocorrs <- data_ml %>%                                         # From dataset:
  dplyr::select(c("stock_id", features)) %>%                     # Keep ids & features
  gather(key = feature, value = value, -stock_id) %>%            # Put in tidy format
  group_by(stock_id, feature) %>%                                # Group
  summarize(acf = acf(value, lag.max = 1, plot = FALSE)$acf[2])  # Compute ACF
autocorrs %>% ggplot(aes(x = acf)) + xlim(-0.1,1) +              # Plot
   geom_histogram(bins = 60)  
```


We then turn to a simple illustration of the different scaling methods. We generate an arbitrary series and then rescale it. The series is not random so that each time the code chunk is executed, the output remains the same.


```{r, message = FALSE, warning = FALSE}
Length <- 100                                 # Length of the sequence
x <- exp(sin(1:Length))                       # Original data
data <- data.frame(index = 1:Length, x = x)   # Data framed into dataframe
ggplot(data, aes(x = index, y = x)) + geom_bar(stat = "identity") # Plot
```

We define and plot the scaled variables below.

```{r, message = FALSE, warning = FALSE}
norm_unif <-  function(v){  # This is a function that uniformalises a vector.
    v <- v %>% as.matrix()
    return(ecdf(v)(v))
}

norm_0_1 <-  function(v){  # This is a function that uniformalises a vector.
    return((v-min(v))/(max(v)-min(v)))
}

data_norm <- data.frame(                        # Formatting the data
    index = 1:Length,                           # Index of point/instance
    standard = (x - mean(x)) / sd(x),           # Standardisation
    norm_0_1 = norm_0_1(x),                     # [0,1] reduction
    unif = norm_unif(x)) %>%                    # Uniformisation
    gather(key = Type, value = value, -index)   # Putting in tidy format
ggplot(data_norm, aes(x = index, y = value, fill = Type)) +   # Plot!
    geom_bar(stat = "identity") +
    facet_grid(Type~.)          # This option creates 3 concatenated graphs to ease comparison
```

Finally, we look at the histogram of the newly created variables.

```{r, warning = FALSE, message = FALSE}
ggplot(data_norm, aes(x = value, fill = Type)) + geom_histogram(position = "dodge")
```



Finally, to illustrate the impact of chosing one particular rescaling method, we build a simple dataset, comprising 3 firms and 3 dates. 

```{r, message = FALSE, warning = FALSE}
firm <- c(rep(1,3), rep(2,3), rep(3,3))             # Firms (3 lines for each)
date <- rep(c(1,2,3),3)                             # Dates
cap <- c(10, 50, 100,                               # Market capitalisation
         15, 10, 15, 
         200, 120, 80)
return <- c(0.06, 0.01, -0.06,                      # Return values
            -0.03, 0.00, 0.02,
            -0.04, -0.02,0.00)
data_toy <- data.frame(firm, date, cap, return)     # Aggregation of data
data_toy <- data_toy %>%                            # Transformation of data
    group_by(date) %>%                            
    mutate(cap_0_1 = norm_0_1(cap), cap_u = norm_unif(cap))
data_toy                                            # Display the data
               
```


Now let us look at the output of simple regressions. Below, the package *broom* is part of the *tidyverse*.


```{r}
lm(return ~ cap_0_1, data = data_toy) %>% # First regression (min-max rescaling)
    broom::tidy() 
```


```{r}
lm(return ~ cap_u, data = data_toy) %>%   # Second regression (uniformised feature)
    broom::tidy()   
```

In terms of *p*-**value** (last column), the first estimate for the cap coefficient is above 5%  while the second is below 1%.

