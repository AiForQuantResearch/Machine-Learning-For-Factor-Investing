# Interpretability 


**NOTES TO USERS**:   
- notebooks are by nature sequential. Chunks at the end depend on variables defined in the snippets at the beginning: don't forget to proceed in order!   
- only the code is provided. For comments of methods & results, we refer to the book.   
- please report errors!    
- don't forget: models are not optimized and variables are probably not optimally chosen, so *performance is often disappointing*.    

**Step 0**: please make sure the dataset is in your working directory!


The first step is to make sure the required packages are installed.


```{r, message = FALSE, warning = FALSE}
if(!require(tidyverse)){install.packages("tidyverse")}
if(!require(lubridate)){install.packages("lubridate")}
if(!require(rpart)){install.packages(c("rpart","rpart.plot"))}
if(!require(randomForest)){install.packages("randomForest")}
if(!require(iml)){install.packages("iml")}
if(!require(lime)){install.packages("lime")}
if(!require(breakDown)){install.packages("breakDown")}
if(!require(glmnet)){install.packages("glmnet")}
if(!require(xgboost)){install.packages("xgboost")}
```


Next, we activate the relevant packages and load the data. 


```{r, message = FALSE, warning = FALSE}
library(tidyverse)                      # Activate the data science package
library(lubridate)                      # Activate the date management package
library(rpart)                          # Package for the trees 
library(rpart.plot)                     # Package for tree plots
library(randomForest)                   # Package for random forests
library(xgboost)                        # Package for boosted trees
load("data_ml.RData")                   # Load the data
data_ml <- data_ml %>% 
    filter(date > "1999-12-31",         # Keep the date with sufficient data points
           date < "2019-01-01") %>%
    arrange(stock_id, date)             # Order the data
```


We also copy/paste & aggregate some chunks from Chapter 2.


```{r, message = FALSE, warning = FALSE}
stock_ids <- levels(as.factor(data_ml$stock_id)) # A list of all stock_ids
stock_days <- data_ml %>%                        # Compute the number of data points per stock
    group_by(stock_id) %>% summarize(nb = n()) 
stock_ids_short <- stock_ids[which(stock_days$nb == max(stock_days$nb))] # Stocks with full data
returns <- data_ml %>%                           # Compute returns, in matrix format, in 3 steps:
    filter(stock_id %in% stock_ids_short) %>%    # 1. Filtering the data
    dplyr::select(date, stock_id, R1M_Usd) %>%   # 2. Keep returns along with dates & firm names
    spread(key = stock_id, value = R1M_Usd)      # 3. Put in matrix shape 
features <- colnames(data_ml[3:95]) # Keep the feature's column names (hard-coded, beware!)
features_short <- c("Div_Yld", "Eps", "Mkt_Cap_12M_Usd", "Mom_11M_Usd", 
                    "Ocf", "Pb", "Vol1Y_Usd")
data_ml <- data_ml %>% 
    group_by(date) %>%                                   # Group by date
    mutate(R1M_Usd_C = R1M_Usd > median(R1M_Usd),        # Create the categorical labels
           R12M_Usd_C = R1M_Usd > median(R12M_Usd)) %>%
    ungroup() %>%
    mutate_if(is.logical, as.factor)
separation_date <- as.Date("2014-01-15")
training_sample <- filter(data_ml, date < separation_date)
testing_sample <- filter(data_ml, date >= separation_date)
```


First, a surrogate model for a random forest.  We start by fitting the model.

```{r, message = FALSE, warning = FALSE}
library(randomForest) 
set.seed(42)                                # Sets the random seed
formula <- paste("R1M_Usd ~", paste(features, collapse = " + ")) # Defines the model 
formula <- as.formula(formula)                                   # Forcing formula object
fit_RF <- randomForest(formula,             # Same formula as for simple trees!
                 data = training_sample,    # Data source: training sample
                 sampsize = 10000,          # Size of (random) sample for each tree
                 replace = FALSE,           # Is the sampling done with replacement?
                 nodesize = 250,            # Minimum size of terminal cluster
                 ntree = 40,                # Nb of random trees
                 mtry = 30                  # Nb of predictive variables for each tree
    )
predict(fit_RF, testing_sample[1:5,])       # Prediction over the first 5 test instances 
```


Then, the surrogate.


```{r, message = FALSE, warning = FALSE}
library(iml)
mod <- Predictor$new(fit_RF, 
                     data = training_sample %>% dplyr::select(features)) 
dt <- TreeSurrogate$new(mod, maxdepth = 2)
plot(dt)
```


Then, variable importance for tree models. We need to train them: simple trees (short code) and boosted trees (long code).

```{r, message = FALSE, warning = FALSE}
fit_tree <- rpart(formula,
             data = data_ml,     # Data source: full sample
             minbucket = 3500,   # Min nb of obs required in each terminal node (leaf)
             minsplit = 8000,    # Min nb of obs required to continue splitting
             cp = 0.0001,        # Precision: smaller = more leaves
             maxdepth = 3        # Maximum depth (i.e. tree levels)
             ) 

train_features_xgb <- training_sample %>% 
    filter(R1M_Usd < quantile(R1M_Usd, 0.2) | 
               R1M_Usd > quantile(R1M_Usd, 0.8)) %>%            # Extreme values only!
    dplyr::select(all_of(features_short)) %>% as.matrix()       # Independent variable
train_label_xgb <- training_sample %>%
    filter(R1M_Usd < quantile(R1M_Usd, 0.2) | 
               R1M_Usd > quantile(R1M_Usd, 0.8)) %>%
    dplyr::select(R1M_Usd) %>% as.matrix()                      # Dependent variable
train_matrix_xgb <- xgb.DMatrix(data = train_features_xgb, 
                                label = train_label_xgb)        # XGB format!
mono_const <- rep(0, length(features))                   # Initialize the vector
mono_const[which(features == "Mkt_Cap_12M_Usd")] <- (-1) # Decreasing in market cap
mono_const[which(features == "Pb")] <- (-1)              # Decreasing in price-to-book
mono_const[which(features == "Mom_11M_Usd")] <- 1        # Increasing in past return
fit_xgb <- xgb.train(data = train_matrix_xgb,     # Data source 
              eta = 0.3,                          # Learning rate
              objective = "reg:linear",           # Objective function
              max_depth = 4,                      # Maximum depth of trees
              lambda = 1,                         # Penalisation of leaf values
              gamma = 0.1,                        # Penalisation of number of leaves
              nrounds = 30,                       # Number of trees used (rather low here)
              monotone_constraints = mono_const,  # Monotonicity constraints
              verbose = 0                         # No comment from the algo 
    )
```


Now we can move forward.

```{r, message = FALSE, warning = FALSE}
tree_VI <- fit_tree$variable.importance  %>%                        # VI from tree model
    as_tibble(rownames = NA) %>%                                    # Transform in tibble 
    rownames_to_column("Feature")                                   # Add feature column
RF_VI <- fit_RF$importance  %>%                                     # VI from random forest
    as_tibble(rownames = NA) %>%                                    # Transform in tibble 
    rownames_to_column("Feature")                                   # Add feature column
XGB_VI <- xgb.importance(model = fit_xgb)[,1:2]                     # VI from boosted trees
VI_trees <- tree_VI %>% left_join(RF_VI) %>% left_join(XGB_VI)      # Aggregate the VIs
colnames(VI_trees)[2:4] <- c("Tree", "RF", "XGB")                   # New column names
norm_1 <- function(x){return(x / sum(x))}                           # Normalizing function
VI_trees %>% na.omit %>% mutate_if(is.numeric,  norm_1) %>%         # Plotting sequence
    gather(key = model, value = value, -Feature) %>%
    ggplot(aes(x = Feature, y = value, fill = model)) + geom_col(position = "dodge") +
    theme(axis.text.x = element_text(angle = 35, hjust = 1))
```


Next, we cover variable importance for general models. We choose a penalized regression. 


```{r, message = FALSE, warning = FALSE}
library(glmnet)
y_penalized_train <- training_sample$R1M_Usd                 # Dependent variable
x_penalized_train <- training_sample %>%                     # Predictors
    dplyr::select(all_of(features)) %>% as.matrix()       
fit_ridge_0 <- glmnet(x_penalized_train, y_penalized_train,                   # Trained model
                      alpha = 0, lambda = 0.01) 
l_star <- mean((y_penalized_train-predict(fit_ridge_0, x_penalized_train))^2) # Loss
```


Next, we evaluate the loss when each of the predictors have been sequentially shuffled. To reduce computation time, we only make one round of shuffling.


```{r, message = FALSE, warning = FALSE}
l <- c()                                                             # Initialize
for(i in 1:nrow(VI_trees)){                                          # Loop on the features
    feat_name <- as.character(VI_trees[i,1])
    temp_data <- training_sample %>% dplyr::select(features)         # Temp feature matrix
    temp_data[, which(colnames(temp_data) == feat_name)] <-          # Shuffles the values
        sample(temp_data[, which(colnames(temp_data) == feat_name)]
               %>% pull(1), replace = FALSE)
    x_penalized_temp <- temp_data %>% as.matrix()                    # Predictors into matrix
    fit_ridge_temp <- glmnet(x_penalized_temp, y_penalized_train,    # Fits the model
                             alpha = 0, lambda = 0.01) 
    l[i] <- mean((y_penalized_train-predict(fit_ridge_temp, x_penalized_temp))^2) # = Loss
}
```


Finally, we plot the results.


```{r, message = FALSE, warning = FALSE}
data.frame(Feature = VI_trees[,1], loss = l - l_star) %>%
    ggplot(aes(x = Feature, y = loss)) + geom_col() +
    theme(axis.text.x = element_text(angle = 35, hjust = 1))
```


Below we plot a partial dependence plot. The original model is a random forest.

```{r, message = FALSE, warning = FALSE}
library(iml)                                         # One package for interpretability
mod_iml <- Predictor$new(fit_RF,                     # This line encapsulates the objects
                         data = training_sample %>% dplyr::select(features))
pdp_PB = FeatureEffect$new(mod_iml, feature = "Pb")  # This line computes the PDP for p/b ratio
plot(pdp_PB)                                         # Plot the partial dependence.
```



We proceed with an example of **LIME** implementation. There are several steps:   

1. Fit a model on some training data.    
2. Wrap everything using the lime() function.    
3. Focus on a few predictors and see their impact over a few particular instances (via the explain() function). 

We start with the first step. This time, we work with a boosted tree model.

```{r lime0, message = FALSE, warning = FALSE}
library(lime)                              # Package for LIME interpretation
params_xgb <- list(                        # Parameters of the boosted tree
    max_depth = 5,                         # Max depth of each tree
    eta = 0.5,                             # Learning rate 
    gamma = 0.1,                           # Penalization
    colsample_bytree = 1,                  # Proportion of predictors to be sampled (1 = all)
    min_child_weight = 10,                 # Min number of instances in each node
    subsample = 1)                         # Proportion of instance to be sampled (1 = all)
xgb_model <- xgb.train(params_xgb,         # Training of the model
                       train_matrix_xgb,   # Training data
                       nrounds = 10)       # Number of trees
```

Then, we head on to steps two and three. As underlined above, we resort to the lime() and explain() functions. 

```{r, message = FALSE, warning = FALSE}
explainer <- lime(training_sample %>% dplyr::select(features_short), xgb_model) # Step 2.
explanation <- explain(x = training_sample %>%                                  # Step 3.
                           dplyr::select(features_short) %>%
                           dplyr::slice(1:2),           # First two instances in train_sample 
                       explainer = explainer,           # Explainer variable created above 
                       n_permutations = 900,            # Nb samples for loss function
                       dist_fun = "euclidean",          # Dist.func. "gower" is one alternative
                       n_features = 6                   # Nb of features shown (important ones)
)
plot_features(explanation, ncol = 1)                    # Visual display
```



Then, we turn to Shapley values.

We start by fitting a new random forest model.

```{r, message = FALSE, warning = FALSE}
fit_RF_short <- randomForest(R1M_Usd ~.,    # Same formula as for simple trees!
                 data = training_sample %>% dplyr::select(c(features_short), "R1M_Usd"),  
                 sampsize = 10000,          # Size of (random) sample for each tree
                 replace = FALSE,           # Is the sampling done with replacement?
                 nodesize = 250,            # Minimum size of terminal cluster
                 ntree = 40,                # Nb of random trees
                 mtry = 4                   # Nb of predictive variables for each tree
    )
```

We can then analyze the behavior of the model around the first instance of the training sample.

```{r, message = FALSE, warning = FALSE}
predictor <- Predictor$new(fit_RF_short,    # This wraps the model & data
                          data = training_sample %>% dplyr::select(features_short), 
                          y = training_sample$R1M_Usd)
shapley <- Shapley$new(predictor,                        # Compute the Shapley values...
                       x.interest = training_sample %>% 
                           dplyr::select(features_short) %>%
                           dplyr::slice(1))              # On the first instance
plot(shapley) + coord_fixed(1500) +                      # Plot
    theme(axis.text.x = element_text(angle = 35, hjust = 1)) + coord_flip()          
```


Last analysis: **breakdown**.

In order to illustrate one implementation of breakdown, we train a random forest on a limited number of features, as shown below. This will increase the readability of the output of the breakdown.

```{r, message = FALSE, warning = FALSE}
formula_short <- paste("R1M_Usd ~", paste(features_short, collapse = " + ")) #  Model 
formula_short <- as.formula(formula_short)                                   #  Formula format
fit_RF_short <- randomForest(formula_short, # Same formula as before
                 data = dplyr::select(training_sample, c(features_short, "R1M_Usd")),  
                 sampsize = 10000,          # Size of (random) sample for each tree
                 replace = FALSE,           # Is the sampling done with replacement?
                 nodesize = 250,            # Minimum size of terminal cluster
                 ntree = 12,                # Nb of random trees
                 mtry = 5                   # Nb of predictive variables for each tree
    )
```

Once the model is trained, the syntax for the breakdown of predictions is very simple. 

```{r,  message = FALSE, warning = FALSE}
library(breakDown)
explain_break <- broken(fit_RF_short, 
                        data_ml[6,] %>% dplyr::select(features_short),
                        data = data_ml %>% dplyr::select(features_short))
plot(explain_break) 
```


