# Reinforcement learning 


**NOTES TO USERS**:   
- notebooks are by nature sequential. Chunks at the end depend on variables defined in the snippets at the beginning: don't forget to proceed in order!   
- only the code is provided. For comments of methods & results, we refer to the book.   
- please report errors!    
- don't forget: models are not optimized and variables are probably not optimally chosen, so *performance is often disappointing*.    

**Step 0**: please make sure the dataset is in your working directory!


The first step is to make sure the required packages are installed.


```{r, message = FALSE, warning = FALSE}
if(!require(tidyverse)){install.packages("tidyverse")}
if(!require(lubridate)){install.packages("lubridate")}
if(!require(ReinforcementLearning)){install.packages("ReinforcementLearning")}
```


Next, we activate the relevant packages and load the data. 


```{r, message = FALSE, warning = FALSE}
library(tidyverse)                      # Activate the data science package
library(lubridate)                      # Activate the date management package

load("data_ml.RData")                   # Load the data
data_ml <- data_ml %>% 
    filter(date > "1999-12-31",         # Keep the date with sufficient data points
           date < "2019-01-01") %>%
    arrange(stock_id, date)             # Order the data
```



We propose two implementations of $Q$-learning. For simplicity, the first one is based on simulations. This helps understand the learning process in a simplified framework. We consider two assets: one risky and one riskless, with return equal to zero. The returns for the risky process follow an autoregressive model of order one (AR(1)): $r_{t+1}=a+\rho r_t+\epsilon_{t+1}$ with $|\rho|<1$ and $\epsilon$ following a standard white noise with variance $\sigma^2$. In practivc, individual (monthly) returns are seldom autocorrelated, but adjusting the autocorrelation helps understand if the algorithm learns correctly (see exercise below). 

The environment consists only in observing the past return $r_t$. Since we seek to estimate the $Q$ function, we need to discretize this state variable. The simplest choice is to resort to a binary variable: equal to -1 (negative) if $r_t<0$ and to +1 (positive) if $r_t\ge 0$. The actions are summarized by the quantity invested in the risky asset. It can take 5 values: 0 (risk-free portfolio), 0.25, 0.5, 0.75 and 1 (fully invested in the risky asset). 

The landscape of R libraries for RL is surprisingly sparse. We resort to the package *ReinforcementLearning* which has an intuitive implementation of $Q$-learning. It requires a dataset with the usual inputs: state, action, reward and subsequent state. We start by simulating the returns: they drive the states and the rewards (portfolio returns). The actions are sampled randomly. Technically, the main function of the package requires that states and actions be of character type. The data is built in the chunk below.

```{r, message = FALSE, warning = FALSE}
library(ReinforcementLearning)                              # Package for RL
set.seed(42)                                                # Fixing the random seed
n_sample <- 10^5                                            # Number of samples to be generated
rho <- 0.8                                                  # Autoregressive parameter
sd <- 0.4                                                   # Std. dev. of noise
a <- 0.06 * rho                                             # Scaled mean of returns
data_RL <- tibble(returns = a/rho + arima.sim(n = n_sample, # Returns via AR(1) simulation
                                      list(ar = rho),       
                                      sd = sd),
                  action = round(runif(n_sample)*4)/4) %>%  # Random action (portfolio)
    mutate(new_state = if_else(returns < 0, "neg", "pos"),  # Coding of state
           reward = returns * action,                       # Reward = portfolio return
           state = lag(new_state),                          # Next state
           action = as.character(action)) %>% 
    na.omit()                                               # Remove one missing state
data_RL %>% head()                                          # Show first lines
```

There are 3 parameters in the implementation of the *Q*-learning algorithm:   

- $\eta$, which is the learning rate;   
- $\gamma$, the discounting rate for the rewards;   
- and $\epsilon$, which controls the rate of exploration versus exploitation.

```{r, message = FALSE, warning = FALSE}
control <- list(alpha = 0.1,                       # Learning rate
                gamma = 0.7,                       # Discount factor for rewards
                epsilon = 0.1)                     # Exploration rate

fit_RL <- ReinforcementLearning(data_RL,           # Main RL function
                               s = "state", 
                               a = "action", 
                               r = "reward", 
                               s_new = "new_state", 
                               control = control)
print(fit_RL)   # Show the output
```



The second application is based on the financial dataset. To reduce the dimensionality of the problem, we will assume:  
- that only one feature (price-to-book ratio) captures the state of the environment. This feature is processed so that is has only a limited number of possible values;   
- that actions take values over a discrete set consisting of three positions: +1 (buy the market), -1 (sell the market) and 0 (hold no risky positions);    
- that only two assets are traded: those with stock_id equal to 3 and 4 - they both have 245 days of trading data.   

The construction of the dataset is unelegantly coded below.

```{r, message = FALSE, warning = FALSE}
return_3 <- data_ml %>% filter(stock_id == 3) %>% pull(R1M_Usd)  # Return of asset 3
return_4 <- data_ml %>% filter(stock_id == 4) %>% pull(R1M_Usd)  # Return of asset 4
pb_3 <- data_ml %>% filter(stock_id == 3) %>% pull(Pb)           # P/B ratio of asset 3
pb_4 <- data_ml %>% filter(stock_id == 4) %>% pull(Pb)           # P/B ratio of asset 4
action_3 <- floor(runif(length(pb_3))*3) - 1                     # Action for asset 3 (random)
action_4 <- floor(runif(length(pb_4))*3) - 1                     # Action for asset 4 (random)

RL_data <- tibble(return_3, return_4,                            # Building the dataset
                  pb_3, pb_4,
                  action_3, action_4) %>%
    mutate(action = paste(action_3, action_4),                   # Uniting actions
           pb_3 = round(5 * pb_3),                               # Simplifying states (P/B)
           pb_4 = round(5 * pb_4),                               # Simplifying states (P/B)
           state = paste(pb_3, pb_4),                            # Uniting states
           reward = action_3*return_3 + action_4*return_4,       # Computing rewards
           new_state = lead(state)) %>%                          # Infer new state
    dplyr::select(-pb_3, -pb_4, -action_3,                       # Remove superfluous vars.
                  -action_4, -return_3, -return_4) 
head(RL_data)                                                    # Showing the result
```

Actions and states have to be merged to yield all possible combinations. To simplify the states, we round 5 times the price-to-book ratios. 

We keep the same hyperparameters as in the previous example. Columns below stand for actions: the first ($resp.$ second) number notes the position in the first ($resp.$ second) asset. The rows correspond to states. The scaled P/B ratios are separated by a point (e.g., "X2.3" means that the first ($resp.$ second) asset has a scaled P/B of 2 ($resp.$ 3). 

```{r, message = FALSE, warning = FALSE}
fit_RL2 <- ReinforcementLearning(RL_data,           # Main RL function
                               s = "state", 
                               a = "action", 
                               r = "reward", 
                               s_new = "new_state", 
                               control = control)
fit_RL2$Q <- round(fit_RL2$Q, 3) # Round the Q-matrix
print(fit_RL2)                   # Show the output 
```

