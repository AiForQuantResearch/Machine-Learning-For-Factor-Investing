<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Data preprocessing | Machine Learning for Factor Investing</title>
  <meta name="description" content="Chapter 5 Data preprocessing | Machine Learning for Factor Investing" />
  <meta name="generator" content="bookdown 0.16 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Data preprocessing | Machine Learning for Factor Investing" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Data preprocessing | Machine Learning for Factor Investing" />
  
  
  

<meta name="author" content="Guillaume Coqueret and Tony Guida" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="factor.html"/>
<link rel="next" href="lasso.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="preface.html"><a href="preface.html#what-this-book-is-not-about"><i class="fa fa-check"></i><b>1.1</b> What this book is not about</a></li>
<li class="chapter" data-level="1.2" data-path="preface.html"><a href="preface.html#the-targeted-audience"><i class="fa fa-check"></i><b>1.2</b> The targeted audience</a></li>
<li class="chapter" data-level="1.3" data-path="preface.html"><a href="preface.html#how-this-book-is-structured"><i class="fa fa-check"></i><b>1.3</b> How this book is structured</a></li>
<li class="chapter" data-level="1.4" data-path="preface.html"><a href="preface.html#companion-website"><i class="fa fa-check"></i><b>1.4</b> Companion website</a></li>
<li class="chapter" data-level="1.5" data-path="preface.html"><a href="preface.html#why-r"><i class="fa fa-check"></i><b>1.5</b> Why R?</a></li>
<li class="chapter" data-level="1.6" data-path="preface.html"><a href="preface.html#coding-instructions"><i class="fa fa-check"></i><b>1.6</b> Coding instructions</a></li>
<li class="chapter" data-level="1.7" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i><b>1.7</b> Acknowledgements</a></li>
<li class="chapter" data-level="1.8" data-path="preface.html"><a href="preface.html#future-developments"><i class="fa fa-check"></i><b>1.8</b> Future developments</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="notdata.html"><a href="notdata.html"><i class="fa fa-check"></i><b>2</b> Notations and data</a><ul>
<li class="chapter" data-level="2.1" data-path="notdata.html"><a href="notdata.html#notations"><i class="fa fa-check"></i><b>2.1</b> Notations</a></li>
<li class="chapter" data-level="2.2" data-path="notdata.html"><a href="notdata.html#dataset"><i class="fa fa-check"></i><b>2.2</b> Dataset</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>3</b> Introduction</a><ul>
<li class="chapter" data-level="3.1" data-path="intro.html"><a href="intro.html#context"><i class="fa fa-check"></i><b>3.1</b> Context</a></li>
<li class="chapter" data-level="3.2" data-path="intro.html"><a href="intro.html#portfolio-construction-the-workflow"><i class="fa fa-check"></i><b>3.2</b> Portfolio construction: the workflow</a></li>
<li class="chapter" data-level="3.3" data-path="intro.html"><a href="intro.html#machine-learning-is-no-magic-wand"><i class="fa fa-check"></i><b>3.3</b> Machine Learning is no Magic Wand</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="factor.html"><a href="factor.html"><i class="fa fa-check"></i><b>4</b> Factor investing and asset pricing anomalies</a><ul>
<li class="chapter" data-level="4.1" data-path="factor.html"><a href="factor.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="factor.html"><a href="factor.html#detecting-anomalies"><i class="fa fa-check"></i><b>4.2</b> Detecting anomalies</a><ul>
<li class="chapter" data-level="4.2.1" data-path="factor.html"><a href="factor.html#simple-portfolio-sorts"><i class="fa fa-check"></i><b>4.2.1</b> Simple portfolio sorts</a></li>
<li class="chapter" data-level="4.2.2" data-path="factor.html"><a href="factor.html#factors"><i class="fa fa-check"></i><b>4.2.2</b> Factors</a></li>
<li class="chapter" data-level="4.2.3" data-path="factor.html"><a href="factor.html#predictive-regressions-sorts-and-p-value-issues"><i class="fa fa-check"></i><b>4.2.3</b> Predictive regressions, sorts, and p-value issues</a></li>
<li class="chapter" data-level="4.2.4" data-path="factor.html"><a href="factor.html#fama-macbeth-regressions"><i class="fa fa-check"></i><b>4.2.4</b> Fama-Macbeth regressions</a></li>
<li class="chapter" data-level="4.2.5" data-path="factor.html"><a href="factor.html#factor-competition"><i class="fa fa-check"></i><b>4.2.5</b> Factor competition</a></li>
<li class="chapter" data-level="4.2.6" data-path="factor.html"><a href="factor.html#advanced-techniques"><i class="fa fa-check"></i><b>4.2.6</b> Advanced techniques</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="factor.html"><a href="factor.html#factors-or-characteristics"><i class="fa fa-check"></i><b>4.3</b> Factors or characteristics?</a></li>
<li class="chapter" data-level="4.4" data-path="factor.html"><a href="factor.html#momentum-and-timing"><i class="fa fa-check"></i><b>4.4</b> Momentum and timing</a><ul>
<li class="chapter" data-level="4.4.1" data-path="factor.html"><a href="factor.html#factor-momentum"><i class="fa fa-check"></i><b>4.4.1</b> Factor momentum</a></li>
<li class="chapter" data-level="4.4.2" data-path="factor.html"><a href="factor.html#factor-timing"><i class="fa fa-check"></i><b>4.4.2</b> Factor timing</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="factor.html"><a href="factor.html#the-link-with-machine-learning"><i class="fa fa-check"></i><b>4.5</b> The link with machine learning</a><ul>
<li class="chapter" data-level="4.5.1" data-path="factor.html"><a href="factor.html#a-short-list-of-recent-references"><i class="fa fa-check"></i><b>4.5.1</b> A short list of recent references</a></li>
<li class="chapter" data-level="4.5.2" data-path="factor.html"><a href="factor.html#explicit-connexions-with-asset-pricing-models"><i class="fa fa-check"></i><b>4.5.2</b> Explicit connexions with asset pricing models</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="factor.html"><a href="factor.html#coding-exercises"><i class="fa fa-check"></i><b>4.6</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Data.html"><a href="Data.html"><i class="fa fa-check"></i><b>5</b> Data preprocessing</a><ul>
<li class="chapter" data-level="5.1" data-path="Data.html"><a href="Data.html#know-your-data"><i class="fa fa-check"></i><b>5.1</b> Know your data</a></li>
<li class="chapter" data-level="5.2" data-path="Data.html"><a href="Data.html#missing-data"><i class="fa fa-check"></i><b>5.2</b> Missing data</a></li>
<li class="chapter" data-level="5.3" data-path="Data.html"><a href="Data.html#outlier-detection"><i class="fa fa-check"></i><b>5.3</b> Outlier detection</a></li>
<li class="chapter" data-level="5.4" data-path="Data.html"><a href="Data.html#feateng"><i class="fa fa-check"></i><b>5.4</b> Feature engineering</a><ul>
<li class="chapter" data-level="5.4.1" data-path="Data.html"><a href="Data.html#feature-selection"><i class="fa fa-check"></i><b>5.4.1</b> Feature selection</a></li>
<li class="chapter" data-level="5.4.2" data-path="Data.html"><a href="Data.html#scaling"><i class="fa fa-check"></i><b>5.4.2</b> Scaling the predictors</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="Data.html"><a href="Data.html#labelling"><i class="fa fa-check"></i><b>5.5</b> Labelling</a><ul>
<li class="chapter" data-level="5.5.1" data-path="Data.html"><a href="Data.html#simple-labels"><i class="fa fa-check"></i><b>5.5.1</b> Simple labels</a></li>
<li class="chapter" data-level="5.5.2" data-path="Data.html"><a href="Data.html#categorical-labels"><i class="fa fa-check"></i><b>5.5.2</b> Categorical labels</a></li>
<li class="chapter" data-level="5.5.3" data-path="Data.html"><a href="Data.html#the-triple-barrier-method"><i class="fa fa-check"></i><b>5.5.3</b> The triple barrier method</a></li>
<li class="chapter" data-level="5.5.4" data-path="Data.html"><a href="Data.html#filtering-the-sample"><i class="fa fa-check"></i><b>5.5.4</b> Filtering the sample</a></li>
<li class="chapter" data-level="5.5.5" data-path="Data.html"><a href="Data.html#horizons"><i class="fa fa-check"></i><b>5.5.5</b> Return horizons</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="Data.html"><a href="Data.html#pers"><i class="fa fa-check"></i><b>5.6</b> Handling persistence</a></li>
<li class="chapter" data-level="5.7" data-path="Data.html"><a href="Data.html#extensions"><i class="fa fa-check"></i><b>5.7</b> Extensions</a><ul>
<li class="chapter" data-level="5.7.1" data-path="Data.html"><a href="Data.html#transforming-features"><i class="fa fa-check"></i><b>5.7.1</b> Transforming features</a></li>
<li class="chapter" data-level="5.7.2" data-path="Data.html"><a href="Data.html#macrovar"><i class="fa fa-check"></i><b>5.7.2</b> Macro-economic variables</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="Data.html"><a href="Data.html#additional-code-and-results"><i class="fa fa-check"></i><b>5.8</b> Additional code and results</a><ul>
<li class="chapter" data-level="5.8.1" data-path="Data.html"><a href="Data.html#impact-of-rescaling-graphical-representation"><i class="fa fa-check"></i><b>5.8.1</b> Impact of rescaling: graphical representation</a></li>
<li class="chapter" data-level="5.8.2" data-path="Data.html"><a href="Data.html#impact-of-rescaling-toy-example"><i class="fa fa-check"></i><b>5.8.2</b> Impact of rescaling: toy example</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="Data.html"><a href="Data.html#coding-exercises-1"><i class="fa fa-check"></i><b>5.9</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>6</b> Penalized regressions and sparse hedging for minimum variance portfolios</a><ul>
<li class="chapter" data-level="6.1" data-path="lasso.html"><a href="lasso.html#penalised-regressions"><i class="fa fa-check"></i><b>6.1</b> Penalised regressions</a><ul>
<li class="chapter" data-level="6.1.1" data-path="lasso.html"><a href="lasso.html#simple-regressions"><i class="fa fa-check"></i><b>6.1.1</b> Simple regressions</a></li>
<li class="chapter" data-level="6.1.2" data-path="lasso.html"><a href="lasso.html#forms-of-penalizations"><i class="fa fa-check"></i><b>6.1.2</b> Forms of penalizations</a></li>
<li class="chapter" data-level="6.1.3" data-path="lasso.html"><a href="lasso.html#illustrations"><i class="fa fa-check"></i><b>6.1.3</b> Illustrations</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="lasso.html"><a href="lasso.html#sparse-hedging-for-minimum-variance-portfolios"><i class="fa fa-check"></i><b>6.2</b> Sparse hedging for minimum variance portfolios</a><ul>
<li class="chapter" data-level="6.2.1" data-path="lasso.html"><a href="lasso.html#presentation-and-derivations"><i class="fa fa-check"></i><b>6.2.1</b> Presentation and derivations</a></li>
<li class="chapter" data-level="6.2.2" data-path="lasso.html"><a href="lasso.html#sparseex"><i class="fa fa-check"></i><b>6.2.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="lasso.html"><a href="lasso.html#predictive-regressions"><i class="fa fa-check"></i><b>6.3</b> Predictive regressions</a><ul>
<li class="chapter" data-level="6.3.1" data-path="lasso.html"><a href="lasso.html#literature-review-and-principle"><i class="fa fa-check"></i><b>6.3.1</b> Literature review and principle</a></li>
<li class="chapter" data-level="6.3.2" data-path="lasso.html"><a href="lasso.html#code-and-results"><i class="fa fa-check"></i><b>6.3.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lasso.html"><a href="lasso.html#coding-exercises-2"><i class="fa fa-check"></i><b>6.4</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>7</b> Tree-based methods</a><ul>
<li class="chapter" data-level="7.1" data-path="trees.html"><a href="trees.html#simple-trees"><i class="fa fa-check"></i><b>7.1</b> Simple trees</a><ul>
<li class="chapter" data-level="7.1.1" data-path="trees.html"><a href="trees.html#principle"><i class="fa fa-check"></i><b>7.1.1</b> Principle</a></li>
<li class="chapter" data-level="7.1.2" data-path="trees.html"><a href="trees.html#further-details-on-classification"><i class="fa fa-check"></i><b>7.1.2</b> Further details on classification</a></li>
<li class="chapter" data-level="7.1.3" data-path="trees.html"><a href="trees.html#pruning-criteria"><i class="fa fa-check"></i><b>7.1.3</b> Pruning criteria</a></li>
<li class="chapter" data-level="7.1.4" data-path="trees.html"><a href="trees.html#code-and-interpretation"><i class="fa fa-check"></i><b>7.1.4</b> Code and interpretation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="trees.html"><a href="trees.html#random-forests"><i class="fa fa-check"></i><b>7.2</b> Random forests</a><ul>
<li class="chapter" data-level="7.2.1" data-path="trees.html"><a href="trees.html#principle-1"><i class="fa fa-check"></i><b>7.2.1</b> Principle</a></li>
<li class="chapter" data-level="7.2.2" data-path="trees.html"><a href="trees.html#code-and-results-1"><i class="fa fa-check"></i><b>7.2.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="trees.html"><a href="trees.html#adaboost"><i class="fa fa-check"></i><b>7.3</b> Boosted trees: Adaboost</a><ul>
<li class="chapter" data-level="7.3.1" data-path="trees.html"><a href="trees.html#methodology"><i class="fa fa-check"></i><b>7.3.1</b> Methodology</a></li>
<li class="chapter" data-level="7.3.2" data-path="trees.html"><a href="trees.html#illustration"><i class="fa fa-check"></i><b>7.3.2</b> Illustration</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="trees.html"><a href="trees.html#boosted-trees-extreme-gradient-boosting"><i class="fa fa-check"></i><b>7.4</b> Boosted trees: extreme gradient boosting</a><ul>
<li class="chapter" data-level="7.4.1" data-path="trees.html"><a href="trees.html#managing-loss"><i class="fa fa-check"></i><b>7.4.1</b> Managing Loss</a></li>
<li class="chapter" data-level="7.4.2" data-path="trees.html"><a href="trees.html#penalisation"><i class="fa fa-check"></i><b>7.4.2</b> Penalisation</a></li>
<li class="chapter" data-level="7.4.3" data-path="trees.html"><a href="trees.html#aggregation"><i class="fa fa-check"></i><b>7.4.3</b> Aggregation</a></li>
<li class="chapter" data-level="7.4.4" data-path="trees.html"><a href="trees.html#tree-structure"><i class="fa fa-check"></i><b>7.4.4</b> Tree structure</a></li>
<li class="chapter" data-level="7.4.5" data-path="trees.html"><a href="trees.html#boostext"><i class="fa fa-check"></i><b>7.4.5</b> Extensions</a></li>
<li class="chapter" data-level="7.4.6" data-path="trees.html"><a href="trees.html#boostcode"><i class="fa fa-check"></i><b>7.4.6</b> Code and results</a></li>
<li class="chapter" data-level="7.4.7" data-path="trees.html"><a href="trees.html#instweight"><i class="fa fa-check"></i><b>7.4.7</b> Instance weighting</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="trees.html"><a href="trees.html#discussion"><i class="fa fa-check"></i><b>7.5</b> Discussion</a></li>
<li class="chapter" data-level="7.6" data-path="trees.html"><a href="trees.html#coding-exercises-3"><i class="fa fa-check"></i><b>7.6</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="NN.html"><a href="NN.html"><i class="fa fa-check"></i><b>8</b> Neural networks</a><ul>
<li class="chapter" data-level="8.1" data-path="NN.html"><a href="NN.html#the-original-perceptron"><i class="fa fa-check"></i><b>8.1</b> The original perceptron</a></li>
<li class="chapter" data-level="8.2" data-path="NN.html"><a href="NN.html#multilayer-perceptron"><i class="fa fa-check"></i><b>8.2</b> Multilayer perceptron</a><ul>
<li class="chapter" data-level="8.2.1" data-path="NN.html"><a href="NN.html#introduction-and-notations"><i class="fa fa-check"></i><b>8.2.1</b> Introduction and notations</a></li>
<li class="chapter" data-level="8.2.2" data-path="NN.html"><a href="NN.html#universal-approximation"><i class="fa fa-check"></i><b>8.2.2</b> Universal approximation</a></li>
<li class="chapter" data-level="8.2.3" data-path="NN.html"><a href="NN.html#backprop"><i class="fa fa-check"></i><b>8.2.3</b> Learning via back-propagation</a></li>
<li class="chapter" data-level="8.2.4" data-path="NN.html"><a href="NN.html#further-details-on-classification-1"><i class="fa fa-check"></i><b>8.2.4</b> Further details on classification</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="NN.html"><a href="NN.html#howdeep"><i class="fa fa-check"></i><b>8.3</b> How deep should we go? And other practical issues</a><ul>
<li class="chapter" data-level="8.3.1" data-path="NN.html"><a href="NN.html#architectural-choices"><i class="fa fa-check"></i><b>8.3.1</b> Architectural choices</a></li>
<li class="chapter" data-level="8.3.2" data-path="NN.html"><a href="NN.html#frequency-of-weight-updates-and-learning-duration"><i class="fa fa-check"></i><b>8.3.2</b> Frequency of weight updates and learning duration</a></li>
<li class="chapter" data-level="8.3.3" data-path="NN.html"><a href="NN.html#penalizations-and-dropout"><i class="fa fa-check"></i><b>8.3.3</b> Penalizations and dropout</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="NN.html"><a href="NN.html#code-samples-and-comments-for-vanilla-mlp"><i class="fa fa-check"></i><b>8.4</b> Code samples and comments for vanilla MLP</a><ul>
<li class="chapter" data-level="8.4.1" data-path="NN.html"><a href="NN.html#regression-example"><i class="fa fa-check"></i><b>8.4.1</b> Regression example</a></li>
<li class="chapter" data-level="8.4.2" data-path="NN.html"><a href="NN.html#classification-example"><i class="fa fa-check"></i><b>8.4.2</b> Classification example</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="NN.html"><a href="NN.html#recurrent-networks"><i class="fa fa-check"></i><b>8.5</b> Recurrent networks</a><ul>
<li class="chapter" data-level="8.5.1" data-path="NN.html"><a href="NN.html#presentation"><i class="fa fa-check"></i><b>8.5.1</b> Presentation</a></li>
<li class="chapter" data-level="8.5.2" data-path="NN.html"><a href="NN.html#code-and-results-2"><i class="fa fa-check"></i><b>8.5.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="NN.html"><a href="NN.html#other-common-architectures"><i class="fa fa-check"></i><b>8.6</b> Other common architectures</a><ul>
<li class="chapter" data-level="8.6.1" data-path="NN.html"><a href="NN.html#generative-aversarial-networks"><i class="fa fa-check"></i><b>8.6.1</b> Generative adversarial networks</a></li>
<li class="chapter" data-level="8.6.2" data-path="NN.html"><a href="NN.html#autoencoders"><i class="fa fa-check"></i><b>8.6.2</b> Auto-encoders</a></li>
<li class="chapter" data-level="8.6.3" data-path="NN.html"><a href="NN.html#a-word-on-convolutional-networks"><i class="fa fa-check"></i><b>8.6.3</b> A word on convolutional networks</a></li>
<li class="chapter" data-level="8.6.4" data-path="NN.html"><a href="NN.html#advanced-architectures"><i class="fa fa-check"></i><b>8.6.4</b> Advanced architectures</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="NN.html"><a href="NN.html#coding-exercise"><i class="fa fa-check"></i><b>8.7</b> Coding exercise</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>9</b> Support vector machines</a><ul>
<li class="chapter" data-level="9.1" data-path="svm.html"><a href="svm.html#svm-for-classification"><i class="fa fa-check"></i><b>9.1</b> SVM for classification</a></li>
<li class="chapter" data-level="9.2" data-path="svm.html"><a href="svm.html#svm-for-regression"><i class="fa fa-check"></i><b>9.2</b> SVM for regression</a></li>
<li class="chapter" data-level="9.3" data-path="svm.html"><a href="svm.html#practice"><i class="fa fa-check"></i><b>9.3</b> Practice</a></li>
<li class="chapter" data-level="9.4" data-path="svm.html"><a href="svm.html#coding-exercises-4"><i class="fa fa-check"></i><b>9.4</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bayes.html"><a href="bayes.html"><i class="fa fa-check"></i><b>10</b> Bayesian methods</a><ul>
<li class="chapter" data-level="10.1" data-path="bayes.html"><a href="bayes.html#the-bayesian-framework"><i class="fa fa-check"></i><b>10.1</b> The Bayesian framework</a></li>
<li class="chapter" data-level="10.2" data-path="bayes.html"><a href="bayes.html#bayesian-sampling"><i class="fa fa-check"></i><b>10.2</b> Bayesian sampling</a><ul>
<li class="chapter" data-level="10.2.1" data-path="bayes.html"><a href="bayes.html#gibbs-sampling"><i class="fa fa-check"></i><b>10.2.1</b> Gibbs sampling</a></li>
<li class="chapter" data-level="10.2.2" data-path="bayes.html"><a href="bayes.html#metropolis-hastings-sampling"><i class="fa fa-check"></i><b>10.2.2</b> Metropolis-Hastings sampling</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="bayes.html"><a href="bayes.html#bayesian-linear-regression"><i class="fa fa-check"></i><b>10.3</b> Bayesian linear regression</a></li>
<li class="chapter" data-level="10.4" data-path="bayes.html"><a href="bayes.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>10.4</b> Naive Bayes classifier</a></li>
<li class="chapter" data-level="10.5" data-path="bayes.html"><a href="bayes.html#BART"><i class="fa fa-check"></i><b>10.5</b> Bayesian additive trees</a><ul>
<li class="chapter" data-level="10.5.1" data-path="bayes.html"><a href="bayes.html#general-formulation"><i class="fa fa-check"></i><b>10.5.1</b> General formulation</a></li>
<li class="chapter" data-level="10.5.2" data-path="bayes.html"><a href="bayes.html#priors"><i class="fa fa-check"></i><b>10.5.2</b> Priors</a></li>
<li class="chapter" data-level="10.5.3" data-path="bayes.html"><a href="bayes.html#sampling-and-predictions"><i class="fa fa-check"></i><b>10.5.3</b> Sampling and predictions</a></li>
<li class="chapter" data-level="10.5.4" data-path="bayes.html"><a href="bayes.html#code"><i class="fa fa-check"></i><b>10.5.4</b> Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="valtune.html"><a href="valtune.html"><i class="fa fa-check"></i><b>11</b> Validating and tuning</a><ul>
<li class="chapter" data-level="11.1" data-path="valtune.html"><a href="valtune.html#mlmetrics"><i class="fa fa-check"></i><b>11.1</b> Learning metrics</a><ul>
<li class="chapter" data-level="11.1.1" data-path="valtune.html"><a href="valtune.html#regression-analysis"><i class="fa fa-check"></i><b>11.1.1</b> Regression analysis</a></li>
<li class="chapter" data-level="11.1.2" data-path="valtune.html"><a href="valtune.html#classification-analysis"><i class="fa fa-check"></i><b>11.1.2</b> Classification analysis</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="valtune.html"><a href="valtune.html#validation"><i class="fa fa-check"></i><b>11.2</b> Validation</a><ul>
<li class="chapter" data-level="11.2.1" data-path="valtune.html"><a href="valtune.html#the-variance-bias-tradeoff-theory"><i class="fa fa-check"></i><b>11.2.1</b> The variance-bias tradeoff: theory</a></li>
<li class="chapter" data-level="11.2.2" data-path="valtune.html"><a href="valtune.html#the-variance-bias-tradeoff-illustration"><i class="fa fa-check"></i><b>11.2.2</b> The variance-bias tradeoff: illustration</a></li>
<li class="chapter" data-level="11.2.3" data-path="valtune.html"><a href="valtune.html#the-risk-of-overfitting-principle"><i class="fa fa-check"></i><b>11.2.3</b> The risk of overfitting: principle</a></li>
<li class="chapter" data-level="11.2.4" data-path="valtune.html"><a href="valtune.html#the-risk-of-overfitting-some-solutions"><i class="fa fa-check"></i><b>11.2.4</b> The risk of overfitting: some solutions</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="valtune.html"><a href="valtune.html#the-search-for-good-hyperparameters"><i class="fa fa-check"></i><b>11.3</b> The search for good hyperparameters</a><ul>
<li class="chapter" data-level="11.3.1" data-path="valtune.html"><a href="valtune.html#methods"><i class="fa fa-check"></i><b>11.3.1</b> Methods</a></li>
<li class="chapter" data-level="11.3.2" data-path="valtune.html"><a href="valtune.html#example-grid-search"><i class="fa fa-check"></i><b>11.3.2</b> Example: grid search</a></li>
<li class="chapter" data-level="11.3.3" data-path="valtune.html"><a href="valtune.html#example-bayesian-optimization"><i class="fa fa-check"></i><b>11.3.3</b> Example: Bayesian optimization</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="valtune.html"><a href="valtune.html#short-discussion-on-validation-in-backtests"><i class="fa fa-check"></i><b>11.4</b> Short discussion on validation in backtests</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ensemble.html"><a href="ensemble.html"><i class="fa fa-check"></i><b>12</b> Ensemble models</a><ul>
<li class="chapter" data-level="12.1" data-path="ensemble.html"><a href="ensemble.html#linear-ensembles"><i class="fa fa-check"></i><b>12.1</b> Linear ensembles</a><ul>
<li class="chapter" data-level="12.1.1" data-path="ensemble.html"><a href="ensemble.html#principles"><i class="fa fa-check"></i><b>12.1.1</b> Principles</a></li>
<li class="chapter" data-level="12.1.2" data-path="ensemble.html"><a href="ensemble.html#example"><i class="fa fa-check"></i><b>12.1.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ensemble.html"><a href="ensemble.html#stacked-ensembles"><i class="fa fa-check"></i><b>12.2</b> Stacked ensembles</a><ul>
<li class="chapter" data-level="12.2.1" data-path="ensemble.html"><a href="ensemble.html#two-stage-training"><i class="fa fa-check"></i><b>12.2.1</b> Two stage training</a></li>
<li class="chapter" data-level="12.2.2" data-path="ensemble.html"><a href="ensemble.html#code-and-results-3"><i class="fa fa-check"></i><b>12.2.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ensemble.html"><a href="ensemble.html#extensions-1"><i class="fa fa-check"></i><b>12.3</b> Extensions</a><ul>
<li class="chapter" data-level="12.3.1" data-path="ensemble.html"><a href="ensemble.html#exogenous-variables"><i class="fa fa-check"></i><b>12.3.1</b> Exogenous variables</a></li>
<li class="chapter" data-level="12.3.2" data-path="ensemble.html"><a href="ensemble.html#shrinking-inter-model-correlations"><i class="fa fa-check"></i><b>12.3.2</b> Shrinking inter-model correlations</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="ensemble.html"><a href="ensemble.html#exercise"><i class="fa fa-check"></i><b>12.4</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="backtest.html"><a href="backtest.html"><i class="fa fa-check"></i><b>13</b> Portfolio backtesting</a><ul>
<li class="chapter" data-level="13.1" data-path="backtest.html"><a href="backtest.html#protocol"><i class="fa fa-check"></i><b>13.1</b> Setting the protocol</a></li>
<li class="chapter" data-level="13.2" data-path="backtest.html"><a href="backtest.html#turning-signals-into-portfolio-weights"><i class="fa fa-check"></i><b>13.2</b> Turning signals into portfolio weights</a></li>
<li class="chapter" data-level="13.3" data-path="backtest.html"><a href="backtest.html#perfmet"><i class="fa fa-check"></i><b>13.3</b> Performance metrics</a><ul>
<li class="chapter" data-level="13.3.1" data-path="backtest.html"><a href="backtest.html#discussion-1"><i class="fa fa-check"></i><b>13.3.1</b> Discussion</a></li>
<li class="chapter" data-level="13.3.2" data-path="backtest.html"><a href="backtest.html#pure-performance-and-risk-indicators"><i class="fa fa-check"></i><b>13.3.2</b> Pure performance and risk indicators</a></li>
<li class="chapter" data-level="13.3.3" data-path="backtest.html"><a href="backtest.html#factor-based-evaluation"><i class="fa fa-check"></i><b>13.3.3</b> Factor-based evaluation</a></li>
<li class="chapter" data-level="13.3.4" data-path="backtest.html"><a href="backtest.html#risk-adjusted-measures"><i class="fa fa-check"></i><b>13.3.4</b> Risk-adjusted measures</a></li>
<li class="chapter" data-level="13.3.5" data-path="backtest.html"><a href="backtest.html#transaction-costs-and-turnover"><i class="fa fa-check"></i><b>13.3.5</b> Transaction costs and turnover</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="backtest.html"><a href="backtest.html#common-errors-and-issues"><i class="fa fa-check"></i><b>13.4</b> Common errors and issues</a><ul>
<li class="chapter" data-level="13.4.1" data-path="backtest.html"><a href="backtest.html#forward-looking-data"><i class="fa fa-check"></i><b>13.4.1</b> Forward looking data</a></li>
<li class="chapter" data-level="13.4.2" data-path="backtest.html"><a href="backtest.html#backtest-overfitting"><i class="fa fa-check"></i><b>13.4.2</b> Backtest overfitting</a></li>
<li class="chapter" data-level="13.4.3" data-path="backtest.html"><a href="backtest.html#simple-saveguards"><i class="fa fa-check"></i><b>13.4.3</b> Simple saveguards</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="backtest.html"><a href="backtest.html#implication-of-non-stationarity-forecasting-is-hard"><i class="fa fa-check"></i><b>13.5</b> Implication of non-stationarity: forecasting is hard</a><ul>
<li class="chapter" data-level="13.5.1" data-path="backtest.html"><a href="backtest.html#general-comments"><i class="fa fa-check"></i><b>13.5.1</b> General comments</a></li>
<li class="chapter" data-level="13.5.2" data-path="backtest.html"><a href="backtest.html#the-no-free-lunch-theorem"><i class="fa fa-check"></i><b>13.5.2</b> The no free lunch theorem</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="backtest.html"><a href="backtest.html#example-1"><i class="fa fa-check"></i><b>13.6</b> Example</a></li>
<li class="chapter" data-level="13.7" data-path="backtest.html"><a href="backtest.html#coding-exercises-5"><i class="fa fa-check"></i><b>13.7</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="interp.html"><a href="interp.html"><i class="fa fa-check"></i><b>14</b> Interpretability</a><ul>
<li class="chapter" data-level="14.1" data-path="interp.html"><a href="interp.html#global-interpretations"><i class="fa fa-check"></i><b>14.1</b> Global interpretations</a><ul>
<li class="chapter" data-level="14.1.1" data-path="interp.html"><a href="interp.html#variable-importance"><i class="fa fa-check"></i><b>14.1.1</b> Variable importance (tree-based)</a></li>
<li class="chapter" data-level="14.1.2" data-path="interp.html"><a href="interp.html#variable-importance-agnostic"><i class="fa fa-check"></i><b>14.1.2</b> Variable importance (agnostic)</a></li>
<li class="chapter" data-level="14.1.3" data-path="interp.html"><a href="interp.html#partial-dependence-plot"><i class="fa fa-check"></i><b>14.1.3</b> Partial dependence plot</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="interp.html"><a href="interp.html#local-interpretations"><i class="fa fa-check"></i><b>14.2</b> Local interpretations</a><ul>
<li class="chapter" data-level="14.2.1" data-path="interp.html"><a href="interp.html#lime"><i class="fa fa-check"></i><b>14.2.1</b> LIME</a></li>
<li class="chapter" data-level="14.2.2" data-path="interp.html"><a href="interp.html#shapley-values"><i class="fa fa-check"></i><b>14.2.2</b> Shapley values</a></li>
<li class="chapter" data-level="14.2.3" data-path="interp.html"><a href="interp.html#breakdown"><i class="fa fa-check"></i><b>14.2.3</b> Breakdown</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="causality.html"><a href="causality.html"><i class="fa fa-check"></i><b>15</b> Two key concepts: causality and non-stationarity</a><ul>
<li class="chapter" data-level="15.1" data-path="causality.html"><a href="causality.html#causality-1"><i class="fa fa-check"></i><b>15.1</b> Causality</a><ul>
<li class="chapter" data-level="15.1.1" data-path="causality.html"><a href="causality.html#granger"><i class="fa fa-check"></i><b>15.1.1</b> Granger causality</a></li>
<li class="chapter" data-level="15.1.2" data-path="causality.html"><a href="causality.html#causal-additive-models"><i class="fa fa-check"></i><b>15.1.2</b> Causal additive models</a></li>
<li class="chapter" data-level="15.1.3" data-path="causality.html"><a href="causality.html#structural-time-series-models"><i class="fa fa-check"></i><b>15.1.3</b> Structural time-series models</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="causality.html"><a href="causality.html#nonstat"><i class="fa fa-check"></i><b>15.2</b> Dealing with changing environments</a><ul>
<li class="chapter" data-level="15.2.1" data-path="causality.html"><a href="causality.html#non-stationarity-an-obvious-illustration"><i class="fa fa-check"></i><b>15.2.1</b> Non-stationarity: an obvious illustration</a></li>
<li class="chapter" data-level="15.2.2" data-path="causality.html"><a href="causality.html#online-learning"><i class="fa fa-check"></i><b>15.2.2</b> Online learning</a></li>
<li class="chapter" data-level="15.2.3" data-path="causality.html"><a href="causality.html#homogeneous-transfer-learning"><i class="fa fa-check"></i><b>15.2.3</b> Homogeneous transfer learning</a></li>
<li class="chapter" data-level="15.2.4" data-path="causality.html"><a href="causality.html#active-learning"><i class="fa fa-check"></i><b>15.2.4</b> Active learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="unsup.html"><a href="unsup.html"><i class="fa fa-check"></i><b>16</b> Unsupervised learning</a><ul>
<li class="chapter" data-level="16.1" data-path="unsup.html"><a href="unsup.html#corpred"><i class="fa fa-check"></i><b>16.1</b> The problem with correlated predictors</a></li>
<li class="chapter" data-level="16.2" data-path="unsup.html"><a href="unsup.html#principal-component-analysis-and-autoencoders"><i class="fa fa-check"></i><b>16.2</b> Principal component analysis and autoencoders</a><ul>
<li class="chapter" data-level="16.2.1" data-path="unsup.html"><a href="unsup.html#a-bit-of-algebra"><i class="fa fa-check"></i><b>16.2.1</b> A bit of algebra</a></li>
<li class="chapter" data-level="16.2.2" data-path="unsup.html"><a href="unsup.html#pca"><i class="fa fa-check"></i><b>16.2.2</b> PCA</a></li>
<li class="chapter" data-level="16.2.3" data-path="unsup.html"><a href="unsup.html#ae"><i class="fa fa-check"></i><b>16.2.3</b> Autoencoders</a></li>
<li class="chapter" data-level="16.2.4" data-path="unsup.html"><a href="unsup.html#application"><i class="fa fa-check"></i><b>16.2.4</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="unsup.html"><a href="unsup.html#clustering-via-k-means"><i class="fa fa-check"></i><b>16.3</b> Clustering via k-means</a></li>
<li class="chapter" data-level="16.4" data-path="unsup.html"><a href="unsup.html#nearest-neighbors"><i class="fa fa-check"></i><b>16.4</b> Nearest neighbors</a></li>
<li class="chapter" data-level="16.5" data-path="unsup.html"><a href="unsup.html#coding-exercise-1"><i class="fa fa-check"></i><b>16.5</b> Coding exercise</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="RL.html"><a href="RL.html"><i class="fa fa-check"></i><b>17</b> Reinforcement learning</a><ul>
<li class="chapter" data-level="17.1" data-path="RL.html"><a href="RL.html#theoretical-layout"><i class="fa fa-check"></i><b>17.1</b> Theoretical layout</a><ul>
<li class="chapter" data-level="17.1.1" data-path="RL.html"><a href="RL.html#general-framework"><i class="fa fa-check"></i><b>17.1.1</b> General framework</a></li>
<li class="chapter" data-level="17.1.2" data-path="RL.html"><a href="RL.html#q-learning"><i class="fa fa-check"></i><b>17.1.2</b> Q-learning</a></li>
<li class="chapter" data-level="17.1.3" data-path="RL.html"><a href="RL.html#sarsa"><i class="fa fa-check"></i><b>17.1.3</b> SARSA</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="RL.html"><a href="RL.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>17.2</b> The curse of dimensionality</a></li>
<li class="chapter" data-level="17.3" data-path="RL.html"><a href="RL.html#policy-gradient"><i class="fa fa-check"></i><b>17.3</b> Policy gradient</a><ul>
<li class="chapter" data-level="17.3.1" data-path="RL.html"><a href="RL.html#principle-2"><i class="fa fa-check"></i><b>17.3.1</b> Principle</a></li>
<li class="chapter" data-level="17.3.2" data-path="RL.html"><a href="RL.html#extensions-2"><i class="fa fa-check"></i><b>17.3.2</b> Extensions</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="RL.html"><a href="RL.html#simple-examples"><i class="fa fa-check"></i><b>17.4</b> Simple examples</a><ul>
<li class="chapter" data-level="17.4.1" data-path="RL.html"><a href="RL.html#q-learning-with-simulations"><i class="fa fa-check"></i><b>17.4.1</b> Q-learning with simulations</a></li>
<li class="chapter" data-level="17.4.2" data-path="RL.html"><a href="RL.html#RLemp2"><i class="fa fa-check"></i><b>17.4.2</b> Q-learning with market data</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="RL.html"><a href="RL.html#concluding-remarks"><i class="fa fa-check"></i><b>17.5</b> Concluding remarks</a></li>
<li class="chapter" data-level="17.6" data-path="RL.html"><a href="RL.html#exercises"><i class="fa fa-check"></i><b>17.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="NLP.html"><a href="NLP.html"><i class="fa fa-check"></i><b>18</b> Natural Language Processing</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="data-description.html"><a href="data-description.html"><i class="fa fa-check"></i><b>A</b> Data Description</a></li>
<li class="chapter" data-level="B" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html"><i class="fa fa-check"></i><b>B</b> Solution to exercises</a><ul>
<li class="chapter" data-level="B.1" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-4"><i class="fa fa-check"></i><b>B.1</b> Chapter 4</a></li>
<li class="chapter" data-level="B.2" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-5"><i class="fa fa-check"></i><b>B.2</b> Chapter 5</a></li>
<li class="chapter" data-level="B.3" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-6"><i class="fa fa-check"></i><b>B.3</b> Chapter 6</a></li>
<li class="chapter" data-level="B.4" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-7"><i class="fa fa-check"></i><b>B.4</b> Chapter 7</a></li>
<li class="chapter" data-level="B.5" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-8-the-autoencoder-model"><i class="fa fa-check"></i><b>B.5</b> Chapter 8: the autoencoder model</a></li>
<li class="chapter" data-level="B.6" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-9"><i class="fa fa-check"></i><b>B.6</b> Chapter 9</a></li>
<li class="chapter" data-level="B.7" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-12-ensemble-neural-network"><i class="fa fa-check"></i><b>B.7</b> Chapter 12: ensemble neural network</a></li>
<li class="chapter" data-level="B.8" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-13"><i class="fa fa-check"></i><b>B.8</b> Chapter 13</a><ul>
<li class="chapter" data-level="B.8.1" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#advanced-weighting-function"><i class="fa fa-check"></i><b>B.8.1</b> Advanced weighting function</a></li>
<li class="chapter" data-level="B.8.2" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#functional-programming-in-the-backtest"><i class="fa fa-check"></i><b>B.8.2</b> Functional programming in the backtest</a></li>
</ul></li>
<li class="chapter" data-level="B.9" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-17"><i class="fa fa-check"></i><b>B.9</b> Chapter 17</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>C</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Factor Investing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="Data" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Data preprocessing</h1>
<p>The methods we describe in this chapter are driven by financial applications. For an introduction to non-financial data processing, we recommend two references: Chapter 3 from the general purpose ML book <span class="citation">Boehmke and Greenwell (<a href="#ref-boehmke2019hands">2019</a>)</span> and the monograph on this dedicated subject: <span class="citation">Kuhn and Johnson (<a href="#ref-kuhn2019feature">2019</a>)</span>.</p>
<div id="know-your-data" class="section level2">
<h2><span class="header-section-number">5.1</span> Know your data</h2>
<p>The first step, as in any quantitative study, is obviously to make sure the data is trustworthy, i.e., comes from a reliable provider. The landscape in financial data provision is vast to say the least: some providers are well established (e.g., Bloomberg, Thomson-Reuters, Datastream, CRSP, Morningstar), some are more recent (e.g., Capital IQ, Ravenpack) and some focus on alternative data niches (see <a href="https://alternativedata.org/data-providers/" class="uri">https://alternativedata.org/data-providers/</a> for an exhaustive list). Unfortunately, and to the best of our knowledge, no study has been published that evaluates a large spectrum of these providers in terms of data reliability.</p>
<p>The second step is to have a look at <strong>summary statistics</strong>: ranges (minimum and maximum values), and averages and medians. Histograms or plots of time-series carry of course more information but cannot be analyzed properly in high dimensions. They are nonetheless sometimes useful to track local patterns or errors for a given stock and a particular feature.
Beyond first order moments, second order quantities (variances and covariances/correlations) also matter because they help spot colinearities. When two features are highly correlated, problems may arise in some models (e.g., simple regressions, see Section <a href="unsup.html#corpred">16.1</a>).</p>
<p>Often, the number of predictors is so large that it is unpractical to look at these simple metrics. A minimal verification is recommended. To further ease the analysis:</p>
<ul>
<li>focus on a subset of predictors, e.g., the ones linked to the most common factors (market-capitalization, price-to-book or book-to-market, momentum (past returns), profitability, asset growth, volatility);<br />
</li>
<li>track outliers in the summary statistics (when the maximum/median or median/minimum ratios seem suspicious).</li>
</ul>
<p>Below, in Figure <a href="Data.html#fig:boxcorr">5.1</a>, we show a box-plot that illustrates the distribution of correlations between features and the one month ahead return. The correlations are computed on a date-by-date basis, over the whole cross-section of stocks. They are mostly located close to zero, but some dates seem to experience extreme shifts (outliers are shown with black circles). The market capitalization has the median which is the most negative while volatility is the only predictor with positive median correlation (this particular example seems to refute the low risk anomaly).</p>

<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb21-1" data-line-number="1">data_ml <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb21-2" data-line-number="2"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="kw">c</span>(features_short, <span class="st">&quot;R1M_Usd&quot;</span>, <span class="st">&quot;date&quot;</span>)) <span class="op">%&gt;%</span><span class="st">   </span><span class="co"># Keep a few feature, label &amp; date</span></a>
<a class="sourceLine" id="cb21-3" data-line-number="3"><span class="st">    </span><span class="kw">group_by</span>(date) <span class="op">%&gt;%</span><span class="st">                                        </span><span class="co"># Group: dates!</span></a>
<a class="sourceLine" id="cb21-4" data-line-number="4"><span class="st">    </span><span class="kw">summarise_all</span>(<span class="kw">funs</span>(<span class="kw">cor</span>(.,R1M_Usd))) <span class="op">%&gt;%</span><span class="st">                   </span><span class="co"># Compute correlations</span></a>
<a class="sourceLine" id="cb21-5" data-line-number="5"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="op">-</span>R1M_Usd) <span class="op">%&gt;%</span><span class="st">                               </span><span class="co"># Remove label</span></a>
<a class="sourceLine" id="cb21-6" data-line-number="6"><span class="st">    </span><span class="kw">gather</span>(<span class="dt">key =</span> key, <span class="dt">value =</span> value, <span class="op">-</span>date) <span class="op">%&gt;%</span><span class="st">               </span><span class="co"># Put in tidy format</span></a>
<a class="sourceLine" id="cb21-7" data-line-number="7"><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> key, <span class="dt">y =</span> value, <span class="dt">color =</span> key)) <span class="op">+</span><span class="st">            </span><span class="co"># Plot</span></a>
<a class="sourceLine" id="cb21-8" data-line-number="8"><span class="st">    </span><span class="kw">geom_boxplot</span>(<span class="dt">outlier.colour =</span> <span class="st">&quot;black&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">coord_flip</span>() <span class="op">+</span><span class="st"> </span><span class="kw">theme</span>(<span class="dt">aspect.ratio =</span> <span class="fl">0.6</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:boxcorr"></span>
<img src="ML_factor_files/figure-html/boxcorr-1.png" alt="Boxplot of correlations with the 1M forward return / label." width="400px" />
<p class="caption">
FIGURE 5.1: Boxplot of correlations with the 1M forward return / label.
</p>
</div>
<p></p>
<p>More importantly, when seeking to work with supervised learning (as we will do most of the time), the link of some features with the dependent variable can be further characterized by the smoothed <strong>conditional average</strong> because it shows how the features impact the label. The use of the conditional average has a deep theoretical grounding. Suppose there is only one feature <span class="math inline">\(X\)</span> and that we seek a model <span class="math inline">\(Y=f(X)+\text{error}\)</span>, where variables are real-valued. The function <span class="math inline">\(f\)</span> that minimizes the average squared error <span class="math inline">\(\mathbb{E}[(Y-f(X))^2]\)</span> is the so-called regression function (see Section 2.4 in <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-friedman2009elements">2009</a>)</span>):
<span class="math display" id="eq:regfun">\[\begin{equation}
\tag{5.1}
f(x)=\mathbb{E}[Y|X=x].
\end{equation}\]</span></p>
<p>In Figure <a href="Data.html#fig:regfun">5.2</a>, we plot two illustrations of this function when the dependent variable is the one month ahead return. The first one pertains to the average market capitalization over the past year and the second to the volatility over the past year as well. Both predictors have been uniformized (see Section <a href="Data.html#scaling">5.4.2</a> below) so that their values are uniformly distributed in the cross-section of assets for any given time period. Thus, the range of features is <span class="math inline">\([0,1]\)</span> and is shown on the <span class="math inline">\(x\)</span>-axis of the plot. The grey corridors around the lines show 95% level confidence interval for the computation of the mean. Essentially, it is narrow when both i) many data points are available and ii) these points are not too dispersed.</p>

<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" data-line-number="1">data_ml <span class="op">%&gt;%</span><span class="st">                                                      </span><span class="co"># From dataset:</span></a>
<a class="sourceLine" id="cb22-2" data-line-number="2"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">y =</span> R1M_Usd)) <span class="op">+</span><span class="st">                                     </span><span class="co"># Plot</span></a>
<a class="sourceLine" id="cb22-3" data-line-number="3"><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Mkt_Cap_12M_Usd, <span class="dt">color =</span> <span class="st">&quot;Market Cap&quot;</span>)) <span class="op">+</span><span class="st">  </span><span class="co"># Cond. Exp. Mkt_cap</span></a>
<a class="sourceLine" id="cb22-4" data-line-number="4"><span class="st">  </span><span class="kw">geom_smooth</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Vol1Y_Usd, <span class="dt">color =</span> <span class="st">&quot;Volatility&quot;</span>)) <span class="op">+</span><span class="st">        </span><span class="co"># Cond. Exp. Vol</span></a>
<a class="sourceLine" id="cb22-5" data-line-number="5"><span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">values=</span><span class="kw">c</span>(<span class="st">&quot;#F87E1F&quot;</span>, <span class="st">&quot;#0570EA&quot;</span>)) <span class="op">+</span><span class="st">           </span><span class="co"># Change color</span></a>
<a class="sourceLine" id="cb22-6" data-line-number="6"><span class="st">  </span><span class="kw">coord_fixed</span>(<span class="dv">10</span>)                                                <span class="co"># Change x/y ratio</span></a></code></pre></div>
<div class="figure"><span id="fig:regfun"></span>
<img src="ML_factor_files/figure-html/regfun-1.png" alt="Conditional expecations: average returns as smooth functions of features." width="672" />
<p class="caption">
FIGURE 5.2: Conditional expecations: average returns as smooth functions of features.
</p>
</div>
<p></p>
<p>The two variables have a close to monotonic impact on future returns. Returns, on average, decrease with market capitalization (thereby corroborating the so-called <em>size</em> effect). The reverse pattern is less pronounced for volatility: the curve is rather flat is the first half of volatility scores and progressively increases, especially over the last quintile of volatility values (thereby contradicting the low-vol anomaly).</p>
<p>One important empirical property of features is <strong>autocorrelation</strong> (or absence thereof). A high level of autocorrelation for one predictors makes it plausible to use simple imputation techniques. But autocorrelation is also important when moving towards prediction tasks and we discuss this issue shortly below in Section <a href="Data.html#pers">5.6</a>. In Figure <a href="Data.html#fig:histcorr">5.3</a>, we build the histogram of autocorrelations, computed stock by stock and feature by feature.</p>

<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb23-1" data-line-number="1">autocorrs &lt;-<span class="st"> </span>data_ml <span class="op">%&gt;%</span><span class="st">                                         </span><span class="co"># From dataset:</span></a>
<a class="sourceLine" id="cb23-2" data-line-number="2"><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="kw">c</span>(<span class="st">&quot;stock_id&quot;</span>, features)) <span class="op">%&gt;%</span><span class="st">                     </span><span class="co"># Keep ids &amp; features</span></a>
<a class="sourceLine" id="cb23-3" data-line-number="3"><span class="st">  </span><span class="kw">gather</span>(<span class="dt">key =</span> feature, <span class="dt">value =</span> value, <span class="op">-</span>stock_id) <span class="op">%&gt;%</span><span class="st">            </span><span class="co"># Put in tidy format</span></a>
<a class="sourceLine" id="cb23-4" data-line-number="4"><span class="st">  </span><span class="kw">group_by</span>(stock_id, feature) <span class="op">%&gt;%</span><span class="st">                                </span><span class="co"># Group</span></a>
<a class="sourceLine" id="cb23-5" data-line-number="5"><span class="st">  </span><span class="kw">summarize</span>(<span class="dt">acf =</span> <span class="kw">acf</span>(value, <span class="dt">lag.max =</span> <span class="dv">1</span>, <span class="dt">plot =</span> <span class="ot">FALSE</span>)<span class="op">$</span>acf[<span class="dv">2</span>])  <span class="co"># Compute ACF</span></a>
<a class="sourceLine" id="cb23-6" data-line-number="6">autocorrs <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> acf)) <span class="op">+</span><span class="st">                             </span><span class="co"># Plot</span></a>
<a class="sourceLine" id="cb23-7" data-line-number="7"><span class="st">   </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">60</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme</span>(<span class="dt">aspect.ratio =</span> <span class="fl">0.3</span>) <span class="op">+</span><span class="st"> </span><span class="kw">theme_grey</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:histcorr"></span>
<img src="ML_factor_files/figure-html/histcorr-1.png" alt="Histogram of sample feature autocorrelations." width="384" />
<p class="caption">
FIGURE 5.3: Histogram of sample feature autocorrelations.
</p>
</div>
<p></p>
<p>Given the large number of values to evaluate, the above chunk is quite time-consuming. The output shows that predictors are highly autocorrelated: most of them have a first order autocorrelation above 0.80.</p>
</div>
<div id="missing-data" class="section level2">
<h2><span class="header-section-number">5.2</span> Missing data</h2>
<p>Similarly to any empirical discipline, portfolio management is bound to face missing data issues. The topic is well known and several books detail solutions to this problem (e.g., <span class="citation">Allison (<a href="#ref-allison2001missing">2001</a>)</span>, <span class="citation">Enders (<a href="#ref-enders2010applied">2010</a>)</span>, <span class="citation">Little and Rubin (<a href="#ref-little2014statistical">2014</a>)</span> and <span class="citation">Van Buuren (<a href="#ref-van2018flexible">2018</a>)</span>). While researchers continuously propose new methods to cope with absent points (<span class="citation">Honaker and King (<a href="#ref-honaker2010missing">2010</a>)</span> or <span class="citation">Che et al. (<a href="#ref-che2018recurrent">2018</a>)</span> to cite but a few), we believe that a simple, heuristic, treatment is usually sufficient as long as some basic cautious safeguards are enforced.</p>
<p>First of all, there are mainly two ways to deal with missing data: removal and imputation. Removal is agnostic but costly, especially if one whole instance is eliminated because of only one missing feature. Imputation is often prefered but relies on some underlying and potentially erroneous assumption.</p>
<p>A simplified classification of imputation is the following:</p>
<ul>
<li>A basic imputation choice is the median (or mean) of the feature for the stock over the past available values. If there is a trend in the time series, this will nonetheless alter the trend. Relatedly, this method is forward looking.<br />
</li>
<li>In time-series contexts with views towards backtesting, the most simple imputation comes from previous values: if <span class="math inline">\(x_t\)</span> is missing, replace it with <span class="math inline">\(x_{t-1}\)</span>. This makes sense most of the time because past values are all that is available and are by definition backward looking. However, in some particular cases, this be a very bad choice (see words of caution below).</li>
<li>Medians and means can also be computed over the cross-section of assets. This roughly implies that the missing feature value will be relocated in the bulk of observed values. When many values are missing, this creates an atom in the distribution of the feature and alters the original distribution. One advantage is that this imputation is not forward looking.<br />
</li>
<li>Many techniques rely on some modelling assumptions for the data generating process. We refer to nonparametric approaches (<span class="citation">Stekhoven and Bühlmann (<a href="#ref-stekhoven2011missforest">2011</a>)</span> and <span class="citation">Shah et al. (<a href="#ref-shah2014comparison">2014</a>)</span>, which both rely on random forests, see Chapter <a href="trees.html#trees">7</a>), Bayesian imputation (<span class="citation">Schafer (<a href="#ref-schafer1999multiple">1999</a>)</span>), maximum likelihood approaches (<span class="citation">Enders (<a href="#ref-enders2001primer">2001</a>)</span>, <span class="citation">Enders (<a href="#ref-enders2010applied">2010</a>)</span>), interpolation or extrapolation and nearest neighbor algorithms (<span class="citation">Garcı'a-Laencina et al. (<a href="#ref-garcia2009k">2009</a>)</span>). More generally, the four books cited at the begining of the subsection detail many such imputation processes. Advanced techniques are much more demanding computationally.</li>
</ul>
<p>A few words of caution:</p>
<ul>
<li>Interpolation should be avoided at all cost. Accounting values or ratios that are released every quarter must never be linearly interpolated for the simple reason that this is forward looking. If numbers are disclosed in January and April, then interpolating February and March requires the knowledge of the April figure, which, in live trading will not be known. Resorting to past values is a better way to go.<br />
</li>
<li>Nevertheless, there are some feature types for which imputation from past values should be avoided. First of all, returns should not be replicated. By default, a superior choice is to set missing return indicators to zero (which is often close to the average or the median). A good indicator that can help the decision is the persistence of the feature through time. If it is highly autocorrelated (and the time-series plot create a smooth curve, like for marjet capitalization), then imputation from the past can make sense. If not, then it should be avoided.<br />
</li>
<li>There are some cases that can require more attention. Let us consider the following fictitious sample of dividend yield:</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Date</th>
<th>Original yield</th>
<th>Replacement value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2015-02</td>
<td>NA</td>
<td>(preceding (if it exists)</td>
</tr>
<tr class="even">
<td>2015-03</td>
<td>0.02</td>
<td>untouched (none)</td>
</tr>
<tr class="odd">
<td>2015-04</td>
<td>NA</td>
<td>0.02 (previous)</td>
</tr>
<tr class="even">
<td>2015-05</td>
<td>NA</td>
<td>0.02 (previous)</td>
</tr>
<tr class="odd">
<td>2015-06</td>
<td>NA</td>
<td><span class="math inline">\(\leftarrow\)</span> !</td>
</tr>
</tbody>
</table>
<p>In this case, the yield is released quarterly, in March, June, September, etc. But in June, the value is missing. The problem is that we cannot know if it is missing because of a genuine data glitch, or because the firm simply did not pay any dividends in June. Thus, imputation from past value may be erroneous here. There is no perfect solution but a decision must nevertheless be taken. For dividend data, three options are:</p>
<ol style="list-style-type: decimal">
<li>Keep the previous value. In R, the function na.locf() from the <em>zoo</em> package is incredibly efficient for this task.<br />
</li>
<li>Extrapolate from previous observations (this is very different from <strong>inter</strong>polation): for instance, evaluate a trend and pursue that trend.<br />
</li>
<li>Set the value to zero. This is tempting but may be sub-optimal due to dividend smoothing practices from executives (see for instance <span class="citation">Leary and Michaely (<a href="#ref-leary2011determinants">2011</a>)</span> and <span class="citation">Chen, Da, and Priestley (<a href="#ref-chen2012dividend">2012</a>)</span> for details on the subject). For persistent time-series, the first two options are probably better.</li>
</ol>
<p>Tests can be perform to evaluate the relative performance of each option. It is also important to <strong>remember</strong> these design choices. There are so many of them that they are easy to forget. Keeping track of them is obviously compulsory. In the ML pipeline, the scripts pertaining to data preparation are often key because they do not serve only once!</p>
</div>
<div id="outlier-detection" class="section level2">
<h2><span class="header-section-number">5.3</span> Outlier detection</h2>
<p>The topic of outlier detection is also well documented and has its own surveys (<span class="citation">Hodge and Austin (<a href="#ref-hodge2004survey">2004</a>)</span>, <span class="citation">Chandola, Banerjee, and Kumar (<a href="#ref-chandola2009anomaly">2009</a>)</span> and <span class="citation">Gupta et al. (<a href="#ref-gupta2014outlier">2014</a>)</span>) and a few dedicated books (<span class="citation">Aggarwal (<a href="#ref-aggarwal2013outlier">2013</a>)</span> and <span class="citation">Rousseeuw and Leroy (<a href="#ref-rousseeuw2005robust">2005</a>)</span>, though the latter is very focused on regression analysis).</p>
<p>Again, incredibly sophisticated methods may require a lot of effort for possibly limited gain. Simple heuristic methods, as long as they are documented in the process may suffice. They often rely on ‘hard’ thresholds:</p>
<ul>
<li>for one given feature (possibly filtered in time), any point outside the interval <span class="math inline">\([\mu-m\sigma, \mu+m\sigma]\)</span> can be deemed an outlier. Here <span class="math inline">\(\mu\)</span> is the mean of the sample and <span class="math inline">\(\sigma\)</span> the standard deviation. The multiple value <span class="math inline">\(m\)</span> usually belongs to the set <span class="math inline">\(\{3, 5, 10\}\)</span>, which is of course arbitrary.</li>
<li>likewise, if the largest value is above <span class="math inline">\(m\)</span> times the second-to-largest, then is can also be classified as an outlier (the same reasoning applied for the other side of the tail).</li>
<li>finally, for a given small threshold <span class="math inline">\(q\)</span>, any value outside the <span class="math inline">\([q,1-q]\)</span> quantile range can be considered outliers.</li>
</ul>
<p>This latter idea was popularized by winsorization. Winsorizing amounts to setting to <span class="math inline">\(x^{(q)}\)</span> all values below <span class="math inline">\(x^{(q)}\)</span> and to <span class="math inline">\(x^{(1-q)}\)</span> all values above <span class="math inline">\(x^{(1-q)}\)</span>. The winsorised variable <span class="math inline">\(\tilde{x}\)</span> is:
<span class="math display">\[\tilde{x}_i=\left\{\begin{array}{ll}
x_i &amp; \text{ if }  x_i \in [x^{(q)},x^{(1-q)}] \quad \text{ (unchanged)}\\
x^{(q)} &amp; \text{ if }  x_i &lt; x^{(q)} \\
x^{(1-q)} &amp; \text{ if }  x_i &gt; x^{(1-q)}
 \end{array} \right. .\]</span></p>
<p>The range for <span class="math inline">\(q\)</span> is usually <span class="math inline">\((0.5\%, 5\%)\)</span> with 1% and 2% being the most often used.</p>
<p>The winsorization stage must be performed on a feature-by-feature and a data-by-date basis. However, keeping a time-series perspective is also useful. For instance, a 800B$ market capitalization may seems out of range, except when looking at the history of Apple’s capitalization.</p>
<p>We conclude this subsection by recalling that <em>true</em> outliers (i.e, extreme points that are not due to data extraction errors) are valuable because they are likely to carry important information.</p>
</div>
<div id="feateng" class="section level2">
<h2><span class="header-section-number">5.4</span> Feature engineering</h2>
<p>Feature engineering is a very important step of the portfolio construction process. Computer scientistic often refer to the saying “garbage in, garbage out”. It is thus paramount to prevent the ML engine of the allocation to be trained on ill-designed variables.
We invite the interested reader to have a look at the recent work of <span class="citation">Kuhn and Johnson (<a href="#ref-kuhn2019feature">2019</a>)</span> on this topic. The (shorter) academic reference is <span class="citation">Guyon and Elisseeff (<a href="#ref-guyon2003introduction">2003</a>)</span>.</p>
<div id="feature-selection" class="section level3">
<h3><span class="header-section-number">5.4.1</span> Feature selection</h3>
<p>The first step is selection. Given a large set of predictors, it seems a sound idea to filter out unwanted or redundant exogenous variables. Heuristically, simple methods include:</p>
<ul>
<li>computing the correlation matrix of all features and making sure that no (absolute) value is above a threshold (0.7 is a common value) so that redundant variables do not pollute the learning engine;<br />
</li>
<li>carrying out a linear regression and removing the non significant variables (e.g., those with <span class="math inline">\(p\)</span>-value above 0.05).</li>
</ul>
<p>Both these methods are somewhat reductive and overlook nonlinear relationships. Another approach would be to fit a decision tree (or a random forest) and retain only the features that have a high variable importance. These topics will be developed in Chapter <a href="trees.html#trees">7</a>.</p>
</div>
<div id="scaling" class="section level3">
<h3><span class="header-section-number">5.4.2</span> Scaling the predictors</h3>
<p>The premise of the need to pre-process the data comes from the large variety of scales in financial data:</p>
<ul>
<li>returns are most of the time smaller than one in absolute value;</li>
<li>stock volatility lies usually between 5% and 80%;</li>
<li>market capitalisation is expressed in million or billion units of a particular currency;</li>
<li>accounting values as well;</li>
<li>accounting ratios have inhomogeneous units;</li>
<li>synthetic attributes like sentiment also have their idiosyncrasies.</li>
</ul>
<p>While it is widely considered that monotonic transformation of the features have a marginal impact on prediction outcomes, <span class="citation">Galili and Meilijson (<a href="#ref-galili2016splitting">2016</a>)</span> show that this is not always the case (see also section <a href="Data.html#impact-of-rescaling-toy-example">5.8.2</a>). Hence, the choice of normalisation may in fact very well matter.</p>
<p>If we write <span class="math inline">\(x_i\)</span> for the raw input and <span class="math inline">\(\tilde{x}_i\)</span> for the transformed data, common scaling practices include:</p>
<ul>
<li><strong>standardization</strong>: <span class="math inline">\(\tilde{x}_i=(x_i-m_x)/\sigma_x\)</span>, where <span class="math inline">\(m_x\)</span> and <span class="math inline">\(\sigma_x\)</span> are the mean and standard deviation of <span class="math inline">\(x\)</span>, respectively;</li>
<li><strong>min-max</strong> rescaling over [0,1]: <span class="math inline">\(\tilde{x}_i=(x_i-\min(\mathbf{x}))/(\max(\mathbf{x})-\min(\mathbf{x}))\)</span>;</li>
<li><strong>min-max</strong> rescaling over [-1,1]: <span class="math inline">\(\tilde{x}_i=2\frac{x_i-\min(\mathbf{x})}{\max(\mathbf{x})-\min(\mathbf{x})}-1\)</span>;</li>
<li><strong>uniformization</strong>: <span class="math inline">\(\tilde{x}_i=F_\mathbf{x}(x_i)\)</span>, where <span class="math inline">\(F_\mathbf{x}\)</span> is the empirical c.d.f. of <span class="math inline">\(\mathbf{x}\)</span>. In this case, the vector <span class="math inline">\(\tilde{\mathbf{x}}\)</span> is defined to follow a uniform distribution over [0,1].</li>
</ul>
<p>Sometimes, it is possible to apply a logarithmic transform of variables with both large values (market capitalization) and large outliers. The scaling can come after this transformation. Obviously, this technique is prohibited for features with negative values.</p>
<p>It is often advised to scale inputs so that they range in [0,1] before sending them through the training of neural networks for instance. The dataset that we use in this book is based on variables that have been uniformized. In factor investing, the scaling of features must be <strong>operated separately for each date and each feature</strong>. This point is critical. It makes sure that for every rebalancing date, the predictors will have a similar shape and do carry information on the cross-section of stocks.</p>
<p>Uniformization is sometimes presented differently: for a given characteristic and time, characteristic values are ranked and the rank is then divided by the number of non missing points. This is done in <span class="citation">Freyberger, Neuhierl, and Weber (<a href="#ref-freyberger2020dissecting">2020</a>)</span> for example. In <span class="citation">Kelly, Pruitt, and Su (<a href="#ref-kelly2019characteristics">2019</a>)</span>, the authors perform this operation but then subtract 0.5 to all features so that their values lie in [-0.5,0.5].</p>
<p>Scaling features across dates should be proscribed. Take for example the case of market capitalization. On the long run (market crashes notwithstanding), this feature increases through time. Thus, scaling across dates, would lead to small values at the beginning of the sample and large values at the end of the sample. This would completely alter and dilute the cross-sectional content of the features.</p>
</div>
</div>
<div id="labelling" class="section level2">
<h2><span class="header-section-number">5.5</span> Labelling</h2>
<div id="simple-labels" class="section level3">
<h3><span class="header-section-number">5.5.1</span> Simple labels</h3>
<p>There are several ways to define labels when constructing portfolio policies. Of course, the finality is the portfolio weight, but it is rarely considered as the best choice for the label.<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></p>
<p>Usual labels in factor investing are the following:</p>
<ul>
<li>raw asset returns;<br />
</li>
<li>future relative returns (versus some benchmark: market-wide index, or sector-based portfolio for instance);<br />
</li>
<li>the probability of positive return (or of return above a specified threshold);<br />
</li>
<li>the probability of outperforming a benchmark;<br />
</li>
<li>the binary version of the above: YES (outperforming) versus NO (underperforming);<br />
</li>
<li>risk-adjusted versions of the above: Sharpe ratios, information ratios (see Section <a href="backtest.html#perfmet">13.3</a>).<br />
When creating binary variables, it is often tempting to create a test that compares returns to zero (profitable versus non-profitable). This is not optimal because very much time-dependent. In good times, many assets will have positive returns while in market crashed, few will experience positive returns, thereby creating very unbalanced classes. It is a better idea to split the returns in two by comparing them to their time-<span class="math inline">\(t\)</span> median (or average). In this case, the indicator is relative and the two classes are much more balanced.</li>
</ul>
<p>As we will discuss later in this chapter, these choices still leave room for additional degrees of freedom. Should the labels be rescaled, just like features are processed? What is the best time horizon on which to compute performance metrics?</p>
</div>
<div id="categorical-labels" class="section level3">
<h3><span class="header-section-number">5.5.2</span> Categorical labels</h3>
<p>In a typical ML analysis, when <span class="math inline">\(y\)</span> is a proxy for future performance, the ML engine will try to minimize some distance between the predicted value and the realized values. For mathematical convenience, the sum of squared error (<span class="math inline">\(L^2\)</span> norm) is used because it has the simplest derivative and makes gradient descent accessible and easy to compute.</p>
<p>Sometimes, it can be interesting not to focus on raw performance proxies, like returns or Sharpe ratios, but on investment decisions - which can be derived from these proxies. A simple example (decision rule) is the following:</p>
<p><span class="math display" id="eq:catlabel">\[\begin{equation}
\tag{5.2}
y_{t,i}=\left\{  \begin{array}{rll}
-1 &amp; \text{ if } &amp; \hat{r}_{t,i} &lt; r_- \\
0 &amp; \text{ if } &amp; \hat{r}_{t,i} \in [r_-,r_+] \\
+1 &amp; \text{ if } &amp; \hat{r}_{t,i} &gt; r_+ \\
\end{array} \right.,
\end{equation}\]</span>
where <span class="math inline">\(\hat{r}_{t,i}\)</span> is the performance proxy (e.g., returns or Sharpe ratio) and <span class="math inline">\(r_\pm\)</span> are the decision thresholds. When the predicted performance is below <span class="math inline">\(r_-\)</span>, the decision is -1 (e.g., <em>sell</em>), when it is above <span class="math inline">\(r_+\)</span>, the decision is +1 (e.g., <em>buy</em>) and when it is in the middle (the model is neither very optimistic nor very pessimistic), then the decision is neutral (e.g., <em>hold</em>). The performance proxy can of course be relative to some benchmark so that the decision is directly related to this benchmark. The thresholds <span class="math inline">\(r_\pm\)</span> should be chosen such that the three categories are relatively balanced, that is, have a comparable number of instances.</p>
<p>In this case, the final output can be both considered as categorical or numerical because it belongs to an important subgroup of categorical variables: the ordered categorical (<strong>ordinal</strong>) variables. If <span class="math inline">\(y\)</span> is taken as a number, then the usual regression tools apply. The transformation of the initial output into a new format is similar to what is performed when engineering features.</p>
<p>When <span class="math inline">\(y\)</span> is treated as non-ordered (<strong>nominal</strong>) categorical variable, then a new layer of processing is required because ML tools only work with numbers. Hence, the categories must be recoded into digits. The mapping that is most often used is called ‘<strong>one-hot encoding</strong>’. The vector of classes is split in a sparse matrix in which each column is dedicated to one class. The matrix is filled with zeros and ones. A one is allocated to the column corresponding to the class of the instance. We provide a simple illustration in the table below.</p>
<table>
<thead>
<tr class="header">
<th>Initial data</th>
<th></th>
<th>One-hot encoding</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Position</td>
<td>Sell</td>
<td>Hold</td>
<td>Buy</td>
</tr>
<tr class="even">
<td>buy</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>buy</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="even">
<td>hold</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>sell</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>buy</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>In classification tasks, the output has a larger dimension. For each instance, it gives the probability of belonging to each class assigned by the model. As we will see in Chapters <a href="trees.html#trees">7</a> and <a href="NN.html#NN">8</a>, this is easily handled via the softmax function.</p>
<p>From the standpoint of allocation, handling categorical predictions is not necessarily easy. For long-short portfolios, plus or minus one signals can provide the sign of the position. For long-only portfolio, two possible solutions: <em>i)</em> work with binary classes (in versus out of the portfolio) or <em>ii)</em> adapt weights according to the prediction: zero weight for a -1 prediction, 0.5 weight for a 0 prediction and full weight for a +1 prediction. Weights are then of course normalized.</p>
</div>
<div id="the-triple-barrier-method" class="section level3">
<h3><span class="header-section-number">5.5.3</span> The triple barrier method</h3>
<p>We conclude this section with an advanced labelling technique mentioned in <span class="citation">De Prado (<a href="#ref-de2018advances">2018</a>)</span>. The idea is to consider the full dynamics of a trading strategy and not a simple performance proxy. The rationale for this extension is that often money managers implement P&amp;L triggers that cash in when gains are sufficient or opt out to stop their losses. Upon inception of the strategy, three barriers are fixed (see Figure <a href="Data.html#fig:triplebarrier">5.4</a>):</p>
<ul>
<li>one above the current level of the asset (majenta line), which measures a reasonable expected profit;<br />
</li>
<li>one below the current level of the asset (cyan line), which acts as a stop-loss signal to prevent large negative returns;<br />
</li>
<li>and finally, one that fixes the horizon of the strategy after which it will be terminated (black line).</li>
</ul>
<p>If the strategy hits the first (<em>resp</em>. second) barrier, the output is +1 (<em>resp</em>. -1) and if it hits the last barrier, the output is equal to zero or to some linear interpolation (between -1 and +1) that represents the position of the terminal value relative to the two horizontal barriers. Computationally, this method is <strong>much</strong> more demanding as it evaluates a whole trajectory for each instance. It is nonetheless considered as more realistic because trading strategies are often accompanied with automatic triggers such as stop-loss, etc.</p>
<div class="figure" style="text-align: center"><span id="fig:triplebarrier"></span>
<img src="ML_factor_files/figure-html/triplebarrier-1.png" alt=" Illustration of the triple barrier method." width="576" />
<p class="caption">
FIGURE 5.4:  Illustration of the triple barrier method.
</p>
</div>
</div>
<div id="filtering-the-sample" class="section level3">
<h3><span class="header-section-number">5.5.4</span> Filtering the sample</h3>
<p>One of the main challenges in Machine Learning is to extract as much <strong>signal</strong> as possible. By signal, we mean patterns that will hold out-of-sample. Intuitively, it may seem reasonable to think that the more data we gather, the more signal we can extract. This is in fact false in all generality because more data also means more noise. Surprisingly, filtering the training samples can improve performance. This idea was for example implemented successfully in <span class="citation">Fu et al. (<a href="#ref-fu2018machine">2018</a>)</span>, <span class="citation">Guida and Coqueret (<a href="#ref-guida2019big">2018</a><a href="#ref-guida2019big">a</a>)</span> and <span class="citation">Guida and Coqueret (<a href="#ref-guida2018machine">2018</a><a href="#ref-guida2018machine">b</a>)</span>.</p>
<p>In our paper <span class="citation">Coqueret and Guida (<a href="#ref-coqueret2019training">2019</a>)</span>, we investigate why smaller samples may lead to superior out-of-sample accuracy for a particular type of ML algorithm: decision trees (see Chapter <a href="trees.html#trees">7</a>). We focus on a particular kind of filter: we exclude the labels (i.e., returns) that are not extreme and retain the 20% values that are the smallest and the 20% that are the largest (the bulk of the distribution is removed). In doing so, we alter the structure of trees in two ways:<br />
- when the splitting points are altered, they are always closer to the center of the distribution of the splitting variable (i.e., the resulting clusters are more balanced);<br />
- the choice of splitting variables is (sometimes) pushed towards the features that have a monotonous impact on the label.<br />
These two properties are desirable. The first reduces the risk of fitting to small groups of instances that may be spurious. The second gives more importance to features that appear globally more relevant in explaning the returns. However, the filtering must not be too intense. If, instead of retaining 20% of each tail of the predictor, we keep just 10%, then the loss in signal becomes too severe and the performance deteriorates.</p>
</div>
<div id="horizons" class="section level3">
<h3><span class="header-section-number">5.5.5</span> Return horizons</h3>
<p>This subsection deals with one of the least debated issue in factor-based machine learning models: horizons. Several horizons come into play during the whole ML-driven allocation workflow: the <strong>horizon of the label</strong>, the <strong>estimation window</strong> (chronological depth of the training samples) and the <strong>holding periods</strong>. One early reference that looks at these aspects is the founding academic paper on momentum by <span class="citation">Jegadeesh and Titman (<a href="#ref-jegadeesh1993returns">1993</a>)</span>. The authors compute the profitability of portfolios based on the returns over the past <span class="math inline">\(J=3, 6, 9, 12\)</span> months. Four holding periods are tested: <span class="math inline">\(K=3,6,9,12\)</span> months. They report: “.” While there is no machine learning whatsoever in this contribution, it is likely that there conclusion that horizons matter may also hold for more sophisticated methods. This topic is in fact much discussed, as is shown by the continuing debate on the impact of horizons in momentum profitability (see, e.g., <span class="citation">Novy-Marx (<a href="#ref-novy2012momentum">2012</a>)</span>, <span class="citation">Gong, Liu, and Liu (<a href="#ref-gong2015momentum">2015</a>)</span> and <span class="citation">Goyal and Wahal (<a href="#ref-goyal2015momentum">2015</a>)</span>).</p>
<p>This debate should also be considered when working with ML algorithms. The issue of estimation windows and holding periods are mentioned later in the book, in Chapter <a href="backtest.html#backtest">13</a>. Naturally, in this chapter, the horizon of the label is the important ingredient. Heuristically, there are four possible combinations if we consider only one feature:<br />
1. oscillating label and feature;<br />
2. oscillating label, smooth feature (highly autocorrelated);<br />
3. smooth label, oscillating feature;<br />
4. smooth label and feature.</p>
<p>Of all of these options, the last one is probably preferable because more robust, all things being equal. By all things equal, we mean that in each case, a model is capable of extracting some relevant pattern. A pattern that holds between two slowly moving series is more likely to persist in time. Thus, since features are often highly autocorrelated (cf Figure <a href="Data.html#fig:histcorr">5.3</a>), combining them with smooth labels is probably a good idea. To illustrate how critical this point is, we will purposefully use 1 month returns in most of the examples of the book and show that the corresponding results are always disappointing. These returns are very weakly autocorrelated while 6 month or 12 month returns are much more persistent and are better choices for labels.</p>
<p>Theoretically, it is possible to understand why that may be the case. For simplicity, let us assume a single feature <span class="math inline">\(x\)</span> that explains returns <span class="math inline">\(r\)</span>: <span class="math inline">\(r_{t+1}=f(x_t)+e_{t+1}\)</span>. If <span class="math inline">\(x_t\)</span> is highly autocorrelated and the noise embeded in <span class="math inline">\(e_{t+1}\)</span> is not too large, then the two-period ahead return <span class="math inline">\((1+r_{t+1})(1+r_{t+2})-1\)</span> may carry more signal than <span class="math inline">\(r_{t+1}\)</span> because the relationship with <span class="math inline">\(x_t\)</span> has diffused through time. We discuss some practicalities related to autocorrelations in the next section.</p>
</div>
</div>
<div id="pers" class="section level2">
<h2><span class="header-section-number">5.6</span> Handling persistence</h2>
<p>While we have separated the steps of feature engineering and labelling in two different subsections, it is probably wiser to consider them jointly. One important property of the dataset processed by the ML algorithm should be the consistency of persistence between features and labels. Intuitively, the autocorrelation patterns between the label <span class="math inline">\(y_{t,n}\)</span> (future performance) and the features <span class="math inline">\(x_{t,n}^{(k)}\)</span> should not be too distant.</p>
<p>One problematic example is when the dataset is sampled at the monthly frequency (not unusual in the money management industry) with the labels being monthly returns and the features being risk-based or fundamental attributes. In this case, the label is very weakly autocorrelated, while the features are often highly autocorrelated. In this situation, most sophisticated forecasting tools will arbitrage between features which will probably result in a lot of noise. In linear predictive models, this configuration is known to generate bias in estimates (see the study of <span class="citation">Stambaugh (<a href="#ref-stambaugh1999predictive">1999</a>)</span> and the review by <span class="citation">Gonzalo and Pitarakis (<a href="#ref-gonzalo2018predictive">2018</a>)</span>).</p>
<p>Among other more technical options, there are two simple solutions when facing this issue. Either introduce autocorrelation into the label, or remove it from the features. Both are rather easy econometrically:</p>
<ul>
<li>to increase the autocorrelation of the label, compute performance over longer time ranges. For instance, when working with monthly data, considering annual or biennial returns will do the trick.<br />
</li>
<li>to get rid of autocorrelation, the shortest route is to resort to difference/variations: <span class="math inline">\(\Delta x_{t,n}^{(k)}=x_{t,n}^{(k)}-x_{t-1,n}^{(k)}\)</span>. One advantage of this procedure is that it makes sense, economically: variations in features may be better drivers of performance, compared to raw levels.</li>
</ul>
<p>The crucial choice is whether to work <em>mostly</em> with persistent or oscillating variables. A mix between the two in the feature space is also possible, but both types should be well represented.</p>
</div>
<div id="extensions" class="section level2">
<h2><span class="header-section-number">5.7</span> Extensions</h2>
<div id="transforming-features" class="section level3">
<h3><span class="header-section-number">5.7.1</span> Transforming features</h3>
<p>The feature space can easily be augmented through simple operations. One of them is lagging, that is, considering older values of features and assuming some memory effect for their impact on the label. This is naturally useful mostly if the features are oscillating (adding a layer of memory on persistent features can be somewhat redudant). New variables are defined by <span class="math inline">\(\breve{x}_{t,n}^{(k)}=x_{t-1,n}^{(k)}\)</span>.</p>
<p>In some cases (e.g., insufficient number of features), it is possible to consider ratios or products between features. Accounting ratios like price-to-book, book-to-market, debt-to-equity are examples of functions of raw features that make sense. The gains brought by a larger spectrum of features are not obvious. The risk of overfitting increases, just like in a simple linear regression adding variables mechanically increases the <span class="math inline">\(R^2\)</span>. The choices must make sense, economically.</p>
<p>Another way to increase the feature space (mentioned above) is to consider variations. Variations in sentiment, variations in book-to-market ratio, etc., can be relevant predictors because sometimes, the change is more important than the level. In this case, a new predictor is <span class="math inline">\(\breve{x}_{t,n}^{(k)}=x_{t,n}^{(k)}-x_{t-1,n}^{(k)}\)</span>.</p>
</div>
<div id="macrovar" class="section level3">
<h3><span class="header-section-number">5.7.2</span> Macro-economic variables</h3>
<p>Finally, we discuss a very important topic. The data should never be seperated from the context it comes from (its environment). In classical financial terms, this means that a particular model is likely to depend on the overarching situation which is often proxied by macro-economic indicators. One way to take this into account at the data level is simply to multiply the feature by a exogenous indicator <span class="math inline">\(z_{t}\)</span> and in this case, the new predictor is
<span class="math display" id="eq:macrocond">\[\begin{equation}
\tag{5.3}
\breve{x}_{t,n}^{(k)}=z_t \times x_{t,n}^{(k)}
\end{equation}\]</span>
This technique is used by <span class="citation">Gu, Kelly, and Xiu (<a href="#ref-gu2018empirical">2018</a>)</span> who use 8 economic indicators (plus the original predictors (<span class="math inline">\(z_t=1\)</span>)). This increase the feature space ninefold.</p>
<p>Another route that integrates shifting economic environements is conditional engineering. Suppose that labels are coded via formula <a href="Data.html#eq:catlabel">(5.2)</a>. The thresholds can be made dependent on some exogenous variable. In times of turbulence, it might be a good idea to increase both <span class="math inline">\(r_+\)</span> (buy threshold) and <span class="math inline">\(r_-\)</span> (sell threshold) so that the labels become more conservative: it takes a higher return to make it to the <em>buy</em> category, while short positions are favored. One such example of dynamic thresholding could be</p>
<p><span class="math display" id="eq:condvix">\[\begin{equation}
\tag{5.4}
r_{t,\pm}=r_{\pm} \times e^{\pm\delta(\text{VIX}_t-\bar{\text{VIX}})},
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\text{VIX}_t\)</span> is the time-<span class="math inline">\(t\)</span> value of the VIX while <span class="math inline">\(\bar{\text{VIX}}\)</span> is some average or median value. When the VIX is above its average and risk seems to be increasing, the thresholds also increase. The parameter <span class="math inline">\(\delta\)</span> tunes the magnitude of the correction. In the above example, we assume <span class="math inline">\(r_-&lt;0&lt;r_+\)</span>.</p>
</div>
</div>
<div id="additional-code-and-results" class="section level2">
<h2><span class="header-section-number">5.8</span> Additional code and results</h2>
<div id="impact-of-rescaling-graphical-representation" class="section level3">
<h3><span class="header-section-number">5.8.1</span> Impact of rescaling: graphical representation</h3>
<p>We start with a simple illustration of the different scaling methods. We generate an arbitrary series and then rescale it. The series is not random so that each time the code chunk is executed, the output remains the same.</p>

<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1">Length &lt;-<span class="st"> </span><span class="dv">100</span>                                 <span class="co"># Length of the sequence</span></a>
<a class="sourceLine" id="cb24-2" data-line-number="2">x &lt;-<span class="st"> </span><span class="kw">exp</span>(<span class="kw">sin</span>(<span class="dv">1</span><span class="op">:</span>Length))                       <span class="co"># Original data</span></a>
<a class="sourceLine" id="cb24-3" data-line-number="3">data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">index =</span> <span class="dv">1</span><span class="op">:</span>Length, <span class="dt">x =</span> x)   <span class="co"># Data framed into dataframe</span></a>
<a class="sourceLine" id="cb24-4" data-line-number="4"><span class="kw">ggplot</span>(data, <span class="kw">aes</span>(<span class="dt">x =</span> index, <span class="dt">y =</span> x)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>) <span class="co"># Plot</span></a></code></pre></div>
<p><img src="ML_factor_files/figure-html/scale_ex-1.png" width="672" /></p>
<p></p>
<p>We define and plot the scaled variables below.</p>

<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb25-1" data-line-number="1">norm_unif &lt;-<span class="st">  </span><span class="cf">function</span>(v){  <span class="co"># This is a function that uniformalises a vector.</span></a>
<a class="sourceLine" id="cb25-2" data-line-number="2">    v &lt;-<span class="st"> </span>v <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.matrix</span>()</a>
<a class="sourceLine" id="cb25-3" data-line-number="3">    <span class="kw">return</span>(<span class="kw">ecdf</span>(v)(v))</a>
<a class="sourceLine" id="cb25-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb25-5" data-line-number="5"></a>
<a class="sourceLine" id="cb25-6" data-line-number="6">norm_<span class="dv">0</span>_<span class="dv">1</span> &lt;-<span class="st">  </span><span class="cf">function</span>(v){  <span class="co"># This is a function that uniformalises a vector.</span></a>
<a class="sourceLine" id="cb25-7" data-line-number="7">    <span class="kw">return</span>((v<span class="op">-</span><span class="kw">min</span>(v))<span class="op">/</span>(<span class="kw">max</span>(v)<span class="op">-</span><span class="kw">min</span>(v)))</a>
<a class="sourceLine" id="cb25-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb25-9" data-line-number="9"></a>
<a class="sourceLine" id="cb25-10" data-line-number="10">data_norm &lt;-<span class="st"> </span><span class="kw">data.frame</span>(                        <span class="co"># Formatting the data</span></a>
<a class="sourceLine" id="cb25-11" data-line-number="11">    <span class="dt">index =</span> <span class="dv">1</span><span class="op">:</span>Length,                           <span class="co"># Index of point/instance</span></a>
<a class="sourceLine" id="cb25-12" data-line-number="12">    <span class="dt">standard =</span> (x <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(x)) <span class="op">/</span><span class="st"> </span><span class="kw">sd</span>(x),           <span class="co"># Standardisation</span></a>
<a class="sourceLine" id="cb25-13" data-line-number="13">    <span class="dt">norm_0_1 =</span> <span class="kw">norm_0_1</span>(x),                     <span class="co"># [0,1] reduction</span></a>
<a class="sourceLine" id="cb25-14" data-line-number="14">    <span class="dt">unif =</span> <span class="kw">norm_unif</span>(x)) <span class="op">%&gt;%</span><span class="st">                    </span><span class="co"># Uniformisation</span></a>
<a class="sourceLine" id="cb25-15" data-line-number="15"><span class="st">    </span><span class="kw">gather</span>(<span class="dt">key =</span> Type, <span class="dt">value =</span> value, <span class="op">-</span>index)   <span class="co"># Putting in tidy format</span></a>
<a class="sourceLine" id="cb25-16" data-line-number="16"><span class="kw">ggplot</span>(data_norm, <span class="kw">aes</span>(<span class="dt">x =</span> index, <span class="dt">y =</span> value, <span class="dt">fill =</span> Type)) <span class="op">+</span><span class="st">   </span><span class="co"># Plot!</span></a>
<a class="sourceLine" id="cb25-17" data-line-number="17"><span class="st">    </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb25-18" data-line-number="18"><span class="st">    </span><span class="kw">facet_grid</span>(Type<span class="op">~</span>.)          <span class="co"># This option creates 3 concatenated graphs to ease comparison</span></a></code></pre></div>
<p><img src="ML_factor_files/figure-html/data_norm-1.png" width="672" /></p>
<p></p>
<p>Finally, we look at the histogram of the newly created variables.</p>

<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="kw">ggplot</span>(data_norm, <span class="kw">aes</span>(<span class="dt">x =</span> value, <span class="dt">fill =</span> Type)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_histogram</span>(<span class="dt">position =</span> <span class="st">&quot;dodge&quot;</span>)</a></code></pre></div>
<p><img src="ML_factor_files/figure-html/data_norm_dist-1.png" width="672" /></p>
<p></p>
<p>With respect to shape, the green and red distributions are close to the original one. It is only the support that changes: the min/max rescaling ensures all values lie in the <span class="math inline">\([0,1]\)</span> interval. In both cases, the smallest values (on the left) display a spike in distribution. By construction, this spike disappears under the uniformisation: the points are evenly distributed over the unit interval.</p>
</div>
<div id="impact-of-rescaling-toy-example" class="section level3">
<h3><span class="header-section-number">5.8.2</span> Impact of rescaling: toy example</h3>
<p>To illustrate the impact of chosing one particular rescaling method,<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> we build a simple dataset, comprising 3 firms and 3 dates.</p>

<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb27-1" data-line-number="1">firm &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">3</span>), <span class="kw">rep</span>(<span class="dv">2</span>,<span class="dv">3</span>), <span class="kw">rep</span>(<span class="dv">3</span>,<span class="dv">3</span>))             <span class="co"># Firms (3 lines for each)</span></a>
<a class="sourceLine" id="cb27-2" data-line-number="2">date &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>),<span class="dv">3</span>)                             <span class="co"># Dates</span></a>
<a class="sourceLine" id="cb27-3" data-line-number="3">cap &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>,                               <span class="co"># Market capitalisation</span></a>
<a class="sourceLine" id="cb27-4" data-line-number="4">         <span class="dv">15</span>, <span class="dv">10</span>, <span class="dv">15</span>, </a>
<a class="sourceLine" id="cb27-5" data-line-number="5">         <span class="dv">200</span>, <span class="dv">120</span>, <span class="dv">80</span>)</a>
<a class="sourceLine" id="cb27-6" data-line-number="6">return &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.06</span>, <span class="fl">0.01</span>, <span class="fl">-0.06</span>,                      <span class="co"># Return values</span></a>
<a class="sourceLine" id="cb27-7" data-line-number="7">            <span class="fl">-0.03</span>, <span class="fl">0.00</span>, <span class="fl">0.02</span>,</a>
<a class="sourceLine" id="cb27-8" data-line-number="8">            <span class="fl">-0.04</span>, <span class="fl">-0.02</span>,<span class="fl">0.00</span>)</a>
<a class="sourceLine" id="cb27-9" data-line-number="9">data_toy &lt;-<span class="st"> </span><span class="kw">data.frame</span>(firm, date, cap, return)     <span class="co"># Aggregation of data</span></a>
<a class="sourceLine" id="cb27-10" data-line-number="10">data_toy &lt;-<span class="st"> </span>data_toy <span class="op">%&gt;%</span><span class="st">                            </span><span class="co"># Transformation of data</span></a>
<a class="sourceLine" id="cb27-11" data-line-number="11"><span class="st">    </span><span class="kw">group_by</span>(date) <span class="op">%&gt;%</span><span class="st">                            </span></a>
<a class="sourceLine" id="cb27-12" data-line-number="12"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">cap_0_1 =</span> <span class="kw">norm_0_1</span>(cap), <span class="dt">cap_u =</span> <span class="kw">norm_unif</span>(cap))</a>
<a class="sourceLine" id="cb27-13" data-line-number="13">knitr<span class="op">::</span><span class="kw">kable</span>(data_toy, <span class="dt">digits =</span> <span class="dv">3</span>,                  <span class="co"># Display the data</span></a>
<a class="sourceLine" id="cb27-14" data-line-number="14">             <span class="dt">caption =</span> <span class="st">&quot;Sample data for a toy example.&quot;</span>)   </a></code></pre></div>

<p></p>
<p>Let’s briefly comment on this synthetic data. We assume that dates are ordered chronologically and far away: each date stands for a year or the beginning of a decade, but the (forward) returns are computed on a monthly basis. The first firm is hugely successful and multiplies its cap ten times over the periods. The second firms remains stable cap-wise, while the third one plummets. If we look at ‘local’ future returns, they are strongly negatively related to size for the first and third firms. For the second one, there is no clear pattern.</p>
<p>Date-by-date, the analysis is fairly similar, though slightly nuanced.</p>
<ol style="list-style-type: decimal">
<li>On date 1, the smallest firm has the largest return and the two others have negative returns.<br />
</li>
<li>On date 2, the biggest firm has a negative return while the two smaller firms do not.<br />
</li>
<li>On date 3, returns are decreasing with size.</li>
</ol>
<p>While the relationship is not always perfectly monotonous, there seems to be a link between size and return and typically, investing in the smallest firm would be a very good strategy with this sample.</p>
<p>Now let us look at the output of simple regressions.</p>

<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb28-1" data-line-number="1"><span class="kw">lm</span>(return <span class="op">~</span><span class="st"> </span>cap_<span class="dv">0</span>_<span class="dv">1</span>, <span class="dt">data =</span> data_toy) <span class="op">%&gt;%</span><span class="st"> </span><span class="co"># First regression (min-max rescaling)</span></a>
<a class="sourceLine" id="cb28-2" data-line-number="2"><span class="st">    </span>broom<span class="op">::</span><span class="kw">tidy</span>() <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb28-3" data-line-number="3"><span class="st">    </span>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="dt">caption =</span> <span class="st">&#39;Regression output when the independent var. comes </span></a>
<a class="sourceLine" id="cb28-4" data-line-number="4"><span class="st">                 from min-max rescaling&#39;</span>) </a></code></pre></div>

<p></p>

<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb29-1" data-line-number="1"><span class="kw">lm</span>(return <span class="op">~</span><span class="st"> </span>cap_u, <span class="dt">data =</span> data_toy) <span class="op">%&gt;%</span><span class="st">   </span><span class="co"># Second regression (uniformised feature)</span></a>
<a class="sourceLine" id="cb29-2" data-line-number="2"><span class="st">    </span>broom<span class="op">::</span><span class="kw">tidy</span>() <span class="op">%&gt;%</span><span class="st">   </span></a>
<a class="sourceLine" id="cb29-3" data-line-number="3"><span class="st">    </span>knitr<span class="op">::</span><span class="kw">kable</span>(<span class="dt">caption =</span> <span class="st">&#39;Regression output when the independent var. comes from uniformisation&#39;</span>)   </a></code></pre></div>

<p>
In terms of <em>p</em>-<strong>value</strong> (last column), the first estimation for the cap coefficient is above 5% (in Table <a href="#tab:datatoyreg"><strong>??</strong></a>) while the second is below 1% (in Table <a href="#tab:datatoyreg2"><strong>??</strong></a>). One possible explanation for this discrepancy is the standard deviation of the variables. The deviations are equal to 0.47 and 0.29 for cap_0 and cap_u, respectively. Values like market capitalisations can have very large ranges and are thus subject to substantial deviations (even after scaling). Working with uniformised variables reduces dispersion and can help solve this problem.</p>
<p>Note that this is a <strong>double-edged sword</strong>: while it can help avoid <strong>false negatives</strong>, it can also lead to <strong>false positives</strong>.</p>
</div>
</div>
<div id="coding-exercises-1" class="section level2">
<h2><span class="header-section-number">5.9</span> Coding exercises</h2>
<ol style="list-style-type: decimal">
<li>The federal reserve of Saint Louis (<a href="https://fred.stlouisfed.org" class="uri">https://fred.stlouisfed.org</a>) hosts thousands of time-series of economic indicators that can serve as conditioning variables. Pick one, and apply formula <a href="Data.html#eq:macrocond">(5.3)</a> to expand the number of predictors. If need be, use the function defined above.<br />
</li>
<li>Create a new categorical label based on formulae <a href="Data.html#eq:condvix">(5.4)</a> and <a href="Data.html#eq:catlabel">(5.2)</a>. The time-series of the VIX can also be retrieve from the federal reserve’s website: <a href="https://fred.stlouisfed.org/series/VIXCLS" class="uri">https://fred.stlouisfed.org/series/VIXCLS</a>.<br />
</li>
<li>Plot the histogram of the R12M_Usd variable. Clearly, some outliers are present. Identify the stock with highest value for this variable and determine if the value can be correct or not.</li>
</ol>

</div>
</div>
<h3><span class="header-section-number">C</span> References</h3>
<div id="refs" class="references">
<div id="ref-aggarwal2013outlier">
<p>Aggarwal, Charu C. 2013. <em>Outlier Analysis</em>. Springer.</p>
</div>
<div id="ref-allison2001missing">
<p>Allison, Paul D. 2001. <em>Missing Data</em>. Vol. 136. Sage publications.</p>
</div>
<div id="ref-boehmke2019hands">
<p>Boehmke, Brad, and Brandon Greenwell. 2019. <em>Hands-on Machine Learning with R</em>. Chapman; Hall.</p>
</div>
<div id="ref-chandola2009anomaly">
<p>Chandola, Varun, Arindam Banerjee, and Vipin Kumar. 2009. “Anomaly Detection: A Survey.” <em>ACM Computing Surveys (CSUR)</em> 41 (3): 15.</p>
</div>
<div id="ref-che2018recurrent">
<p>Che, Zhengping, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. 2018. “Recurrent Neural Networks for Multivariate Time Series with Missing Values.” <em>Scientific Reports</em> 8 (1): 6085.</p>
</div>
<div id="ref-chen2012dividend">
<p>Chen, Long, Zhi Da, and Richard Priestley. 2012. “Dividend Smoothing and Predictability.” <em>Management Science</em> 58 (10): 1834–53.</p>
</div>
<div id="ref-coqueret2019training">
<p>Coqueret, Guillaume, and Tony Guida. 2019. “Training Trees on Tails with Applications to Portfolio Choice.” <em>SSRN Working Paper</em> 3403009.</p>
</div>
<div id="ref-de2018advances">
<p>De Prado, Marcos Lopez. 2018. <em>Advances in Financial Machine Learning</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-enders2001primer">
<p>Enders, Craig K. 2001. “A Primer on Maximum Likelihood Algorithms Available for Use with Missing Data.” <em>Structural Equation Modeling</em> 8 (1). Taylor &amp; Francis: 128–41.</p>
</div>
<div id="ref-enders2010applied">
<p>Enders, Craig K. 2010. <em>Applied Missing Data Analysis</em>. Guilford press.</p>
</div>
<div id="ref-freyberger2020dissecting">
<p>Freyberger, Joachim, Andreas Neuhierl, and Michael Weber. 2020. “Dissecting Characteristics Nonparametrically.” <em>Review of Financial Studies</em> XXX-XXX.</p>
</div>
<div id="ref-fu2018machine">
<p>Fu, XingYu, JinHong Du, YiFeng Guo, MingWen Liu, Tao Dong, and XiuWen Duan. 2018. “A Machine Learning Framework for Stock Selection.” <em>arXiv Preprint</em>, no. 1806.01743.</p>
</div>
<div id="ref-galili2016splitting">
<p>Galili, Tal, and Isaac Meilijson. 2016. “Splitting Matters: How Monotone Transformation of Predictor Variables May Improve the Predictions of Decision Tree Models.” <em>arXiv Preprint</em>, no. 1611.04561.</p>
</div>
<div id="ref-garcia2009k">
<p>Garcı'a-Laencina, Pedro J, José-Luis Sancho-Gómez, Anı'bal R Figueiras-Vidal, and Michel Verleysen. 2009. “K Nearest Neighbours with Mutual Information for Simultaneous Classification and Missing Data Imputation.” <em>Neurocomputing</em> 72 (7-9). Elsevier: 1483–93.</p>
</div>
<div id="ref-gong2015momentum">
<p>Gong, Qiang, Ming Liu, and Qianqiu Liu. 2015. “Momentum Is Really Short-Term Momentum.” <em>Journal of Banking &amp; Finance</em> 50: 169–82.</p>
</div>
<div id="ref-gonzalo2018predictive">
<p>Gonzalo, Jesús, and Jean-Yves Pitarakis. 2018. “Predictive Regressions.” In <em>Oxford Research Encyclopedia of Economics and Finance</em>.</p>
</div>
<div id="ref-goyal2015momentum">
<p>Goyal, Amit, and Sunil Wahal. 2015. “Is Momentum an Echo?” <em>Journal of Financial and Quantitative Analysis</em> 50 (6): 1237–67.</p>
</div>
<div id="ref-gu2018empirical">
<p>Gu, Shihao, Bryan T Kelly, and Dacheng Xiu. 2018. “Empirical Asset Pricing via Machine Learning.” <em>SSRN Working Paper</em> 3159577.</p>
</div>
<div id="ref-guida2019big">
<p>Guida, Tony, and Guillaume Coqueret. 2018a. “Ensemble Learning Applied to Quant Equity: Gradient Boosting in a Multifactor Framework.” In <em>Big Data and Machine Learning in Quantitative Investment</em>, 129–48. Wiley.</p>
</div>
<div id="ref-guida2018machine">
<p>Guida, Tony, and Guillaume Coqueret. 2018b. “Machine Learning in Systematic Equity Allocation: A Model Comparison.” <em>Wilmott</em> 2018 (98): 24–33.</p>
</div>
<div id="ref-gupta2014outlier">
<p>Gupta, Manish, Jing Gao, Charu Aggarwal, and Jiawei Han. 2014. “Outlier Detection for Temporal Data.” <em>IEEE Transactions on Knowledge and Data Engineering</em> 26 (9): 2250–67.</p>
</div>
<div id="ref-guyon2003introduction">
<p>Guyon, Isabelle, and André Elisseeff. 2003. “An Introduction to Variable and Feature Selection.” <em>Journal of Lachine Learning Research</em> 3 (Mar): 1157–82.</p>
</div>
<div id="ref-friedman2009elements">
<p>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning</em>. Springer.</p>
</div>
<div id="ref-hodge2004survey">
<p>Hodge, Victoria, and Jim Austin. 2004. “A Survey of Outlier Detection Methodologies.” <em>Artificial Intelligence Review</em> 22 (2): 85–126.</p>
</div>
<div id="ref-honaker2010missing">
<p>Honaker, James, and Gary King. 2010. “What to Do About Missing Values in Time-Series Cross-Section Data.” <em>American Journal of Political Science</em> 54 (2): 561–81.</p>
</div>
<div id="ref-jegadeesh1993returns">
<p>Jegadeesh, Narasimhan, and Sheridan Titman. 1993. “Returns to Buying Winners and Selling Losers: Implications for Stock Market Efficiency.” <em>Journal of Finance</em> 48 (1): 65–91.</p>
</div>
<div id="ref-kelly2019characteristics">
<p>Kelly, Bryan T, Seth Pruitt, and Yinan Su. 2019. “Characteristics Are Covariances: A Unified Model of Risk and Return.” <em>Journal of Financial Economics</em> 134 (3): 501–24.</p>
</div>
<div id="ref-kuhn2019feature">
<p>Kuhn, Max, and Kjell Johnson. 2019. <em>Feature Engineering and Selection: A Practical Approach for Predictive Models</em>. CRC Press.</p>
</div>
<div id="ref-leary2011determinants">
<p>Leary, Mark T, and Roni Michaely. 2011. “Determinants of Dividend Smoothing: Empirical Evidence.” <em>Review of Financial Studies</em> 24 (10): 3197–3249.</p>
</div>
<div id="ref-little2014statistical">
<p>Little, Roderick JA, and Donald B Rubin. 2014. <em>Statistical Analysis with Missing Data</em>. Vol. 333. John Wiley &amp; Sons.</p>
</div>
<div id="ref-novy2012momentum">
<p>Novy-Marx, Robert. 2012. “Is Momentum Really Momentum?” <em>Journal of Financial Economics</em> 103 (3): 429–53.</p>
</div>
<div id="ref-rousseeuw2005robust">
<p>Rousseeuw, Peter J, and Annick M Leroy. 2005. <em>Robust Regression and Outlier Detection</em>. Vol. 589. Wiley.</p>
</div>
<div id="ref-schafer1999multiple">
<p>Schafer, Joseph L. 1999. “Multiple Imputation: A Primer.” <em>Statistical Methods in Medical Research</em> 8 (1): 3–15.</p>
</div>
<div id="ref-shah2014comparison">
<p>Shah, Anoop D, Jonathan W Bartlett, James Carpenter, Owen Nicholas, and Harry Hemingway. 2014. “Comparison of Random Forest and Parametric Imputation Models for Imputing Missing Data Using Mice: A Caliber Study.” <em>American Journal of Epidemiology</em> 179 (6): 764–74.</p>
</div>
<div id="ref-stambaugh1999predictive">
<p>Stambaugh, Robert F. 1999. “Predictive Regressions.” <em>Journal of Financial Economics</em> 54 (3): 375–421.</p>
</div>
<div id="ref-stekhoven2011missforest">
<p>Stekhoven, Daniel J, and Peter Bühlmann. 2011. “MissForest—Non-Parametric Missing Value Imputation for Mixed-Type Data.” <em>Bioinformatics</em> 28 (1): 112–18.</p>
</div>
<div id="ref-van2018flexible">
<p>Van Buuren, Stef. 2018. <em>Flexible Imputation of Missing Data</em>. Chapman; Hall/CRC.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="6">
<li id="fn6"><p>Some methodologies do map firm attributes into final weights, e.g., <span class="citation">Brandt, Santa-Clara, and Valkanov (<a href="#ref-brandt2009parametric">2009</a>)</span> and <span class="citation">Ammann, Coqueret, and Schade (<a href="#ref-ammann2016characteristics">2016</a>)</span> but these are outside the scope of the book.<a href="Data.html#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>For a more thorough technical discussion on the impact of feature engineering, we refer to <span class="citation">Galili and Meilijson (<a href="#ref-galili2016splitting">2016</a>)</span>.<a href="Data.html#fnref7" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="factor.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lasso.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ML_factor.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true,
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
