<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Ensemble models | Machine Learning for Factor Investing</title>
  <meta name="description" content="Chapter 12 Ensemble models | Machine Learning for Factor Investing" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Ensemble models | Machine Learning for Factor Investing" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Ensemble models | Machine Learning for Factor Investing" />
  
  
  

<meta name="author" content="Guillaume Coqueret and Tony Guida" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="valtune.html"/>
<link rel="next" href="backtest.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="preface.html"><a href="preface.html#what-this-book-is-not-about"><i class="fa fa-check"></i><b>1.1</b> What this book is not about</a></li>
<li class="chapter" data-level="1.2" data-path="preface.html"><a href="preface.html#the-targeted-audience"><i class="fa fa-check"></i><b>1.2</b> The targeted audience</a></li>
<li class="chapter" data-level="1.3" data-path="preface.html"><a href="preface.html#how-this-book-is-structured"><i class="fa fa-check"></i><b>1.3</b> How this book is structured</a></li>
<li class="chapter" data-level="1.4" data-path="preface.html"><a href="preface.html#companion-website"><i class="fa fa-check"></i><b>1.4</b> Companion website</a></li>
<li class="chapter" data-level="1.5" data-path="preface.html"><a href="preface.html#why-r"><i class="fa fa-check"></i><b>1.5</b> Why R?</a></li>
<li class="chapter" data-level="1.6" data-path="preface.html"><a href="preface.html#coding-instructions"><i class="fa fa-check"></i><b>1.6</b> Coding instructions</a></li>
<li class="chapter" data-level="1.7" data-path="preface.html"><a href="preface.html#acknowledgements"><i class="fa fa-check"></i><b>1.7</b> Acknowledgements</a></li>
<li class="chapter" data-level="1.8" data-path="preface.html"><a href="preface.html#future-developments"><i class="fa fa-check"></i><b>1.8</b> Future developments</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="notdata.html"><a href="notdata.html"><i class="fa fa-check"></i><b>2</b> Notations and data</a><ul>
<li class="chapter" data-level="2.1" data-path="notdata.html"><a href="notdata.html#notations"><i class="fa fa-check"></i><b>2.1</b> Notations</a></li>
<li class="chapter" data-level="2.2" data-path="notdata.html"><a href="notdata.html#dataset"><i class="fa fa-check"></i><b>2.2</b> Dataset</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>3</b> Introduction</a><ul>
<li class="chapter" data-level="3.1" data-path="intro.html"><a href="intro.html#context"><i class="fa fa-check"></i><b>3.1</b> Context</a></li>
<li class="chapter" data-level="3.2" data-path="intro.html"><a href="intro.html#portfolio-construction-the-workflow"><i class="fa fa-check"></i><b>3.2</b> Portfolio construction: the workflow</a></li>
<li class="chapter" data-level="3.3" data-path="intro.html"><a href="intro.html#machine-learning-is-no-magic-wand"><i class="fa fa-check"></i><b>3.3</b> Machine Learning is no Magic Wand</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="factor.html"><a href="factor.html"><i class="fa fa-check"></i><b>4</b> Factor investing and asset pricing anomalies</a><ul>
<li class="chapter" data-level="4.1" data-path="factor.html"><a href="factor.html#introduction"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="factor.html"><a href="factor.html#detecting-anomalies"><i class="fa fa-check"></i><b>4.2</b> Detecting anomalies</a><ul>
<li class="chapter" data-level="4.2.1" data-path="factor.html"><a href="factor.html#simple-portfolio-sorts"><i class="fa fa-check"></i><b>4.2.1</b> Simple portfolio sorts</a></li>
<li class="chapter" data-level="4.2.2" data-path="factor.html"><a href="factor.html#factors"><i class="fa fa-check"></i><b>4.2.2</b> Factors</a></li>
<li class="chapter" data-level="4.2.3" data-path="factor.html"><a href="factor.html#predictive-regressions-sorts-and-p-value-issues"><i class="fa fa-check"></i><b>4.2.3</b> Predictive regressions, sorts, and p-value issues</a></li>
<li class="chapter" data-level="4.2.4" data-path="factor.html"><a href="factor.html#fama-macbeth-regressions"><i class="fa fa-check"></i><b>4.2.4</b> Fama-Macbeth regressions</a></li>
<li class="chapter" data-level="4.2.5" data-path="factor.html"><a href="factor.html#factor-competition"><i class="fa fa-check"></i><b>4.2.5</b> Factor competition</a></li>
<li class="chapter" data-level="4.2.6" data-path="factor.html"><a href="factor.html#advanced-techniques"><i class="fa fa-check"></i><b>4.2.6</b> Advanced techniques</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="factor.html"><a href="factor.html#factors-or-characteristics"><i class="fa fa-check"></i><b>4.3</b> Factors or characteristics?</a></li>
<li class="chapter" data-level="4.4" data-path="factor.html"><a href="factor.html#momentum-and-timing"><i class="fa fa-check"></i><b>4.4</b> Momentum and timing</a><ul>
<li class="chapter" data-level="4.4.1" data-path="factor.html"><a href="factor.html#factor-momentum"><i class="fa fa-check"></i><b>4.4.1</b> Factor momentum</a></li>
<li class="chapter" data-level="4.4.2" data-path="factor.html"><a href="factor.html#factor-timing"><i class="fa fa-check"></i><b>4.4.2</b> Factor timing</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="factor.html"><a href="factor.html#the-link-with-machine-learning"><i class="fa fa-check"></i><b>4.5</b> The link with machine learning</a><ul>
<li class="chapter" data-level="4.5.1" data-path="factor.html"><a href="factor.html#a-short-list-of-recent-references"><i class="fa fa-check"></i><b>4.5.1</b> A short list of recent references</a></li>
<li class="chapter" data-level="4.5.2" data-path="factor.html"><a href="factor.html#explicit-connections-with-asset-pricing-models"><i class="fa fa-check"></i><b>4.5.2</b> Explicit connections with asset pricing models</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="factor.html"><a href="factor.html#coding-exercises"><i class="fa fa-check"></i><b>4.6</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="Data.html"><a href="Data.html"><i class="fa fa-check"></i><b>5</b> Data preprocessing</a><ul>
<li class="chapter" data-level="5.1" data-path="Data.html"><a href="Data.html#know-your-data"><i class="fa fa-check"></i><b>5.1</b> Know your data</a></li>
<li class="chapter" data-level="5.2" data-path="Data.html"><a href="Data.html#missing-data"><i class="fa fa-check"></i><b>5.2</b> Missing data</a></li>
<li class="chapter" data-level="5.3" data-path="Data.html"><a href="Data.html#outlier-detection"><i class="fa fa-check"></i><b>5.3</b> Outlier detection</a></li>
<li class="chapter" data-level="5.4" data-path="Data.html"><a href="Data.html#feateng"><i class="fa fa-check"></i><b>5.4</b> Feature engineering</a><ul>
<li class="chapter" data-level="5.4.1" data-path="Data.html"><a href="Data.html#feature-selection"><i class="fa fa-check"></i><b>5.4.1</b> Feature selection</a></li>
<li class="chapter" data-level="5.4.2" data-path="Data.html"><a href="Data.html#scaling"><i class="fa fa-check"></i><b>5.4.2</b> Scaling the predictors</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="Data.html"><a href="Data.html#labelling"><i class="fa fa-check"></i><b>5.5</b> Labelling</a><ul>
<li class="chapter" data-level="5.5.1" data-path="Data.html"><a href="Data.html#simple-labels"><i class="fa fa-check"></i><b>5.5.1</b> Simple labels</a></li>
<li class="chapter" data-level="5.5.2" data-path="Data.html"><a href="Data.html#categorical-labels"><i class="fa fa-check"></i><b>5.5.2</b> Categorical labels</a></li>
<li class="chapter" data-level="5.5.3" data-path="Data.html"><a href="Data.html#the-triple-barrier-method"><i class="fa fa-check"></i><b>5.5.3</b> The triple barrier method</a></li>
<li class="chapter" data-level="5.5.4" data-path="Data.html"><a href="Data.html#filtering-the-sample"><i class="fa fa-check"></i><b>5.5.4</b> Filtering the sample</a></li>
<li class="chapter" data-level="5.5.5" data-path="Data.html"><a href="Data.html#horizons"><i class="fa fa-check"></i><b>5.5.5</b> Return horizons</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="Data.html"><a href="Data.html#pers"><i class="fa fa-check"></i><b>5.6</b> Handling persistence</a></li>
<li class="chapter" data-level="5.7" data-path="Data.html"><a href="Data.html#extensions"><i class="fa fa-check"></i><b>5.7</b> Extensions</a><ul>
<li class="chapter" data-level="5.7.1" data-path="Data.html"><a href="Data.html#transforming-features"><i class="fa fa-check"></i><b>5.7.1</b> Transforming features</a></li>
<li class="chapter" data-level="5.7.2" data-path="Data.html"><a href="Data.html#macrovar"><i class="fa fa-check"></i><b>5.7.2</b> Macro-economic variables</a></li>
<li class="chapter" data-level="5.7.3" data-path="Data.html"><a href="Data.html#active-learning"><i class="fa fa-check"></i><b>5.7.3</b> Active learning</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="Data.html"><a href="Data.html#additional-code-and-results"><i class="fa fa-check"></i><b>5.8</b> Additional code and results</a><ul>
<li class="chapter" data-level="5.8.1" data-path="Data.html"><a href="Data.html#impact-of-rescaling-graphical-representation"><i class="fa fa-check"></i><b>5.8.1</b> Impact of rescaling: graphical representation</a></li>
<li class="chapter" data-level="5.8.2" data-path="Data.html"><a href="Data.html#impact-of-rescaling-toy-example"><i class="fa fa-check"></i><b>5.8.2</b> Impact of rescaling: toy example</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="Data.html"><a href="Data.html#coding-exercises-1"><i class="fa fa-check"></i><b>5.9</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>6</b> Penalized regressions and sparse hedging for minimum variance portfolios</a><ul>
<li class="chapter" data-level="6.1" data-path="lasso.html"><a href="lasso.html#penalised-regressions"><i class="fa fa-check"></i><b>6.1</b> Penalised regressions</a><ul>
<li class="chapter" data-level="6.1.1" data-path="lasso.html"><a href="lasso.html#penreg"><i class="fa fa-check"></i><b>6.1.1</b> Simple regressions</a></li>
<li class="chapter" data-level="6.1.2" data-path="lasso.html"><a href="lasso.html#forms-of-penalizations"><i class="fa fa-check"></i><b>6.1.2</b> Forms of penalizations</a></li>
<li class="chapter" data-level="6.1.3" data-path="lasso.html"><a href="lasso.html#illustrations"><i class="fa fa-check"></i><b>6.1.3</b> Illustrations</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="lasso.html"><a href="lasso.html#sparse-hedging-for-minimum-variance-portfolios"><i class="fa fa-check"></i><b>6.2</b> Sparse hedging for minimum variance portfolios</a><ul>
<li class="chapter" data-level="6.2.1" data-path="lasso.html"><a href="lasso.html#presentation-and-derivations"><i class="fa fa-check"></i><b>6.2.1</b> Presentation and derivations</a></li>
<li class="chapter" data-level="6.2.2" data-path="lasso.html"><a href="lasso.html#sparseex"><i class="fa fa-check"></i><b>6.2.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="lasso.html"><a href="lasso.html#predictive-regressions"><i class="fa fa-check"></i><b>6.3</b> Predictive regressions</a><ul>
<li class="chapter" data-level="6.3.1" data-path="lasso.html"><a href="lasso.html#literature-review-and-principle"><i class="fa fa-check"></i><b>6.3.1</b> Literature review and principle</a></li>
<li class="chapter" data-level="6.3.2" data-path="lasso.html"><a href="lasso.html#code-and-results"><i class="fa fa-check"></i><b>6.3.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="lasso.html"><a href="lasso.html#coding-exercise"><i class="fa fa-check"></i><b>6.4</b> Coding exercise</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>7</b> Tree-based methods</a><ul>
<li class="chapter" data-level="7.1" data-path="trees.html"><a href="trees.html#simple-trees"><i class="fa fa-check"></i><b>7.1</b> Simple trees</a><ul>
<li class="chapter" data-level="7.1.1" data-path="trees.html"><a href="trees.html#principle"><i class="fa fa-check"></i><b>7.1.1</b> Principle</a></li>
<li class="chapter" data-level="7.1.2" data-path="trees.html"><a href="trees.html#treeclass"><i class="fa fa-check"></i><b>7.1.2</b> Further details on classification</a></li>
<li class="chapter" data-level="7.1.3" data-path="trees.html"><a href="trees.html#pruning-criteria"><i class="fa fa-check"></i><b>7.1.3</b> Pruning criteria</a></li>
<li class="chapter" data-level="7.1.4" data-path="trees.html"><a href="trees.html#code-and-interpretation"><i class="fa fa-check"></i><b>7.1.4</b> Code and interpretation</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="trees.html"><a href="trees.html#random-forests"><i class="fa fa-check"></i><b>7.2</b> Random forests</a><ul>
<li class="chapter" data-level="7.2.1" data-path="trees.html"><a href="trees.html#principle-1"><i class="fa fa-check"></i><b>7.2.1</b> Principle</a></li>
<li class="chapter" data-level="7.2.2" data-path="trees.html"><a href="trees.html#code-and-results-1"><i class="fa fa-check"></i><b>7.2.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="trees.html"><a href="trees.html#adaboost"><i class="fa fa-check"></i><b>7.3</b> Boosted trees: Adaboost</a><ul>
<li class="chapter" data-level="7.3.1" data-path="trees.html"><a href="trees.html#methodology"><i class="fa fa-check"></i><b>7.3.1</b> Methodology</a></li>
<li class="chapter" data-level="7.3.2" data-path="trees.html"><a href="trees.html#illustration"><i class="fa fa-check"></i><b>7.3.2</b> Illustration</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="trees.html"><a href="trees.html#boosted-trees-extreme-gradient-boosting"><i class="fa fa-check"></i><b>7.4</b> Boosted trees: extreme gradient boosting</a><ul>
<li class="chapter" data-level="7.4.1" data-path="trees.html"><a href="trees.html#managing-loss"><i class="fa fa-check"></i><b>7.4.1</b> Managing Loss</a></li>
<li class="chapter" data-level="7.4.2" data-path="trees.html"><a href="trees.html#penalisation"><i class="fa fa-check"></i><b>7.4.2</b> Penalisation</a></li>
<li class="chapter" data-level="7.4.3" data-path="trees.html"><a href="trees.html#aggregation"><i class="fa fa-check"></i><b>7.4.3</b> Aggregation</a></li>
<li class="chapter" data-level="7.4.4" data-path="trees.html"><a href="trees.html#tree-structure"><i class="fa fa-check"></i><b>7.4.4</b> Tree structure</a></li>
<li class="chapter" data-level="7.4.5" data-path="trees.html"><a href="trees.html#boostext"><i class="fa fa-check"></i><b>7.4.5</b> Extensions</a></li>
<li class="chapter" data-level="7.4.6" data-path="trees.html"><a href="trees.html#boostcode"><i class="fa fa-check"></i><b>7.4.6</b> Code and results</a></li>
<li class="chapter" data-level="7.4.7" data-path="trees.html"><a href="trees.html#instweight"><i class="fa fa-check"></i><b>7.4.7</b> Instance weighting</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="trees.html"><a href="trees.html#discussion"><i class="fa fa-check"></i><b>7.5</b> Discussion</a></li>
<li class="chapter" data-level="7.6" data-path="trees.html"><a href="trees.html#coding-exercises-2"><i class="fa fa-check"></i><b>7.6</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="NN.html"><a href="NN.html"><i class="fa fa-check"></i><b>8</b> Neural networks</a><ul>
<li class="chapter" data-level="8.1" data-path="NN.html"><a href="NN.html#the-original-perceptron"><i class="fa fa-check"></i><b>8.1</b> The original perceptron</a></li>
<li class="chapter" data-level="8.2" data-path="NN.html"><a href="NN.html#multilayer-perceptron-mlp"><i class="fa fa-check"></i><b>8.2</b> Multilayer perceptron (MLP)</a><ul>
<li class="chapter" data-level="8.2.1" data-path="NN.html"><a href="NN.html#introduction-and-notations"><i class="fa fa-check"></i><b>8.2.1</b> Introduction and notations</a></li>
<li class="chapter" data-level="8.2.2" data-path="NN.html"><a href="NN.html#universal-approximation"><i class="fa fa-check"></i><b>8.2.2</b> Universal approximation</a></li>
<li class="chapter" data-level="8.2.3" data-path="NN.html"><a href="NN.html#backprop"><i class="fa fa-check"></i><b>8.2.3</b> Learning via back-propagation</a></li>
<li class="chapter" data-level="8.2.4" data-path="NN.html"><a href="NN.html#further-details-on-classification"><i class="fa fa-check"></i><b>8.2.4</b> Further details on classification</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="NN.html"><a href="NN.html#howdeep"><i class="fa fa-check"></i><b>8.3</b> How deep should we go? And other practical issues</a><ul>
<li class="chapter" data-level="8.3.1" data-path="NN.html"><a href="NN.html#architectural-choices"><i class="fa fa-check"></i><b>8.3.1</b> Architectural choices</a></li>
<li class="chapter" data-level="8.3.2" data-path="NN.html"><a href="NN.html#frequency-of-weight-updates-and-learning-duration"><i class="fa fa-check"></i><b>8.3.2</b> Frequency of weight updates and learning duration</a></li>
<li class="chapter" data-level="8.3.3" data-path="NN.html"><a href="NN.html#penalizations-and-dropout"><i class="fa fa-check"></i><b>8.3.3</b> Penalizations and dropout</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="NN.html"><a href="NN.html#code-samples-and-comments-for-vanilla-mlp"><i class="fa fa-check"></i><b>8.4</b> Code samples and comments for vanilla MLP</a><ul>
<li class="chapter" data-level="8.4.1" data-path="NN.html"><a href="NN.html#regression-example"><i class="fa fa-check"></i><b>8.4.1</b> Regression example</a></li>
<li class="chapter" data-level="8.4.2" data-path="NN.html"><a href="NN.html#classification-example"><i class="fa fa-check"></i><b>8.4.2</b> Classification example</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="NN.html"><a href="NN.html#recurrent-networks"><i class="fa fa-check"></i><b>8.5</b> Recurrent networks</a><ul>
<li class="chapter" data-level="8.5.1" data-path="NN.html"><a href="NN.html#presentation"><i class="fa fa-check"></i><b>8.5.1</b> Presentation</a></li>
<li class="chapter" data-level="8.5.2" data-path="NN.html"><a href="NN.html#code-and-results-2"><i class="fa fa-check"></i><b>8.5.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="NN.html"><a href="NN.html#other-common-architectures"><i class="fa fa-check"></i><b>8.6</b> Other common architectures</a><ul>
<li class="chapter" data-level="8.6.1" data-path="NN.html"><a href="NN.html#generative-aversarial-networks"><i class="fa fa-check"></i><b>8.6.1</b> Generative adversarial networks</a></li>
<li class="chapter" data-level="8.6.2" data-path="NN.html"><a href="NN.html#autoencoders"><i class="fa fa-check"></i><b>8.6.2</b> Auto-encoders</a></li>
<li class="chapter" data-level="8.6.3" data-path="NN.html"><a href="NN.html#a-word-on-convolutional-networks"><i class="fa fa-check"></i><b>8.6.3</b> A word on convolutional networks</a></li>
<li class="chapter" data-level="8.6.4" data-path="NN.html"><a href="NN.html#advanced-architectures"><i class="fa fa-check"></i><b>8.6.4</b> Advanced architectures</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="NN.html"><a href="NN.html#coding-exercise-1"><i class="fa fa-check"></i><b>8.7</b> Coding exercise</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>9</b> Support vector machines</a><ul>
<li class="chapter" data-level="9.1" data-path="svm.html"><a href="svm.html#svm-for-classification"><i class="fa fa-check"></i><b>9.1</b> SVM for classification</a></li>
<li class="chapter" data-level="9.2" data-path="svm.html"><a href="svm.html#svm-for-regression"><i class="fa fa-check"></i><b>9.2</b> SVM for regression</a></li>
<li class="chapter" data-level="9.3" data-path="svm.html"><a href="svm.html#practice"><i class="fa fa-check"></i><b>9.3</b> Practice</a></li>
<li class="chapter" data-level="9.4" data-path="svm.html"><a href="svm.html#coding-exercises-3"><i class="fa fa-check"></i><b>9.4</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="bayes.html"><a href="bayes.html"><i class="fa fa-check"></i><b>10</b> Bayesian methods</a><ul>
<li class="chapter" data-level="10.1" data-path="bayes.html"><a href="bayes.html#the-bayesian-framework"><i class="fa fa-check"></i><b>10.1</b> The Bayesian framework</a></li>
<li class="chapter" data-level="10.2" data-path="bayes.html"><a href="bayes.html#bayesian-sampling"><i class="fa fa-check"></i><b>10.2</b> Bayesian sampling</a><ul>
<li class="chapter" data-level="10.2.1" data-path="bayes.html"><a href="bayes.html#gibbs-sampling"><i class="fa fa-check"></i><b>10.2.1</b> Gibbs sampling</a></li>
<li class="chapter" data-level="10.2.2" data-path="bayes.html"><a href="bayes.html#metropolis-hastings-sampling"><i class="fa fa-check"></i><b>10.2.2</b> Metropolis-Hastings sampling</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="bayes.html"><a href="bayes.html#bayesian-linear-regression"><i class="fa fa-check"></i><b>10.3</b> Bayesian linear regression</a></li>
<li class="chapter" data-level="10.4" data-path="bayes.html"><a href="bayes.html#naive-bayes-classifier"><i class="fa fa-check"></i><b>10.4</b> Naive Bayes classifier</a></li>
<li class="chapter" data-level="10.5" data-path="bayes.html"><a href="bayes.html#BART"><i class="fa fa-check"></i><b>10.5</b> Bayesian additive trees</a><ul>
<li class="chapter" data-level="10.5.1" data-path="bayes.html"><a href="bayes.html#general-formulation"><i class="fa fa-check"></i><b>10.5.1</b> General formulation</a></li>
<li class="chapter" data-level="10.5.2" data-path="bayes.html"><a href="bayes.html#priors"><i class="fa fa-check"></i><b>10.5.2</b> Priors</a></li>
<li class="chapter" data-level="10.5.3" data-path="bayes.html"><a href="bayes.html#sampling-and-predictions"><i class="fa fa-check"></i><b>10.5.3</b> Sampling and predictions</a></li>
<li class="chapter" data-level="10.5.4" data-path="bayes.html"><a href="bayes.html#code"><i class="fa fa-check"></i><b>10.5.4</b> Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="valtune.html"><a href="valtune.html"><i class="fa fa-check"></i><b>11</b> Validating and tuning</a><ul>
<li class="chapter" data-level="11.1" data-path="valtune.html"><a href="valtune.html#mlmetrics"><i class="fa fa-check"></i><b>11.1</b> Learning metrics</a><ul>
<li class="chapter" data-level="11.1.1" data-path="valtune.html"><a href="valtune.html#regression-analysis"><i class="fa fa-check"></i><b>11.1.1</b> Regression analysis</a></li>
<li class="chapter" data-level="11.1.2" data-path="valtune.html"><a href="valtune.html#classification-analysis"><i class="fa fa-check"></i><b>11.1.2</b> Classification analysis</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="valtune.html"><a href="valtune.html#validation"><i class="fa fa-check"></i><b>11.2</b> Validation</a><ul>
<li class="chapter" data-level="11.2.1" data-path="valtune.html"><a href="valtune.html#the-variance-bias-tradeoff-theory"><i class="fa fa-check"></i><b>11.2.1</b> The variance-bias tradeoff: theory</a></li>
<li class="chapter" data-level="11.2.2" data-path="valtune.html"><a href="valtune.html#the-variance-bias-tradeoff-illustration"><i class="fa fa-check"></i><b>11.2.2</b> The variance-bias tradeoff: illustration</a></li>
<li class="chapter" data-level="11.2.3" data-path="valtune.html"><a href="valtune.html#the-risk-of-overfitting-principle"><i class="fa fa-check"></i><b>11.2.3</b> The risk of overfitting: principle</a></li>
<li class="chapter" data-level="11.2.4" data-path="valtune.html"><a href="valtune.html#the-risk-of-overfitting-some-solutions"><i class="fa fa-check"></i><b>11.2.4</b> The risk of overfitting: some solutions</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="valtune.html"><a href="valtune.html#the-search-for-good-hyperparameters"><i class="fa fa-check"></i><b>11.3</b> The search for good hyperparameters</a><ul>
<li class="chapter" data-level="11.3.1" data-path="valtune.html"><a href="valtune.html#methods"><i class="fa fa-check"></i><b>11.3.1</b> Methods</a></li>
<li class="chapter" data-level="11.3.2" data-path="valtune.html"><a href="valtune.html#example-grid-search"><i class="fa fa-check"></i><b>11.3.2</b> Example: grid search</a></li>
<li class="chapter" data-level="11.3.3" data-path="valtune.html"><a href="valtune.html#example-bayesian-optimization"><i class="fa fa-check"></i><b>11.3.3</b> Example: Bayesian optimization</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="valtune.html"><a href="valtune.html#short-discussion-on-validation-in-backtests"><i class="fa fa-check"></i><b>11.4</b> Short discussion on validation in backtests</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ensemble.html"><a href="ensemble.html"><i class="fa fa-check"></i><b>12</b> Ensemble models</a><ul>
<li class="chapter" data-level="12.1" data-path="ensemble.html"><a href="ensemble.html#linear-ensembles"><i class="fa fa-check"></i><b>12.1</b> Linear ensembles</a><ul>
<li class="chapter" data-level="12.1.1" data-path="ensemble.html"><a href="ensemble.html#principles"><i class="fa fa-check"></i><b>12.1.1</b> Principles</a></li>
<li class="chapter" data-level="12.1.2" data-path="ensemble.html"><a href="ensemble.html#example"><i class="fa fa-check"></i><b>12.1.2</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="ensemble.html"><a href="ensemble.html#stacked-ensembles"><i class="fa fa-check"></i><b>12.2</b> Stacked ensembles</a><ul>
<li class="chapter" data-level="12.2.1" data-path="ensemble.html"><a href="ensemble.html#two-stage-training"><i class="fa fa-check"></i><b>12.2.1</b> Two stage training</a></li>
<li class="chapter" data-level="12.2.2" data-path="ensemble.html"><a href="ensemble.html#code-and-results-3"><i class="fa fa-check"></i><b>12.2.2</b> Code and results</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="ensemble.html"><a href="ensemble.html#extensions-1"><i class="fa fa-check"></i><b>12.3</b> Extensions</a><ul>
<li class="chapter" data-level="12.3.1" data-path="ensemble.html"><a href="ensemble.html#exogenous-variables"><i class="fa fa-check"></i><b>12.3.1</b> Exogenous variables</a></li>
<li class="chapter" data-level="12.3.2" data-path="ensemble.html"><a href="ensemble.html#shrinking-inter-model-correlations"><i class="fa fa-check"></i><b>12.3.2</b> Shrinking inter-model correlations</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="ensemble.html"><a href="ensemble.html#exercise"><i class="fa fa-check"></i><b>12.4</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="backtest.html"><a href="backtest.html"><i class="fa fa-check"></i><b>13</b> Portfolio backtesting</a><ul>
<li class="chapter" data-level="13.1" data-path="backtest.html"><a href="backtest.html#protocol"><i class="fa fa-check"></i><b>13.1</b> Setting the protocol</a></li>
<li class="chapter" data-level="13.2" data-path="backtest.html"><a href="backtest.html#turning-signals-into-portfolio-weights"><i class="fa fa-check"></i><b>13.2</b> Turning signals into portfolio weights</a></li>
<li class="chapter" data-level="13.3" data-path="backtest.html"><a href="backtest.html#perfmet"><i class="fa fa-check"></i><b>13.3</b> Performance metrics</a><ul>
<li class="chapter" data-level="13.3.1" data-path="backtest.html"><a href="backtest.html#discussion-1"><i class="fa fa-check"></i><b>13.3.1</b> Discussion</a></li>
<li class="chapter" data-level="13.3.2" data-path="backtest.html"><a href="backtest.html#pure-performance-and-risk-indicators"><i class="fa fa-check"></i><b>13.3.2</b> Pure performance and risk indicators</a></li>
<li class="chapter" data-level="13.3.3" data-path="backtest.html"><a href="backtest.html#factor-based-evaluation"><i class="fa fa-check"></i><b>13.3.3</b> Factor-based evaluation</a></li>
<li class="chapter" data-level="13.3.4" data-path="backtest.html"><a href="backtest.html#risk-adjusted-measures"><i class="fa fa-check"></i><b>13.3.4</b> Risk-adjusted measures</a></li>
<li class="chapter" data-level="13.3.5" data-path="backtest.html"><a href="backtest.html#transaction-costs-and-turnover"><i class="fa fa-check"></i><b>13.3.5</b> Transaction costs and turnover</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="backtest.html"><a href="backtest.html#common-errors-and-issues"><i class="fa fa-check"></i><b>13.4</b> Common errors and issues</a><ul>
<li class="chapter" data-level="13.4.1" data-path="backtest.html"><a href="backtest.html#forward-looking-data"><i class="fa fa-check"></i><b>13.4.1</b> Forward looking data</a></li>
<li class="chapter" data-level="13.4.2" data-path="backtest.html"><a href="backtest.html#backtest-overfitting"><i class="fa fa-check"></i><b>13.4.2</b> Backtest overfitting</a></li>
<li class="chapter" data-level="13.4.3" data-path="backtest.html"><a href="backtest.html#simple-safeguards"><i class="fa fa-check"></i><b>13.4.3</b> Simple safeguards</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="backtest.html"><a href="backtest.html#implication-of-non-stationarity-forecasting-is-hard"><i class="fa fa-check"></i><b>13.5</b> Implication of non-stationarity: forecasting is hard</a><ul>
<li class="chapter" data-level="13.5.1" data-path="backtest.html"><a href="backtest.html#general-comments"><i class="fa fa-check"></i><b>13.5.1</b> General comments</a></li>
<li class="chapter" data-level="13.5.2" data-path="backtest.html"><a href="backtest.html#the-no-free-lunch-theorem"><i class="fa fa-check"></i><b>13.5.2</b> The no free lunch theorem</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="backtest.html"><a href="backtest.html#example-1"><i class="fa fa-check"></i><b>13.6</b> Example</a></li>
<li class="chapter" data-level="13.7" data-path="backtest.html"><a href="backtest.html#coding-exercises-4"><i class="fa fa-check"></i><b>13.7</b> Coding exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="interp.html"><a href="interp.html"><i class="fa fa-check"></i><b>14</b> Interpretability</a><ul>
<li class="chapter" data-level="14.1" data-path="interp.html"><a href="interp.html#global-interpretations"><i class="fa fa-check"></i><b>14.1</b> Global interpretations</a><ul>
<li class="chapter" data-level="14.1.1" data-path="interp.html"><a href="interp.html#variable-importance"><i class="fa fa-check"></i><b>14.1.1</b> Variable importance (tree-based)</a></li>
<li class="chapter" data-level="14.1.2" data-path="interp.html"><a href="interp.html#variable-importance-agnostic"><i class="fa fa-check"></i><b>14.1.2</b> Variable importance (agnostic)</a></li>
<li class="chapter" data-level="14.1.3" data-path="interp.html"><a href="interp.html#partial-dependence-plot"><i class="fa fa-check"></i><b>14.1.3</b> Partial dependence plot</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="interp.html"><a href="interp.html#local-interpretations"><i class="fa fa-check"></i><b>14.2</b> Local interpretations</a><ul>
<li class="chapter" data-level="14.2.1" data-path="interp.html"><a href="interp.html#lime"><i class="fa fa-check"></i><b>14.2.1</b> LIME</a></li>
<li class="chapter" data-level="14.2.2" data-path="interp.html"><a href="interp.html#shapley-values"><i class="fa fa-check"></i><b>14.2.2</b> Shapley values</a></li>
<li class="chapter" data-level="14.2.3" data-path="interp.html"><a href="interp.html#breakdown"><i class="fa fa-check"></i><b>14.2.3</b> Breakdown</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="causality.html"><a href="causality.html"><i class="fa fa-check"></i><b>15</b> Two key concepts: causality and non-stationarity</a><ul>
<li class="chapter" data-level="15.1" data-path="causality.html"><a href="causality.html#causality-1"><i class="fa fa-check"></i><b>15.1</b> Causality</a><ul>
<li class="chapter" data-level="15.1.1" data-path="causality.html"><a href="causality.html#granger"><i class="fa fa-check"></i><b>15.1.1</b> Granger causality</a></li>
<li class="chapter" data-level="15.1.2" data-path="causality.html"><a href="causality.html#causal-additive-models"><i class="fa fa-check"></i><b>15.1.2</b> Causal additive models</a></li>
<li class="chapter" data-level="15.1.3" data-path="causality.html"><a href="causality.html#structural-time-series-models"><i class="fa fa-check"></i><b>15.1.3</b> Structural time-series models</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="causality.html"><a href="causality.html#nonstat"><i class="fa fa-check"></i><b>15.2</b> Dealing with changing environments</a><ul>
<li class="chapter" data-level="15.2.1" data-path="causality.html"><a href="causality.html#non-stationarity-yet-another-illustration"><i class="fa fa-check"></i><b>15.2.1</b> Non-stationarity: yet another illustration</a></li>
<li class="chapter" data-level="15.2.2" data-path="causality.html"><a href="causality.html#online-learning"><i class="fa fa-check"></i><b>15.2.2</b> Online learning</a></li>
<li class="chapter" data-level="15.2.3" data-path="causality.html"><a href="causality.html#homogeneous-transfer-learning"><i class="fa fa-check"></i><b>15.2.3</b> Homogeneous transfer learning</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="unsup.html"><a href="unsup.html"><i class="fa fa-check"></i><b>16</b> Unsupervised learning</a><ul>
<li class="chapter" data-level="16.1" data-path="unsup.html"><a href="unsup.html#corpred"><i class="fa fa-check"></i><b>16.1</b> The problem with correlated predictors</a></li>
<li class="chapter" data-level="16.2" data-path="unsup.html"><a href="unsup.html#principal-component-analysis-and-autoencoders"><i class="fa fa-check"></i><b>16.2</b> Principal component analysis and autoencoders</a><ul>
<li class="chapter" data-level="16.2.1" data-path="unsup.html"><a href="unsup.html#a-bit-of-algebra"><i class="fa fa-check"></i><b>16.2.1</b> A bit of algebra</a></li>
<li class="chapter" data-level="16.2.2" data-path="unsup.html"><a href="unsup.html#pca"><i class="fa fa-check"></i><b>16.2.2</b> PCA</a></li>
<li class="chapter" data-level="16.2.3" data-path="unsup.html"><a href="unsup.html#ae"><i class="fa fa-check"></i><b>16.2.3</b> Autoencoders</a></li>
<li class="chapter" data-level="16.2.4" data-path="unsup.html"><a href="unsup.html#application"><i class="fa fa-check"></i><b>16.2.4</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="unsup.html"><a href="unsup.html#clustering-via-k-means"><i class="fa fa-check"></i><b>16.3</b> Clustering via k-means</a></li>
<li class="chapter" data-level="16.4" data-path="unsup.html"><a href="unsup.html#nearest-neighbors"><i class="fa fa-check"></i><b>16.4</b> Nearest neighbors</a></li>
<li class="chapter" data-level="16.5" data-path="unsup.html"><a href="unsup.html#coding-exercise-2"><i class="fa fa-check"></i><b>16.5</b> Coding exercise</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="RL.html"><a href="RL.html"><i class="fa fa-check"></i><b>17</b> Reinforcement learning</a><ul>
<li class="chapter" data-level="17.1" data-path="RL.html"><a href="RL.html#theoretical-layout"><i class="fa fa-check"></i><b>17.1</b> Theoretical layout</a><ul>
<li class="chapter" data-level="17.1.1" data-path="RL.html"><a href="RL.html#general-framework"><i class="fa fa-check"></i><b>17.1.1</b> General framework</a></li>
<li class="chapter" data-level="17.1.2" data-path="RL.html"><a href="RL.html#q-learning"><i class="fa fa-check"></i><b>17.1.2</b> Q-learning</a></li>
<li class="chapter" data-level="17.1.3" data-path="RL.html"><a href="RL.html#sarsa"><i class="fa fa-check"></i><b>17.1.3</b> SARSA</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="RL.html"><a href="RL.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>17.2</b> The curse of dimensionality</a></li>
<li class="chapter" data-level="17.3" data-path="RL.html"><a href="RL.html#policy-gradient"><i class="fa fa-check"></i><b>17.3</b> Policy gradient</a><ul>
<li class="chapter" data-level="17.3.1" data-path="RL.html"><a href="RL.html#principle-2"><i class="fa fa-check"></i><b>17.3.1</b> Principle</a></li>
<li class="chapter" data-level="17.3.2" data-path="RL.html"><a href="RL.html#extensions-2"><i class="fa fa-check"></i><b>17.3.2</b> Extensions</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="RL.html"><a href="RL.html#simple-examples"><i class="fa fa-check"></i><b>17.4</b> Simple examples</a><ul>
<li class="chapter" data-level="17.4.1" data-path="RL.html"><a href="RL.html#q-learning-with-simulations"><i class="fa fa-check"></i><b>17.4.1</b> Q-learning with simulations</a></li>
<li class="chapter" data-level="17.4.2" data-path="RL.html"><a href="RL.html#RLemp2"><i class="fa fa-check"></i><b>17.4.2</b> Q-learning with market data</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="RL.html"><a href="RL.html#concluding-remarks"><i class="fa fa-check"></i><b>17.5</b> Concluding remarks</a></li>
<li class="chapter" data-level="17.6" data-path="RL.html"><a href="RL.html#exercises"><i class="fa fa-check"></i><b>17.6</b> Exercises</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="data-description.html"><a href="data-description.html"><i class="fa fa-check"></i><b>A</b> Data Description</a></li>
<li class="chapter" data-level="B" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html"><i class="fa fa-check"></i><b>B</b> Solution to exercises</a><ul>
<li class="chapter" data-level="B.1" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-4"><i class="fa fa-check"></i><b>B.1</b> Chapter 4</a></li>
<li class="chapter" data-level="B.2" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-5"><i class="fa fa-check"></i><b>B.2</b> Chapter 5</a></li>
<li class="chapter" data-level="B.3" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-6"><i class="fa fa-check"></i><b>B.3</b> Chapter 6</a></li>
<li class="chapter" data-level="B.4" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-7"><i class="fa fa-check"></i><b>B.4</b> Chapter 7</a></li>
<li class="chapter" data-level="B.5" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-8-the-autoencoder-model"><i class="fa fa-check"></i><b>B.5</b> Chapter 8: the autoencoder model</a></li>
<li class="chapter" data-level="B.6" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-9"><i class="fa fa-check"></i><b>B.6</b> Chapter 9</a></li>
<li class="chapter" data-level="B.7" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-12-ensemble-neural-network"><i class="fa fa-check"></i><b>B.7</b> Chapter 12: ensemble neural network</a></li>
<li class="chapter" data-level="B.8" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-13"><i class="fa fa-check"></i><b>B.8</b> Chapter 13</a><ul>
<li class="chapter" data-level="B.8.1" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#ew-portfolios-with-the-tidyverse"><i class="fa fa-check"></i><b>B.8.1</b> EW portfolios with the tidyverse</a></li>
<li class="chapter" data-level="B.8.2" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#advanced-weighting-function"><i class="fa fa-check"></i><b>B.8.2</b> Advanced weighting function</a></li>
<li class="chapter" data-level="B.8.3" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#functional-programming-in-the-backtest"><i class="fa fa-check"></i><b>B.8.3</b> Functional programming in the backtest</a></li>
</ul></li>
<li class="chapter" data-level="B.9" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-16"><i class="fa fa-check"></i><b>B.9</b> Chapter 16</a></li>
<li class="chapter" data-level="B.10" data-path="solution-to-exercises.html"><a href="solution-to-exercises.html#chapter-17"><i class="fa fa-check"></i><b>B.10</b> Chapter 17</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning for Factor Investing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ensemble" class="section level1">
<h1><span class="header-section-number">Chapter 12</span> Ensemble models</h1>
<p>Let us be honest. When facing a prediction task, it is not obvious to determine the best choice between ML tools: penalized regressions, tree methods, neural networks, SVMs, etc. A natural and tempting alternative is to <strong>combine</strong> several algorithms (or the predictions that result from them) to try to extract value out of each engine (or learner). This intention is not new and contributions towards this goal go back at least to <span class="citation">Bates and Granger (<a href="#ref-bates1969combination">1969</a>)</span> (for the prupose of passenger flow forecasting).</p>
<p>Below, we outline a few books on the topic of ensembles. The latter have many names and synonyms, such as <strong>forecast aggregation</strong>, <strong>model averaging</strong>, <strong>mixture of experts</strong> or <strong>prediction combination</strong>. The first four references below are monographs while the last two are compilations of contributions:</p>
<ul>
<li><span class="citation">Zhou (<a href="#ref-zhou2012ensemble">2012</a>)</span>: a very didactic book that covers the main ideas of ensembles;<br />
</li>
<li><span class="citation">Schapire and Freund (<a href="#ref-schapire2012boosting">2012</a>)</span>: the main reference for boosting (and hence, ensembling) with many theoretical results and thus strong mathematical groundings;<br />
</li>
<li><span class="citation">Seni and Elder (<a href="#ref-seni2010ensemble">2010</a>)</span>: an introduction dedicated to tree methods mainly;<br />
</li>
<li><span class="citation">Claeskens and Hjort (<a href="#ref-claeskens2008model">2008</a>)</span>: an overview of model selection techniques with a few chapter focused on model averaging;<br />
</li>
<li><span class="citation">Zhang and Ma (<a href="#ref-zhang2012ensemble">2012</a>)</span>: a collection of thematic chapters on ensemble learning;<br />
</li>
<li><span class="citation">Okun, Valentini, and Re (<a href="#ref-okun2011ensembles">2011</a>)</span>: examples of applications of ensembles.</li>
</ul>
<p>In this chapter, we cover the basic ideas and concepts behind the notion of ensembles. We refer to the above books for deeper treatments on the topic. We underline that several ensemble methods have already been mentioned and covered earlier, notably in Chapter <a href="trees.html#trees">7</a>. Indeed, random forests and boosted trees are examples of ensembles. Hence, other early articles on the combination of learners are <span class="citation">Schapire (<a href="#ref-schapire1990strength">1990</a>)</span>, <span class="citation">Jacobs et al. (<a href="#ref-jacobs1991adaptive">1991</a>)</span> (for neural networks particularly), and <span class="citation">Freund and Schapire (<a href="#ref-freund1997decision">1997</a>)</span>.</p>
<div id="linear-ensembles" class="section level2">
<h2><span class="header-section-number">12.1</span> Linear ensembles</h2>
<div id="principles" class="section level3">
<h3><span class="header-section-number">12.1.1</span> Principles</h3>
<p>In this chapter we adopt the following notations. We work with <span class="math inline">\(M\)</span> models where <span class="math inline">\(\tilde{y}_{i,m}\)</span> is the prediction of model <span class="math inline">\(m\)</span> for instance <span class="math inline">\(i\)</span> and errors <span class="math inline">\(\epsilon_{i,m}=y_i-\tilde{y}_{i,m}\)</span> are stacked into a <span class="math inline">\((I\times M)\)</span> matrix <span class="math inline">\(\textbf{E}\)</span>. A linear combination of models has sample errors equal to <span class="math inline">\(\textbf{Ew}\)</span>, where <span class="math inline">\(\textbf{w}=w_m\)</span> are the weights assigned to each model and we assume <span class="math inline">\(\textbf{w}&#39;\textbf{1}_M=1\)</span>. Minimizing the total (squared) error is thus a simple quadratic program with unique constraint. The Lagrange function is <span class="math inline">\(L(\textbf{w})=\textbf{w}&#39;\textbf{E}&#39;\textbf{E}\textbf{w}-\lambda (\textbf{w}&#39;\textbf{1}_M-1)\)</span> and hence
<span class="math display">\[\frac{\partial}{\partial \textbf{w}}L(\textbf{w})=\textbf{E}&#39;\textbf{E}\textbf{w}-\lambda \textbf{1}_M=0 \quad \Leftrightarrow \quad \textbf{w}=\lambda(\textbf{E}&#39;\textbf{E})^{-1}\textbf{1}_M,\]</span></p>
<p>and the constraint imposes <span class="math inline">\(\textbf{w}^*=\frac{(\textbf{E}&#39;\textbf{E})^{-1}\textbf{1}_M}{(\textbf{1}_M&#39;\textbf{E}&#39;\textbf{E})^{-1}\textbf{1}_M}\)</span>. This form is similar to that of minimum variance portfolios. If errors are unbiased (<span class="math inline">\(\textbf{1}_I&#39;\textbf{E}=\textbf{0}_M&#39;\)</span>), then <span class="math inline">\(\textbf{E}&#39;\textbf{E}\)</span> is the covariance matrix of errors.</p>
<p>This expression shows an important feature of optimized linear ensembles: they can only add value if the models tell different stories. If two models are redundant, <span class="math inline">\(\textbf{E}&#39;\textbf{E}\)</span> will be close to singular and <span class="math inline">\(\textbf{w}^*\)</span> will arbitrage one against the other in a spurious fashion. This is the exact same problem as when mean-variance portfolios are constituted with highly correlated assets: in this case, diversification fails because when things go wrong, all assets go down. Another problem arises when the number of observations is too small compared to the number of assets so that the covariance matrix of returns is singular. This is not an issue for ensembles because the number of observations will usually be much larger than the number of models (<span class="math inline">\(I&gt;&gt;M\)</span>).</p>
<p>In the limit when correlations increase to one, the above formulation becomes highly unstable and ensembles cannot be trusted. One heuristic way to see this is when <span class="math inline">\(M=2\)</span> and
<span class="math display">\[\textbf{E}&#39;\textbf{E}=\left[
\begin{array}{cc} \sigma_1^2 &amp; \rho\sigma_1\sigma_2 \\
\rho\sigma_1\sigma_2 &amp; \sigma_2^2 \\
\end{array}
\right] \quad \Leftrightarrow  \quad 
(\textbf{E}&#39;\textbf{E})^{-1}=\frac{1}{1-\rho^2}\left[
\begin{array}{cc} \sigma_1^{-2} &amp; -\rho(\sigma_1\sigma_2)^{-1} \\
-\rho(\sigma_1\sigma_2)^{-1} &amp; \sigma_2^{-2} \\
\end{array}
\right]\]</span></p>
<p>so that when <span class="math inline">\(\rho \rightarrow 1\)</span>, the model with the smallest errors (minimum <span class="math inline">\(\sigma_i^2\)</span>) will see its weight increasing towards infinity while the other model will have a similarly large negative weight: the model arbitrages between two highly correlated variable. This seems like a very bad idea.</p>
<p>There is another illustration of the issues caused by correlations. Let’s assume we face <span class="math inline">\(M\)</span> correlated errors <span class="math inline">\(\epsilon_m\)</span> with pairwise correlation <span class="math inline">\(\rho\)</span>, zero mean and variance <span class="math inline">\(\sigma^2\)</span>. The variance of errors is
<span class="math display">\[\begin{align*}
\mathbb{E}\left[\frac{1}{M}\sum_{m=1}^M \epsilon_m^2 \right]&amp;=\frac{1}{M^2}\left[\sum_{m=1}^M\epsilon_m^2+\sum_{m\neq n}\epsilon_n\epsilon_m\right] \\
&amp;=\frac{\sigma^2}{M}+\frac{1}{M^2}\sum_{n\neq m} \rho \sigma^2 \\
&amp; =\rho \sigma^2 +\frac{\sigma^2(1-\rho)}{M}
\end{align*}\]</span>
where while the second term converges to zero as <span class="math inline">\(M\)</span> increases, the second term remains and is <strong>linearly increasing</strong> with <span class="math inline">\(\rho\)</span>. In passing, because variances are always positive, this result implies that the common pairwise correlation between <span class="math inline">\(M\)</span> variables is bounded by below by <span class="math inline">\(-(M-1)^{-1}\)</span>. This result is interesting but rarely found in textbooks.</p>
<p>One improvement proposed to circumvent the trouble caused by correlations, advocated in a seminal publication (<span class="citation">Breiman (<a href="#ref-breiman1996stacked">1996</a>)</span>), is to enforce positivity constraints on the weights and solve</p>
<p><span class="math display">\[\underset{\textbf{w}}{\text{argmin}} \ \textbf{w}&#39;\textbf{E}&#39;\textbf{E}\textbf{w} , \quad \text{s.t.} \quad \left\{ 
\begin{array}{l} \textbf{w}&#39;\textbf{1}_M=1 \\ w_m \ge 0 \quad \forall m \end{array}\right. .\]</span></p>
<p>Mechanically, if several models are highly correlated, the constraint will impose that only one of them will have a nonzero weight. If there are many models, then just a few of them will be selected by the minimization program. In the context of portfolio optimization, <span class="citation">Jagannathan and Ma (<a href="#ref-jagannathan2003risk">2003</a>)</span> have shown the benefits of constraint in the construction mean-variance allocations In our setting, the constraint will similarly help discriminate wisely among the ‘best’ models.</p>
<p>In the literature, forecast combination and model averaging (which are synonyms of ensembles) have been tested on stock markets as early as in <span class="citation">Von Holstein (<a href="#ref-von1972probabilistic">1972</a>)</span>. Surprisingly, the articles were not published in Finance journals but rather in fields such as Management (<span class="citation">Virtanen and Yli-Olli (<a href="#ref-virtanen1987forecasting">1987</a>)</span>, <span class="citation">Wang et al. (<a href="#ref-wang2012stock">2012</a>)</span>), Economics and Econometrics (<span class="citation">Donaldson and Kamstra (<a href="#ref-donaldson1996forecast">1996</a>)</span>, <span class="citation">Clark and McCracken (<a href="#ref-clark2009improving">2009</a>)</span>), Operations Reasearch (<span class="citation">Huang, Nakamori, and Wang (<a href="#ref-huang2005forecasting">2005</a>)</span>, <span class="citation">Leung, Daouk, and Chen (<a href="#ref-leung2001using">2001</a>)</span>, and <span class="citation">Bonaccolto and Paterlini (<a href="#ref-bonaccolto2019developing">2019</a>)</span>), and Computer Science (<span class="citation">Harrald and Kamstra (<a href="#ref-harrald1997evolving">1997</a>)</span>, <span class="citation">Hassan, Nath, and Kirley (<a href="#ref-hassan2007fusion">2007</a>)</span>).</p>
<p>In the general forecasting literature, many alternative (refined) methods for combining forecasts have been studied. Trimmed opinion pools (<span class="citation">Grushka-Cockayne, Jose, and Lichtendahl Jr (<a href="#ref-grushka2016ensembles">2016</a>)</span>) compute averages over the predictions that are not too extreme. We refer to <span class="citation">Gaba, Tsetlin, and Winkler (<a href="#ref-gaba2017combining">2017</a>)</span> for a more exhaustive list of combinations as well as for an empirical study of their respective efficiency. Overall, findings are mixed and the heuristic simple average is, as usual, hard to beat (see, e.g., <span class="citation">Genre et al. (<a href="#ref-genre2013combining">2013</a>)</span>).</p>
</div>
<div id="example" class="section level3">
<h3><span class="header-section-number">12.1.2</span> Example</h3>
<p>In order to build an ensemble, we must gather the predictions and the corresponding errors into the <span class="math inline">\(\textbf{E}\)</span> matrix. We will work with 5 models that were trained in the previous chapters: penalized regression, simple tree, random forest, xgboost and feedforward neural network. The training errors have zero means, hence <span class="math inline">\(\textbf{E}&#39;\textbf{E}\)</span> is the covariance matrix of errors between models.</p>

<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb135-1" data-line-number="1">err_pen_train &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_pen_pred, x_penalized_train) <span class="op">-</span><span class="st"> </span>training_sample<span class="op">$</span>R1M_Usd  <span class="co"># Reg.</span></a>
<a class="sourceLine" id="cb135-2" data-line-number="2">err_tree_train &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_tree, training_sample) <span class="op">-</span><span class="st"> </span>training_sample<span class="op">$</span>R1M_Usd       <span class="co"># Tree</span></a>
<a class="sourceLine" id="cb135-3" data-line-number="3">err_RF_train &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_RF, training_sample) <span class="op">-</span><span class="st"> </span>training_sample<span class="op">$</span>R1M_Usd           <span class="co"># RF</span></a>
<a class="sourceLine" id="cb135-4" data-line-number="4">err_XGB_train &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_xgb, train_matrix_xgb) <span class="op">-</span><span class="st"> </span>training_sample<span class="op">$</span>R1M_Usd        <span class="co"># XGBoost</span></a>
<a class="sourceLine" id="cb135-5" data-line-number="5">err_NN_train &lt;-<span class="st"> </span><span class="kw">predict</span>(model, NN_train_features) <span class="op">-</span><span class="st"> </span>training_sample<span class="op">$</span>R1M_Usd          <span class="co"># NN</span></a>
<a class="sourceLine" id="cb135-6" data-line-number="6">E &lt;-<span class="st"> </span><span class="kw">cbind</span>(err_pen_train, err_tree_train, err_RF_train, err_XGB_train, err_NN_train) <span class="co"># E matrix</span></a>
<a class="sourceLine" id="cb135-7" data-line-number="7"><span class="kw">colnames</span>(E) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Pen_reg&quot;</span>, <span class="st">&quot;Tree&quot;</span>, <span class="st">&quot;RF&quot;</span>, <span class="st">&quot;XGB&quot;</span>, <span class="st">&quot;NN&quot;</span>)                               <span class="co"># Col. names</span></a>
<a class="sourceLine" id="cb135-8" data-line-number="8"><span class="kw">cor</span>(E)                                                                               <span class="co"># Cor. mat.</span></a></code></pre></div>
<pre><code>##           Pen_reg      Tree        RF       XGB        NN
## Pen_reg 1.0000000 0.9984394 0.9968224 0.9475378 0.9959888
## Tree    0.9984394 1.0000000 0.9974647 0.9461082 0.9969750
## RF      0.9968224 0.9974647 1.0000000 0.9446178 0.9971208
## XGB     0.9475378 0.9461082 0.9446178 1.0000000 0.9438213
## NN      0.9959888 0.9969750 0.9971208 0.9438213 1.0000000</code></pre>
<p></p>
<p>As is shown by the correlation matrix, the models fail to generate heterogeneity in their predictions. The minimum correlation (though above 95%!) is obtained by the boosted tree models. Below, we compare the training accuracy of models by computing the average absolute value of errors.</p>

<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb137-1" data-line-number="1"><span class="kw">apply</span>(<span class="kw">abs</span>(E), <span class="dv">2</span>, mean) <span class="co"># Mean absolute error or columns of E </span></a></code></pre></div>
<pre><code>##    Pen_reg       Tree         RF        XGB         NN 
## 0.08345916 0.08362133 0.08327121 0.08916813 0.08364130</code></pre>
<p></p>
<p>The best performing ML engine is the random forest. The boosted tree model is the worst, by far. Below, we compute the optimal (non constrained) weights for the combination of models.</p>

<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb139-1" data-line-number="1">w_ensemble &lt;-<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(E) <span class="op">%*%</span><span class="st"> </span>E) <span class="op">%*%</span><span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">5</span>)                             <span class="co"># Optimal weights</span></a>
<a class="sourceLine" id="cb139-2" data-line-number="2">w_ensemble &lt;-<span class="st"> </span>w_ensemble <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(w_ensemble)</a>
<a class="sourceLine" id="cb139-3" data-line-number="3">w_ensemble</a></code></pre></div>
<pre><code>##                 [,1]
## Pen_reg -0.558958761
## Tree    -0.162143644
## RF       1.259889316
## XGB     -0.001512708
## NN       0.462725797</code></pre>
<p></p>
<p>Because of the high correlations, the optimal weights are not balanced and diversified: they load heavily on the random forest learner (best in sample model) and ‘short’ a few models in order to compensate. As one could expect, the model with the largest negative weights (Pen_reg) has a very high correlation with the random forest algorithm (0.997).</p>
<p>Note that the weights are of course computed with <strong>training errors</strong>. The optimal combination is then tested on the testing sample. Below, we compute out-of-sample (testing) errors and their average absolute value.</p>

<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb141-1" data-line-number="1">err_pen_test &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_pen_pred, x_penalized_test) <span class="op">-</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd     <span class="co"># Reg.</span></a>
<a class="sourceLine" id="cb141-2" data-line-number="2">err_tree_test &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_tree, testing_sample) <span class="op">-</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd          <span class="co"># Tree</span></a>
<a class="sourceLine" id="cb141-3" data-line-number="3">err_RF_test &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_RF, testing_sample) <span class="op">-</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd              <span class="co"># RF</span></a>
<a class="sourceLine" id="cb141-4" data-line-number="4">err_XGB_test &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_xgb, xgb_test) <span class="op">-</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd                  <span class="co"># XGBoost</span></a>
<a class="sourceLine" id="cb141-5" data-line-number="5">err_NN_test &lt;-<span class="st"> </span><span class="kw">predict</span>(model, NN_test_features) <span class="op">-</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd             <span class="co"># NN</span></a>
<a class="sourceLine" id="cb141-6" data-line-number="6">E_test &lt;-<span class="st"> </span><span class="kw">cbind</span>(err_pen_test, err_tree_test, err_RF_test, err_XGB_test, err_NN_test) <span class="co"># E matrix</span></a>
<a class="sourceLine" id="cb141-7" data-line-number="7"><span class="kw">colnames</span>(E_test) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Pen_reg&quot;</span>, <span class="st">&quot;Tree&quot;</span>, <span class="st">&quot;RF&quot;</span>, <span class="st">&quot;XGB&quot;</span>, <span class="st">&quot;NN&quot;</span>)</a>
<a class="sourceLine" id="cb141-8" data-line-number="8"><span class="kw">apply</span>(<span class="kw">abs</span>(E_test), <span class="dv">2</span>, mean)             <span class="co"># Mean absolute error or columns of E </span></a></code></pre></div>
<pre><code>##    Pen_reg       Tree         RF        XGB         NN 
## 0.06618181 0.06653527 0.06710349 0.07149006 0.06718217</code></pre>
<p></p>
<p>The boosted tree model is still the worst performing algorithm while the simple models (regression and simple tree) are the ones that fare the best. The most naive combination is the simple average of model and predictions.</p>

<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb143-1" data-line-number="1">err_EW_test &lt;-<span class="st"> </span><span class="kw">apply</span>(E_test, <span class="dv">1</span>, mean)  <span class="co"># Equally weighted combination</span></a>
<a class="sourceLine" id="cb143-2" data-line-number="2"><span class="kw">mean</span>(<span class="kw">abs</span>(err_EW_test))</a></code></pre></div>
<pre><code>## [1] 0.06691358</code></pre>
<p></p>
<p>Because the errors are very correlated, the equally-weighted combination of forecasts yields an average error which lies ‘in the middle’ of individual errors. The diversification benefits are too small. Let us now test the ‘optimal’ combination <span class="math inline">\(\textbf{w}^*=\frac{(\textbf{E}&#39;\textbf{E})^{-1}\textbf{1}_M}{(\textbf{1}_M&#39;\textbf{E}&#39;\textbf{E})^{-1}\textbf{1}_M}\)</span>.</p>

<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb145-1" data-line-number="1">err_opt_test &lt;-<span class="st"> </span>E_test <span class="op">%*%</span><span class="st"> </span>w_ensemble   <span class="co"># Optimal unconstrained combination</span></a>
<a class="sourceLine" id="cb145-2" data-line-number="2"><span class="kw">mean</span>(<span class="kw">abs</span>(err_opt_test))</a></code></pre></div>
<pre><code>## [1] 0.06838195</code></pre>
<p></p>
<p>Again, the result is disappointing because of the lack of diversification across models. The correlations between errors are high not only on the training sample, but also on the testing sample, as shown below.</p>

<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb147-1" data-line-number="1"><span class="kw">cor</span>(E_test)</a></code></pre></div>
<pre><code>##           Pen_reg      Tree        RF       XGB        NN
## Pen_reg 1.0000000 0.9987069 0.9968882 0.9706767 0.9955922
## Tree    0.9987069 1.0000000 0.9978366 0.9753921 0.9970012
## RF      0.9968882 0.9978366 1.0000000 0.9776554 0.9972918
## XGB     0.9706767 0.9753921 0.9776554 1.0000000 0.9782558
## NN      0.9955922 0.9970012 0.9972918 0.9782558 1.0000000</code></pre>
<p></p>
<p>The leverage from the optimal solution only exacerbates the problem and underperforms the heuristic uniform combination. We end this section with the constrained formulation of <span class="citation">Breiman (<a href="#ref-breiman1996stacked">1996</a>)</span> using the <em>quadprog</em> package. If we write <span class="math inline">\(\mathbf{\Sigma}\)</span> for the covariance matrix of errors, we seek
<span class="math display">\[\mathbf{w}^*=\underset{\mathbf{w}}{\text{argmin}} \ \mathbf{w}&#39;\mathbf{\Sigma}\mathbf{w}, \quad \mathbf{1}&#39;\mathbf{w}=1, \quad w_i\ge 0,\]</span>
The constraints will be handled as:</p>
<p><span class="math display">\[\mathbf{A} \mathbf{w}= \begin{bmatrix} 
1 &amp; 1 &amp; 1 \\
1 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \mathbf{w} \hspace{9mm} \text{ compared to} \hspace{9mm} \mathbf{b}=\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix},  \]</span></p>
<p>where the first line will be an equality (weights sum to one) and the last three will be inequalities (weights are all positive).</p>

<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb149-1" data-line-number="1"><span class="kw">library</span>(quadprog)                       <span class="co"># Package for quadratic programming</span></a>
<a class="sourceLine" id="cb149-2" data-line-number="2">Sigma &lt;-<span class="st"> </span><span class="kw">t</span>(E) <span class="op">%*%</span><span class="st"> </span>E                     <span class="co"># Unscaled covariance matrix</span></a>
<a class="sourceLine" id="cb149-3" data-line-number="3">nb_mods &lt;-<span class="st"> </span><span class="kw">nrow</span>(Sigma)                  <span class="co"># Number of models</span></a>
<a class="sourceLine" id="cb149-4" data-line-number="4">w_const &lt;-<span class="st"> </span><span class="kw">solve.QP</span>(<span class="dt">Dmat =</span> Sigma,       <span class="co"># D matrix =  Sigma</span></a>
<a class="sourceLine" id="cb149-5" data-line-number="5">              <span class="dt">dvec =</span> <span class="kw">rep</span>(<span class="dv">0</span>, nb_mods),   <span class="co"># Zero vector</span></a>
<a class="sourceLine" id="cb149-6" data-line-number="6">              <span class="dt">Amat =</span> <span class="kw">rbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>, nb_mods), <span class="kw">diag</span>(nb_mods)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">t</span>(), <span class="co"># A matrix for constraints</span></a>
<a class="sourceLine" id="cb149-7" data-line-number="7">              <span class="dt">bvec =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="kw">rep</span>(<span class="dv">0</span>, nb_mods)),                          <span class="co"># b vector for constraints</span></a>
<a class="sourceLine" id="cb149-8" data-line-number="8">              <span class="dt">meq =</span> <span class="dv">1</span>                   <span class="co"># 1 line of equality constraints, others = inequalities</span></a>
<a class="sourceLine" id="cb149-9" data-line-number="9">              )</a>
<a class="sourceLine" id="cb149-10" data-line-number="10">w_const<span class="op">$</span>solution <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">3</span>)           <span class="co"># Solution</span></a></code></pre></div>
<pre><code>## [1] 0.000 0.000 0.802 0.000 0.198</code></pre>
<p></p>
<p>Compared to the unconstrained solution, the weights are sparse and concentrated in one or two models, usually those with small training sample errors.</p>
</div>
</div>
<div id="stacked-ensembles" class="section level2">
<h2><span class="header-section-number">12.2</span> Stacked ensembles</h2>
<div id="two-stage-training" class="section level3">
<h3><span class="header-section-number">12.2.1</span> Two stage training</h3>
<p>Stacked ensembles are a natural generalization of linear ensembles. The idea of generalizing linear ensembles goes back at least to <span class="citation">Wolpert (<a href="#ref-wolpert1992stacked">1992</a><a href="#ref-wolpert1992stacked">b</a>)</span>. In the general case, the training is performed in two stages. The first stage is the simple one, whereby the <span class="math inline">\(M\)</span> models are trained independently, yielding the predictions <span class="math inline">\(\tilde{y}_{i,m}\)</span> for instance <span class="math inline">\(i\)</span> and model <span class="math inline">\(m\)</span>. The second step is to consider the output of the trained models as input for a new level of machine learning optimization. The second level predictions are <span class="math inline">\(\breve{y}_i=h(\tilde{y}_{i,1},\dots,\tilde{y}_{i,M})\)</span>, where <span class="math inline">\(h\)</span> is a new learner (see Figure <a href="ensemble.html#fig:stackscheme">12.1</a>). Linear ensembles are of course stacked ensembles in which the second layer is a linear regression.</p>
<p>The same techniques are then applied to minimize the error between the true values <span class="math inline">\(y_i\)</span> and the predicted ones <span class="math inline">\(\breve{y}_i\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:stackscheme"></span>
<img src="images/stack.png" alt="Scheme of Stacked Ensembles." width="350px" />
<p class="caption">
FIGURE 12.1: Scheme of Stacked Ensembles.
</p>
</div>
</div>
<div id="code-and-results-3" class="section level3">
<h3><span class="header-section-number">12.2.2</span> Code and results</h3>
<p>Below, we create a low-dimensional neural network which takes in the individual predictions of each model and compiles them into a synthetic forecast.</p>

<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb151-1" data-line-number="1">model_stack &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>()</a>
<a class="sourceLine" id="cb151-2" data-line-number="2">model_stack <span class="op">%&gt;%</span><span class="st">   </span><span class="co"># This defines the structure of the network, i.e. how layers are organized</span></a>
<a class="sourceLine" id="cb151-3" data-line-number="3"><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">8</span>, <span class="dt">activation =</span> <span class="st">&#39;relu&#39;</span>, <span class="dt">input_shape =</span> nb_mods) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb151-4" data-line-number="4"><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">4</span>, <span class="dt">activation =</span> <span class="st">&#39;tanh&#39;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb151-5" data-line-number="5"><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">1</span>) </a></code></pre></div>
<p></p>
<p>The configuration is very simple. We do not include any optional arguments and hence the model is likely to overfit. As we seek to predict returns, the loss function is the standard <span class="math inline">\(L^2\)</span> norm.</p>

<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb152-1" data-line-number="1">model_stack <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(                       <span class="co"># Model specification</span></a>
<a class="sourceLine" id="cb152-2" data-line-number="2">    <span class="dt">loss =</span> <span class="st">&#39;mean_squared_error&#39;</span>,               <span class="co"># Loss function</span></a>
<a class="sourceLine" id="cb152-3" data-line-number="3">    <span class="dt">optimizer =</span> <span class="kw">optimizer_rmsprop</span>(),           <span class="co"># Optimisation method (weight updating)</span></a>
<a class="sourceLine" id="cb152-4" data-line-number="4">    <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">&#39;mean_absolute_error&#39;</span>)         <span class="co"># Output metric</span></a>
<a class="sourceLine" id="cb152-5" data-line-number="5">)</a>
<a class="sourceLine" id="cb152-6" data-line-number="6"><span class="kw">summary</span>(model_stack)                           <span class="co"># Model architecture</span></a></code></pre></div>
<pre><code>## Model: &quot;sequential_10&quot;
## __________________________________________________________________________________________
## Layer (type)                            Output Shape                        Param #       
## ==========================================================================================
## dense_29 (Dense)                        (None, 8)                           48            
## __________________________________________________________________________________________
## dense_30 (Dense)                        (None, 4)                           36            
## __________________________________________________________________________________________
## dense_31 (Dense)                        (None, 1)                           5             
## ==========================================================================================
## Total params: 89
## Trainable params: 89
## Non-trainable params: 0
## __________________________________________________________________________________________</code></pre>
<p></p>

<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb154-1" data-line-number="1">y_tilde &lt;-<span class="st"> </span>E <span class="op">+</span><span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(training_sample<span class="op">$</span>R1M_Usd, nb_mods), <span class="dt">ncol =</span> nb_mods)    <span class="co"># Train predictions</span></a>
<a class="sourceLine" id="cb154-2" data-line-number="2">y_test &lt;-<span class="st"> </span>E_test <span class="op">+</span><span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(testing_sample<span class="op">$</span>R1M_Usd, nb_mods), <span class="dt">ncol =</span> nb_mods) <span class="co"># Testing</span></a>
<a class="sourceLine" id="cb154-3" data-line-number="3">fit_NN_stack &lt;-<span class="st"> </span>model_stack <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(y_tilde,                                  <span class="co"># Train features</span></a>
<a class="sourceLine" id="cb154-4" data-line-number="4">                     training_sample<span class="op">$</span>R1M_Usd,                                 <span class="co"># Train labels</span></a>
<a class="sourceLine" id="cb154-5" data-line-number="5">                     <span class="dt">epochs =</span> <span class="dv">12</span>, <span class="dt">batch_size =</span> <span class="dv">512</span>,                           <span class="co"># Train parameters</span></a>
<a class="sourceLine" id="cb154-6" data-line-number="6">                     <span class="dt">validation_data =</span> <span class="kw">list</span>(y_test,                           <span class="co"># Test features</span></a>
<a class="sourceLine" id="cb154-7" data-line-number="7">                                            testing_sample<span class="op">$</span>R1M_Usd)           <span class="co"># Test labels</span></a>
<a class="sourceLine" id="cb154-8" data-line-number="8">)</a>
<a class="sourceLine" id="cb154-9" data-line-number="9"><span class="kw">plot</span>(fit_NN_stack)                                                            <span class="co"># Plot, evidently!</span></a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:stackNN2"></span>
<img src="ML_factor_files/figure-html/stackNN2-1.png" alt="Training metrics for the ensemble model." width="350px" />
<p class="caption">
FIGURE 12.2: Training metrics for the ensemble model.
</p>
</div>
<p></p>
<p>The performance of the ensemble is again disappointing: the learning curve is flat, hence the rounds of backpropagation are useless. The training adds little value which means that the new overarching layer of ML does not enhance the original predictions. Again, this is because all ML engines seem to be capturing the same patterns and both their linear and non-linear combinations fail to improve their performance.</p>
</div>
</div>
<div id="extensions-1" class="section level2">
<h2><span class="header-section-number">12.3</span> Extensions</h2>
<div id="exogenous-variables" class="section level3">
<h3><span class="header-section-number">12.3.1</span> Exogenous variables</h3>
<p>In a financial context, macro-economic indicators could add value to the process. It is possible that some models perform better under certain conditions and exogenous predictors can help introduce a flavor of <strong>economic-driven conditionality</strong> in the predictions.</p>
<p>Adding macro variables to the set of predictors (here, predictions) <span class="math inline">\(\tilde{y}_{i,m}\)</span> could seem like one way to achieve this. However, this would amount to mix predicted values with (possibly scaled) economic indicators and that would not make much sense.</p>
<p>One alternative outside the perimeter of ensembles is to train simple trees on a set of macroeconomic indicators. If the labels are the (possibly absolute) errors stemming from the original predictions, then the trees will create clusters of homogeneous error values. This will hint towards which conditions lead to the best and worst forecasts.
We test this idea below, using aggregate data from the Federal Reserve of Saint Louis. A simple downloading function is available in the <em>quantmod</em> package. We download and format the data in the next chunk. CPIAUCSL is a code for consumer price index and T10Y2YM is a code for the term spread (10Y minus 2Y).</p>

<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb155-1" data-line-number="1"><span class="kw">library</span>(quantmod)                                     <span class="co"># Package that extracts the data</span></a>
<a class="sourceLine" id="cb155-2" data-line-number="2"><span class="kw">library</span>(lubridate)                                    <span class="co"># Package for date management</span></a>
<a class="sourceLine" id="cb155-3" data-line-number="3"><span class="kw">getSymbols</span>(<span class="st">&quot;CPIAUCSL&quot;</span>, <span class="dt">src =</span> <span class="st">&quot;FRED&quot;</span>)                  <span class="co"># FRED is the Fed of St Louis</span></a></code></pre></div>
<pre><code>## [1] &quot;CPIAUCSL&quot;</code></pre>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb157-1" data-line-number="1"><span class="kw">getSymbols</span>(<span class="st">&quot;T10Y2YM&quot;</span>, <span class="dt">src =</span> <span class="st">&quot;FRED&quot;</span>) </a></code></pre></div>
<pre><code>## [1] &quot;T10Y2YM&quot;</code></pre>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb159-1" data-line-number="1">cpi &lt;-<span class="st"> </span><span class="kw">fortify</span>(CPIAUCSL) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb159-2" data-line-number="2"><span class="st">    </span><span class="kw">mutate</span> (<span class="dt">inflation =</span> CPIAUCSL <span class="op">/</span><span class="st"> </span><span class="kw">lag</span>(CPIAUCSL) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="co"># Inflation via Consumer Price Index</span></a>
<a class="sourceLine" id="cb159-3" data-line-number="3">ts &lt;-<span class="st"> </span><span class="kw">fortify</span>(T10Y2YM)                                <span class="co"># Term spread (10Y minus 2Y rates)</span></a>
<a class="sourceLine" id="cb159-4" data-line-number="4"><span class="kw">colnames</span>(ts)[<span class="dv">2</span>] &lt;-<span class="st"> &quot;termspread&quot;</span>                       <span class="co"># To make things clear</span></a>
<a class="sourceLine" id="cb159-5" data-line-number="5">ens_data &lt;-<span class="st"> </span>testing_sample <span class="op">%&gt;%</span><span class="st">                        </span><span class="co"># Creating aggregate dataset</span></a>
<a class="sourceLine" id="cb159-6" data-line-number="6"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(date) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb159-7" data-line-number="7"><span class="st">    </span><span class="kw">cbind</span>(err_NN_test) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb159-8" data-line-number="8"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">Index =</span> <span class="kw">make_date</span>(<span class="dt">year =</span> lubridate<span class="op">::</span><span class="kw">year</span>(date),  <span class="co"># Change date to first day of month</span></a>
<a class="sourceLine" id="cb159-9" data-line-number="9">                             <span class="dt">month =</span> lubridate<span class="op">::</span><span class="kw">month</span>(date), </a>
<a class="sourceLine" id="cb159-10" data-line-number="10">                             <span class="dt">day =</span> <span class="dv">1</span>)) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb159-11" data-line-number="11"><span class="st">    </span><span class="kw">left_join</span>(cpi) <span class="op">%&gt;%</span><span class="st">                                </span><span class="co"># Add CPI to the dataset</span></a>
<a class="sourceLine" id="cb159-12" data-line-number="12"><span class="st">    </span><span class="kw">left_join</span>(ts)                                     <span class="co"># Add termspread</span></a>
<a class="sourceLine" id="cb159-13" data-line-number="13"><span class="kw">head</span>(ens_data)                                        <span class="co"># Show first lines</span></a></code></pre></div>
<pre><code>##         date err_NN_test      Index CPIAUCSL   inflation termspread
## 1 2014-01-31 -0.14800541 2014-01-01  235.288 0.002424175       2.47
## 2 2014-02-28  0.07650783 2014-02-01  235.547 0.001100779       2.38
## 3 2014-03-31 -0.01954964 2014-03-01  236.028 0.002042055       2.32
## 4 2014-04-30 -0.08216642 2014-04-01  236.468 0.001864186       2.29
## 5 2014-05-31 -0.09444443 2014-05-01  236.918 0.001903006       2.17
## 6 2014-06-30  0.03530064 2014-06-01  237.231 0.001321132       2.15</code></pre>
<p></p>
<p>We can now build a tree that tries to explain the accuracy of models as a function of macro variables.</p>

<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb161-1" data-line-number="1"><span class="kw">library</span>(rpart.plot)     <span class="co"># Load package for tree plotting</span></a>
<a class="sourceLine" id="cb161-2" data-line-number="2">fit_ens &lt;-<span class="st"> </span><span class="kw">rpart</span>(<span class="kw">abs</span>(err_NN_test) <span class="op">~</span><span class="st"> </span>inflation <span class="op">+</span><span class="st"> </span>termspread, <span class="co"># Tree model</span></a>
<a class="sourceLine" id="cb161-3" data-line-number="3">                 <span class="dt">data =</span> ens_data,</a>
<a class="sourceLine" id="cb161-4" data-line-number="4">                 <span class="dt">cp =</span> <span class="fl">0.001</span>)                                <span class="co"># Complexity parameter (size of tree)</span></a>
<a class="sourceLine" id="cb161-5" data-line-number="5"><span class="kw">rpart.plot</span>(fit_ens)                                         <span class="co"># Plot tree</span></a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:ensfred2"></span>
<img src="ML_factor_files/figure-html/ensfred2-1.png" alt="Conditional performance of a ML engine." width="250px" />
<p class="caption">
FIGURE 12.3: Conditional performance of a ML engine.
</p>
</div>
<p></p>
<p>The tree creates clusters which have homogeneous values of absolute errors. One big cluster gathers 92% of predictions (the left one) and is the one with the smallest average. It corresponds to the periods when the term spread is above 0.29 (in percentage points). The other two groups (when the term spread is below 0.29%) are determined according to the level of inflation. If the latter is positive, then the average absolute error is 7%, if not, it is 12%. This last number, the highest of the three clusters, indicates that when the term spread is low and the inflation negative, the model’s predictions are not trustworthy because their errors have a magnitude twice as large as in other periods. Under these circumstances (which seem to be linked to a dire economic environment), it may be wiser not to use ML-based forecasts.</p>
</div>
<div id="shrinking-inter-model-correlations" class="section level3">
<h3><span class="header-section-number">12.3.2</span> Shrinking inter-model correlations</h3>
<p>As shown earlier in this chapter, one major problem with ensembles arises when the first layer of predictions is highly correlated. In this case, ensemble are pretty much useless. Their are several tricks that can help reduce this correlation but the simplest and best is probably to alter training samples. If algorithms do not see the same data, they will probably infer different patterns.</p>
<p>There are several ways to split the training data so as to build different subsets of training samples. The first dichotomy is between random versus deterministic splits. Random splits are easy and require only the target sample size to be fixed. Note that the training samples can be overlapping as long as the overlap is not too large. Hence if the original training sample has <span class="math inline">\(I\)</span> instance and the ensemble requires <span class="math inline">\(M\)</span> models, then a subsample size of <span class="math inline">\(\lfloor I/M \rfloor\)</span> may be too conservative especially if the training sample is not very large. In this case <span class="math inline">\(\lfloor I/\sqrt{M} \rfloor\)</span> may be a better alternative. Random forests are one example of ensembles built in random training samples.</p>
<p>One advantage of deterministic splits is that they are easy to reproduce and their outcome does not depend on the random seed. By the nature of factor-based training samples, the second splitting dichotomy is between time and assets. A split within assets is straightforward: each model is trained on a different set of stocks. Note that the choices of sets can be random, or dictacted by some factor-based criterion: size, momentum, book-to-market ratio, etc.</p>
<p>A split in dates requires other decisions: is the data split in large blocks (like years) and each model gets a block, which may stand for one particular kind of market condition? Or are the training dates divided more regularly? For instance, if there are 12 models in the ensemble, each model can be trained on data from a given month (e.g., January for the first models, February for the second, etc.).</p>
<p>Below, we train four models on four different years to see if this help reduce the inter-model correlations. This process is a bit lengthy because the samples and models need to be all redefined. We start by creating the four training samples. The third model works on the small subset of features, hence the sample is smaller.</p>

<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb162-1" data-line-number="1">training_sample_<span class="dv">2007</span> &lt;-<span class="st"> </span>training_sample <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb162-2" data-line-number="2"><span class="st">    </span><span class="kw">filter</span>(date <span class="op">&gt;</span><span class="st"> &quot;2006-12-31&quot;</span>, date <span class="op">&lt;</span><span class="st"> &quot;2008-01-01&quot;</span>)</a>
<a class="sourceLine" id="cb162-3" data-line-number="3">training_sample_<span class="dv">2009</span> &lt;-<span class="st"> </span>training_sample <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb162-4" data-line-number="4"><span class="st">    </span><span class="kw">filter</span>(date <span class="op">&gt;</span><span class="st"> &quot;2008-12-31&quot;</span>, date <span class="op">&lt;</span><span class="st"> &quot;2010-01-01&quot;</span>)</a>
<a class="sourceLine" id="cb162-5" data-line-number="5">training_sample_<span class="dv">2011</span> &lt;-<span class="st"> </span>training_sample <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb162-6" data-line-number="6"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(<span class="kw">c</span>(<span class="st">&quot;date&quot;</span>,features_short, <span class="st">&quot;R1M_Usd&quot;</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb162-7" data-line-number="7"><span class="st">    </span><span class="kw">filter</span>(date <span class="op">&gt;</span><span class="st"> &quot;2010-12-31&quot;</span>, date <span class="op">&lt;</span><span class="st"> &quot;2012-01-01&quot;</span>)</a>
<a class="sourceLine" id="cb162-8" data-line-number="8">training_sample_<span class="dv">2013</span> &lt;-<span class="st"> </span>training_sample <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb162-9" data-line-number="9"><span class="st">    </span><span class="kw">filter</span>(date <span class="op">&gt;</span><span class="st"> &quot;2012-12-31&quot;</span>, date <span class="op">&lt;</span><span class="st"> &quot;2014-01-01&quot;</span>)</a></code></pre></div>
<p></p>
<p>Then, we proceed to the training of the models. The syntaxes are those used in the previous chapters, nothing new here. We start with a penalized regression. In all predictions below, the original testing sample is used <em>for all models</em>.</p>

<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb163-1" data-line-number="1">y_ens_<span class="dv">2007</span> &lt;-<span class="st"> </span>training_sample_<span class="dv">2007</span><span class="op">$</span>R1M_Usd                                       <span class="co"># Dep. variable</span></a>
<a class="sourceLine" id="cb163-2" data-line-number="2">x_ens_<span class="dv">2007</span> &lt;-<span class="st"> </span>training_sample_<span class="dv">2007</span> <span class="op">%&gt;%</span><span class="st">                                           </span><span class="co"># Predictors</span></a>
<a class="sourceLine" id="cb163-3" data-line-number="3"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(features) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.matrix</span>() </a>
<a class="sourceLine" id="cb163-4" data-line-number="4">fit_ens_<span class="dv">2007</span> &lt;-<span class="st"> </span><span class="kw">glmnet</span>(x_ens_<span class="dv">2007</span>, y_ens_<span class="dv">2007</span>, <span class="dt">alpha =</span> <span class="fl">0.1</span>, <span class="dt">lambda =</span> <span class="fl">0.1</span>)        <span class="co"># Model</span></a>
<a class="sourceLine" id="cb163-5" data-line-number="5">err_ens_<span class="dv">2007</span> &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_ens_<span class="dv">2007</span>, x_penalized_test) <span class="op">-</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd <span class="co"># Prediction errors</span></a></code></pre></div>
<p></p>
<p>We continue with a random forest.</p>

<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb164-1" data-line-number="1">fit_ens_<span class="dv">2009</span> &lt;-<span class="st"> </span><span class="kw">randomForest</span>(formula,            <span class="co"># Same formula as for simple trees!</span></a>
<a class="sourceLine" id="cb164-2" data-line-number="2">                 <span class="dt">data =</span> training_sample_<span class="dv">2009</span>,    <span class="co"># Data source: 2011 training sample</span></a>
<a class="sourceLine" id="cb164-3" data-line-number="3">                 <span class="dt">sampsize =</span> <span class="dv">4000</span>,                <span class="co"># Size of (random) sample for each tree</span></a>
<a class="sourceLine" id="cb164-4" data-line-number="4">                 <span class="dt">replace =</span> <span class="ot">FALSE</span>,                <span class="co"># Is the sampling done with replacement?</span></a>
<a class="sourceLine" id="cb164-5" data-line-number="5">                 <span class="dt">nodesize =</span> <span class="dv">100</span>,                 <span class="co"># Minimum size of terminal cluster</span></a>
<a class="sourceLine" id="cb164-6" data-line-number="6">                 <span class="dt">ntree =</span> <span class="dv">40</span>,                     <span class="co"># Nb of random trees</span></a>
<a class="sourceLine" id="cb164-7" data-line-number="7">                 <span class="dt">mtry =</span> <span class="dv">30</span>                       <span class="co"># Nb of predictive variables for each tree</span></a>
<a class="sourceLine" id="cb164-8" data-line-number="8">    )</a>
<a class="sourceLine" id="cb164-9" data-line-number="9">err_ens_<span class="dv">2009</span> &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_ens_<span class="dv">2009</span>, testing_sample) <span class="op">-</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd <span class="co"># Prediction errors</span></a></code></pre></div>
<p></p>
<p>The third model is a boosted tree.</p>

<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb165-1" data-line-number="1">train_features_<span class="dv">2011</span> &lt;-<span class="st"> </span>training_sample_<span class="dv">2011</span> <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb165-2" data-line-number="2"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(features_short) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.matrix</span>()               <span class="co"># Independent variable</span></a>
<a class="sourceLine" id="cb165-3" data-line-number="3">train_label_<span class="dv">2011</span> &lt;-<span class="st"> </span>training_sample_<span class="dv">2011</span> <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb165-4" data-line-number="4"><span class="st">    </span>dplyr<span class="op">::</span><span class="kw">select</span>(R1M_Usd) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.matrix</span>()                      <span class="co"># Dependent variable</span></a>
<a class="sourceLine" id="cb165-5" data-line-number="5">train_matrix_<span class="dv">2011</span> &lt;-<span class="st"> </span><span class="kw">xgb.DMatrix</span>(<span class="dt">data =</span> train_features_<span class="dv">2011</span>, </a>
<a class="sourceLine" id="cb165-6" data-line-number="6">                                <span class="dt">label =</span> train_label_<span class="dv">2011</span>)       <span class="co"># XGB format!</span></a>
<a class="sourceLine" id="cb165-7" data-line-number="7">fit_ens_<span class="dv">2011</span> &lt;-<span class="st"> </span><span class="kw">xgb.train</span>(<span class="dt">data =</span> train_matrix_<span class="dv">2011</span>,             <span class="co"># Data source </span></a>
<a class="sourceLine" id="cb165-8" data-line-number="8">              <span class="dt">eta =</span> <span class="fl">0.4</span>,                                        <span class="co"># Learning rate</span></a>
<a class="sourceLine" id="cb165-9" data-line-number="9">              <span class="dt">objective =</span> <span class="st">&quot;reg:linear&quot;</span>,                         <span class="co"># Objective function</span></a>
<a class="sourceLine" id="cb165-10" data-line-number="10">              <span class="dt">max_depth =</span> <span class="dv">4</span>,                                    <span class="co"># Maximum depth of trees</span></a>
<a class="sourceLine" id="cb165-11" data-line-number="11">              <span class="dt">nrounds =</span> <span class="dv">18</span>                                      <span class="co"># Number of trees used</span></a>
<a class="sourceLine" id="cb165-12" data-line-number="12">    )</a>
<a class="sourceLine" id="cb165-13" data-line-number="13">err_ens_<span class="dv">2011</span> &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_ens_<span class="dv">2011</span>, xgb_test) <span class="op">-</span><span class="st">  </span>testing_sample<span class="op">$</span>R1M_Usd <span class="co"># Prediction errors</span></a></code></pre></div>
<p></p>
<p>Finally, the last model is a simple neural network.</p>

<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb166-1" data-line-number="1">NN_features_<span class="dv">2013</span> &lt;-<span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(training_sample_<span class="dv">2013</span>, features) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb166-2" data-line-number="2"><span class="st">    </span><span class="kw">as.matrix</span>()      <span class="co"># Matrix format is important</span></a>
<a class="sourceLine" id="cb166-3" data-line-number="3">NN_labels_<span class="dv">2013</span> &lt;-<span class="st"> </span>training_sample_<span class="dv">2013</span><span class="op">$</span>R1M_Usd</a>
<a class="sourceLine" id="cb166-4" data-line-number="4">model_ens_<span class="dv">2013</span> &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>()</a>
<a class="sourceLine" id="cb166-5" data-line-number="5">model_ens_<span class="dv">2013</span> <span class="op">%&gt;%</span><span class="st">   </span><span class="co"># This defines the structure of the network, i.e. how layers are organized</span></a>
<a class="sourceLine" id="cb166-6" data-line-number="6"><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">16</span>, <span class="dt">activation =</span> <span class="st">&#39;relu&#39;</span>, <span class="dt">input_shape =</span> <span class="kw">ncol</span>(NN_features_<span class="dv">2013</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb166-7" data-line-number="7"><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">8</span>, <span class="dt">activation =</span> <span class="st">&#39;tanh&#39;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb166-8" data-line-number="8"><span class="st">    </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">1</span>) </a>
<a class="sourceLine" id="cb166-9" data-line-number="9">model_ens_<span class="dv">2013</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(                    <span class="co"># Model specification</span></a>
<a class="sourceLine" id="cb166-10" data-line-number="10">    <span class="dt">loss =</span> <span class="st">&#39;mean_squared_error&#39;</span>,               <span class="co"># Loss function</span></a>
<a class="sourceLine" id="cb166-11" data-line-number="11">    <span class="dt">optimizer =</span> <span class="kw">optimizer_rmsprop</span>(),           <span class="co"># Optimisation method (weight updating)</span></a>
<a class="sourceLine" id="cb166-12" data-line-number="12">    <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">&#39;mean_absolute_error&#39;</span>)         <span class="co"># Output metric</span></a>
<a class="sourceLine" id="cb166-13" data-line-number="13">)</a>
<a class="sourceLine" id="cb166-14" data-line-number="14">model_ens_<span class="dv">2013</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(NN_features_<span class="dv">2013</span>,                        <span class="co"># Training features</span></a>
<a class="sourceLine" id="cb166-15" data-line-number="15">                       NN_labels_<span class="dv">2013</span>,                          <span class="co"># Training labels</span></a>
<a class="sourceLine" id="cb166-16" data-line-number="16">                       <span class="dt">epochs =</span> <span class="dv">9</span>, <span class="dt">batch_size =</span> <span class="dv">128</span>             <span class="co"># Training parameters</span></a>
<a class="sourceLine" id="cb166-17" data-line-number="17">)</a>
<a class="sourceLine" id="cb166-18" data-line-number="18">err_ens_<span class="dv">2013</span> &lt;-<span class="st"> </span><span class="kw">predict</span>(model_ens_<span class="dv">2013</span>, NN_test_features) <span class="op">-</span><span class="st"> </span>testing_sample<span class="op">$</span>R1M_Usd</a></code></pre></div>
<p></p>
<p>Endowed with the errors of the four models, we can compute their correlation matrix.</p>

<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb167-1" data-line-number="1">E_subtraining &lt;-<span class="st"> </span><span class="kw">tibble</span>(err_ens_<span class="dv">2007</span>,</a>
<a class="sourceLine" id="cb167-2" data-line-number="2">                        err_ens_<span class="dv">2009</span>,</a>
<a class="sourceLine" id="cb167-3" data-line-number="3">                        err_ens_<span class="dv">2011</span>,</a>
<a class="sourceLine" id="cb167-4" data-line-number="4">                        err_ens_<span class="dv">2013</span>)</a>
<a class="sourceLine" id="cb167-5" data-line-number="5"><span class="kw">cor</span>(E_subtraining)</a></code></pre></div>
<pre><code>##              err_ens_2007 err_ens_2009 err_ens_2011 err_ens_2013
## err_ens_2007    1.0000000    0.9610165    0.6460091    0.9991898
## err_ens_2009    0.9610165    1.0000000    0.6340258    0.9628624
## err_ens_2011    0.6460091    0.6340258    1.0000000    0.6459006
## err_ens_2013    0.9991898    0.9628624    0.6459006    1.0000000</code></pre>
<p></p>
<p>The results are overall disappointing. Only one model manages to extract patterns that are somewhat different from the other ones, resulting in a 65% correlation across the board. Neural networks (on 2013 data) and penalized regressions (2007) remain highly correlated. One possible explanation could be that the models capture mainly noise and little signal. Working with long term labels like annual returns could help improve diversification across models.</p>
</div>
</div>
<div id="exercise" class="section level2">
<h2><span class="header-section-number">12.4</span> Exercise</h2>
<p>Build an integrated ensemble of top of 3 neural networks trained entirely with Keras. Each network obtains one third of predictors as input. The three networks yield a classification (yes/no or buy/sell). The overarching network aggregates the three outputs into a final decision. Evaluate its performance on the testing sample. Use the functional API.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-bates1969combination">
<p>Bates, John M, and Clive WJ Granger. 1969. “The Combination of Forecasts.” <em>Journal of the Operational Research Society</em> 20 (4): 451–68.</p>
</div>
<div id="ref-bonaccolto2019developing">
<p>Bonaccolto, Giovanni, and Sandra Paterlini. 2019. “Developing New Portfolio Strategies by Aggregation.” <em>Annals of Operations Research</em>, 1–39.</p>
</div>
<div id="ref-breiman1996stacked">
<p>Breiman, Leo. 1996. “Stacked Regressions.” <em>Machine Learning</em> 24 (1): 49–64.</p>
</div>
<div id="ref-claeskens2008model">
<p>Claeskens, Gerda, and Nils Lid Hjort. 2008. <em>Model Selection and Model Averaging</em>. Cambridge University Press.</p>
</div>
<div id="ref-clark2009improving">
<p>Clark, Todd E, and Michael W McCracken. 2009. “Improving Forecast Accuracy by Combining Recursive and Rolling Forecasts.” <em>International Economic Review</em> 50 (2): 363–95.</p>
</div>
<div id="ref-donaldson1996forecast">
<p>Donaldson, R Glen, and Mark Kamstra. 1996. “Forecast Combining with Neural Networks.” <em>Journal of Forecasting</em> 15 (1): 49–61.</p>
</div>
<div id="ref-freund1997decision">
<p>Freund, Yoav, and Robert E Schapire. 1997. “A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting.” <em>Journal of Computer and System Sciences</em> 55 (1): 119–39.</p>
</div>
<div id="ref-gaba2017combining">
<p>Gaba, Anil, Ilia Tsetlin, and Robert L Winkler. 2017. “Combining Interval Forecasts.” <em>Decision Analysis</em> 14 (1): 1–20.</p>
</div>
<div id="ref-genre2013combining">
<p>Genre, Véronique, Geoff Kenny, Aidan Meyler, and Allan Timmermann. 2013. “Combining Expert Forecasts: Can Anything Beat the Simple Average?” <em>International Journal of Forecasting</em> 29 (1): 108–21.</p>
</div>
<div id="ref-grushka2016ensembles">
<p>Grushka-Cockayne, Yael, Victor Richmond R Jose, and Kenneth C Lichtendahl Jr. 2016. “Ensembles of Overfit and Overconfident Forecasts.” <em>Management Science</em> 63 (4): 1110–30.</p>
</div>
<div id="ref-harrald1997evolving">
<p>Harrald, Paul G, and Mark Kamstra. 1997. “Evolving Artificial Neural Networks to Combine Financial Forecasts.” <em>IEEE Transactions on Evolutionary Computation</em> 1 (1): 40–52.</p>
</div>
<div id="ref-hassan2007fusion">
<p>Hassan, Md Rafiul, Baikunth Nath, and Michael Kirley. 2007. “A Fusion Model of Hmm, Ann and Ga for Stock Market Forecasting.” <em>Expert Systems with Applications</em> 33 (1): 171–80.</p>
</div>
<div id="ref-huang2005forecasting">
<p>Huang, Wei, Yoshiteru Nakamori, and Shou-Yang Wang. 2005. “Forecasting Stock Market Movement Direction with Support Vector Machine.” <em>Computers &amp; Operations Research</em> 32 (10): 2513–22.</p>
</div>
<div id="ref-jacobs1991adaptive">
<p>Jacobs, Robert A, Michael I Jordan, Steven J Nowlan, Geoffrey E Hinton, and others. 1991. “Adaptive Mixtures of Local Experts.” <em>Neural Computation</em> 3 (1): 79–87.</p>
</div>
<div id="ref-jagannathan2003risk">
<p>Jagannathan, Ravi, and Tongshu Ma. 2003. “Risk Reduction in Large Portfolios: Why Imposing the Wrong Constraints Helps.” <em>Journal of Finance</em> 58 (4): 1651–83.</p>
</div>
<div id="ref-leung2001using">
<p>Leung, Mark T, Hazem Daouk, and An-Sing Chen. 2001. “Using Investment Portfolio Return to Combine Forecasts: A Multiobjective Approach.” <em>European Journal of Operational Research</em> 134 (1): 84–102.</p>
</div>
<div id="ref-okun2011ensembles">
<p>Okun, Oleg, Giorgio Valentini, and Matteo Re. 2011. <em>Ensembles in Machine Learning Applications</em>. Vol. 373. Springer Science &amp; Business Media.</p>
</div>
<div id="ref-schapire1990strength">
<p>Schapire, Robert E. 1990. “The Strength of Weak Learnability.” <em>Machine Learning</em> 5 (2): 197–227.</p>
</div>
<div id="ref-schapire2012boosting">
<p>Schapire, Robert E, and Yoav Freund. 2012. <em>Boosting: Foundations and Algorithms</em>. MIT press.</p>
</div>
<div id="ref-seni2010ensemble">
<p>Seni, Giovanni, and John F Elder. 2010. “Ensemble Methods in Data Mining: Improving Accuracy Through Combining Predictions.” <em>Synthesis Lectures on Data Mining and Knowledge Discovery</em> 2 (1): 1–126.</p>
</div>
<div id="ref-virtanen1987forecasting">
<p>Virtanen, Ilkka, and Paavo Yli-Olli. 1987. “Forecasting Stock Market Prices in a Thin Security Market.” <em>Omega</em> 15 (2): 145–55.</p>
</div>
<div id="ref-von1972probabilistic">
<p>Von Holstein, Carl-Axel S Staël. 1972. “Probabilistic Forecasting: An Experiment Related to the Stock Market.” <em>Organizational Behavior and Human Performance</em> 8 (1): 139–58.</p>
</div>
<div id="ref-wang2012stock">
<p>Wang, Ju-Jie, Jian-Zhou Wang, Zhe-George Zhang, and Shu-Po Guo. 2012. “Stock Index Forecasting Based on a Hybrid Model.” <em>Omega</em> 40 (6): 758–66.</p>
</div>
<div id="ref-wolpert1992stacked">
<p>Wolpert, David H. 1992b. “Stacked Generalization.” <em>Neural Networks</em> 5 (2): 241–59.</p>
</div>
<div id="ref-zhang2012ensemble">
<p>Zhang, Cha, and Yunqian Ma. 2012. <em>Ensemble Machine Learning: Methods and Applications</em>. Springer.</p>
</div>
<div id="ref-zhou2012ensemble">
<p>Zhou, Zhi-Hua. 2012. <em>Ensemble Methods: Foundations and Algorithms</em>. Chapman; Hall/CRC.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="valtune.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="backtest.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ML_factor.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true,
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
