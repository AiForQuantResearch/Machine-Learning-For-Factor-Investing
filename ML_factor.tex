\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[]{krantz}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Machine Learning for Factor Investing},
            pdfauthor={Guillaume Coqueret and Tony Guida},
            colorlinks=true,
            linkcolor=Maroon,
            filecolor=Maroon,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.61,0.61,0.61}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.14,0.14,0.14}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.43,0.43,0.43}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{longtable}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother

\usepackage[bf,singlelinecheck=off]{caption}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\usepackage{float}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amsbsy}
\usepackage{color}
\usepackage{mathrsfs}
\usepackage{tabularx,tabulary}
\usepackage{verbatim}
\usepackage{dsfont}
\usepackage{booktabs}
\usepackage{longtable}

\textwidth 5.5in
\textheight 8.5in
\topmargin -2cm
\evensidemargin 1.1cm
\oddsidemargin 1.1cm

\frontmatter
% https://github.com/rstudio/rmarkdown/issues/337
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

% https://github.com/rstudio/rmarkdown/pull/252
\usepackage{titling}
\setlength{\droptitle}{-2em}

\pretitle{\vspace{\droptitle}\centering\huge}
\posttitle{\par}

\preauthor{\centering\large\emph}
\postauthor{\par}

\predate{\centering\large\emph}
\postdate{\par}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Machine Learning for Factor Investing}
\author{Guillaume Coqueret and Tony Guida}
\date{2019-12-13 PRELIMINARY \& VERY INCOMPLETE}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

% you may need to leave a few empty pages before the dedication page

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}

\begin{center}
To Leslie and Selin.
%\includegraphics{images/dedication.pdf}
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoftables
\listoffigures
\mainmatter

\hypertarget{preface}{%
\chapter{Preface}\label{preface}}

\hypertarget{foreword}{%
\section{Foreword}\label{foreword}}

By definition, many topics and references will have escaped our
scrutiny. Our intent is to progressively improve the content of the
book. We will be grateful to any comment that helps correct or update
the monograph. Thank you for sending your feedback at
guillaume.coqueret(at)gmail.com.

\url{https://github.com/josephmisiti/awesome-machine-learning/blob/master/books.md}

\hypertarget{what-this-book-is-not-about}{%
\section{What this book is not
about}\label{what-this-book-is-not-about}}

This book deals with machine learning tools and their applications in
factor investing. The topics we discussed are related to other themes
that will not be covered in the monographs. These themes include:

\begin{itemize}
\tightlist
\item
  applications of ML in \textbf{other financial fields}, such as fraud
  detection or credit scoring. We refer to \citet{ngai2011application}
  and \citet{baesens2015fraud} for general purpose fraud detection, to
  \citet{bhattacharyya2011data} for a focus on credit cards and to
  \citet{ravisankar2011detection} and \citet{abbasi2012metafraud} for
  studies on fraudulent financial reporting. On the topic of credit
  scoring, \citet{wang2011comparative} and \citet{brown2012experimental}
  provide overviews of methods and some empirical results.\\
\item
  \textbf{use cases of alternative datasets} that show how to leverage
  textual data from social media, satellite imagery, or credit card logs
  to predict sales, earning reports, and, ultimately, future returns.
  The literature on this topic is still emerging (see, e.g.,
  \citet{blank2019using}, \citet{jha2019implementing} and
  \citet{ke2019predicting}) but will likely blossom in the near
  future.\\
\item
  \textbf{technical details} of machine learning tools. While we do
  provide some insights on specificities of some approaches (those we
  believe are important), the purpose of the book is not to serve as
  reference manual on statistical learning. We refer to
  \citet{friedman2009elements} for a general treatment on the subject,
  to \citet{du2013neural} and \citet{goodfellow2016deep} for monographs
  on neural networks particularly and to \citet{sutton2018reinforcement}
  for a tour in reinforcement learning.
\end{itemize}

\hypertarget{the-targeted-audience}{%
\section{The targeted audience}\label{the-targeted-audience}}

Who should read this book? This book is intended for two audiences.
First, postgraduate students who wish to pursue their studies in
quantitative finance with a view towards investment and asset
management. The second target are professionals from the money
management industry who either seek to pivot towards allocation methods
that are based on machine learning or are simply interested in these new
tools and want to upgrade their set of competences. To a lesser extent,
the book can serve to scholars or researchers who need a manual with a
broad spectrum of references both on recent asset pricing issues and
machine learning algorithms applied to money management.

The book assumes basic knowledge in \textbf{algebra} (matrix
manipulation), \textbf{analysis} (function differentiation, gradients),
\textbf{optimization} (first and second order conditions), and
\textbf{statistics} (distributions, moments, tests). A minimal
\textbf{financial culture} is also required: simple notions like stocks,
accounting quantities (e.g., book value) will not be defined in this
book.

\hypertarget{how-this-book-is-structured}{%
\section{How this book is
structured}\label{how-this-book-is-structured}}

The book can be divided into four parts.

The first part gathers preparatory material like notations (Chapter
\ref{notdata}) and introductory remarks (Chapter \ref{intro}). The next
two chapters are very important. The first one (Chapter \ref{factor})
outlines the economic foundations (theoretical and empirical) of factor
investing and sums up recent literature. The second one (Chapter
\ref{Data}) deals with data preparation. It briefly recalls the basic
tips and warns about major issues.

The second part of the book is dedicated to predictive algorithms in
supervised learning. Those are the most common tools that are used to
forecast financial quantities (returns, volatilities, Sharpe ratios,
etc.). They range from penalized regressions (Chapter \ref{lasso}), to
tree methods (Chapter \ref{trees}), neural networks (Chapter \ref{NN}),
support vector machines (Chapter \ref{svm}) and Bayesian approaches
(Chapter \ref{bayes}).

The next portion of the book bridges the gap between the tools and their
application in finance. Chapter \ref{valtune} details how to assess and
improve the ML engines defined beforehand. Chapter \ref{ensemble}
explains how models can be combined and often why that may not be a good
idea. Finally, one of the most important chapters (number
\ref{backtest}) reviews the critical steps of portfolio backtesting and
mentions the frequent mistakes that are often encountered at this stage.

The end of the book covers a range of advanced topics connected to
machine learning more specifically. The first one is interpretability.
ML models are often considered to be black boxes and this raises trust
issues: how and why should one trust ML-based predictions? Chapter
\ref{interp} is intended to present methods that help understand what is
happening under the hood. Chapter \ref{causality} is focused on
causality, which is a much more powerful concept than correlation. Most
ML tools rely on correlation-like patterns and it is important to
underline the benefits and techniques related to causality. Chapters
\ref{unsup} and \ref{RL} are dedicated to non supervised methods. The
latter can be useful, but their financial applications should be wisely
and cautiously motivated. Laslty, the final chapter (\ref{NLP})
introduces standard approaches for the treatment of textual data.

\hypertarget{companion-website}{%
\section{Companion website}\label{companion-website}}

www.mlfactor.com copy paste code easily.

\hypertarget{why-r}{%
\section{Why R?}\label{why-r}}

The supremacy of Python as the ML programming language is a widespread
belief. This is because almost all applications of deep learning (which
is as of 2020 one of the most fashionable branches of ML) are coded in
Python via Tensorflow or Pytorch. The fact is that \textbf{R} has a
\textbf{lot} to offer as well. First of all, let us not forget that one
of the most influencial textbooks in ML (\citet{friedman2009elements})
is written by statisticians who code in R. Moreover, many
statistics-orientated algorithms (e.g.~BARTs in Section \ref{BART}) are
primarily coded in R and not Python. The R offering in Bayesian packages
in general (\url{https://cran.r-project.org/web/views/Bayesian.html})
and in Bayesian learning in particular is probably unmatched.

There are currently several ML frameworks available in R.

\begin{itemize}
\tightlist
\item
  \textbf{caret}: \url{https://topepo.github.io/caret/index.html}, a
  compilation of more than 200 ML models;\\
\item
  \textbf{tidymodels}: \url{https://github.com/tidymodels}, a collection
  of packages for ML workflow;\\
\item
  \textbf{rtemis}: \url{https://rtemis.netlify.com}, a general purpose
  package for ML and visualization;\\
\item
  \textbf{mlr3}: \url{https://mlr3.mlr-org.com/index.html}, also a
  simple framework for ML models;\\
\item
  \textbf{h2o}: \url{https://github.com/h2oai/h2o-3/tree/master/h2o-r},
  a large set of tools provided by h2o (coded in Java);\\
\item
  \textbf{Open ML}: \url{https://github.com/openml/openml-r}, the R
  version of the OpenML (www.openml.org) community.
\end{itemize}

Moreover, via the \emph{reticulate} package, it is possible (but not
always easy) to benefit from Python tools as well. The most prominent
example is the adaptation of the \emph{tensorflow} and \emph{keras}
libraries to R. Thus, some very advanced Python material is readily
available to R users. This is also true for other resources, like
Stanford's CoreNLP library (in Java) which was adapted to R in the
package \emph{coreNLP} (which we will not use in this book).

\hypertarget{coding-instructions}{%
\section{Coding instructions}\label{coding-instructions}}

One of the purposes of the book is to propose a large scale tutorial of
ML applications in financial predictions and portfolio selection. Thus,
one keyword is \textbf{REPRODUCIBILITY}!

R and RStudio. Coding requirements, tidyverse / dplyr+tidyr, filter,
select, arrange, spread, gather. R libraries Install packages
install.packages(``nameofthepackage'') Short chunks Comments \#\# output
Big tutorial so most of the chunks depend on previously defined
variables. When replicating parts of the code, make sure that the
environment includes all relevant variables.

\begin{table}
\begin{center}
\begin{tabular}{rlc}
\textit{Package} & Purpose & Chapter(s) \\ \hline
\textit{adabag} & Boosted trees & 7 \\
\textit{BART} & Bayesian additive trees &  10\\
\textit{broom} & Tidy regression output & XXX \\
\textit{CAM} & Causal Additive Models & XXX \\
\textit{caTools} & AUC curves & 11 \\
\textit{breakDown} & Breakdown interpretability & 14 \\
\textit{dummies} & One-hot encoding & 8 \\
\textit{e1071} & Support Vector Machines & 9 \\
\textit{factoextra} &  PCA visualization & 16XXX \\
\textit{FNN} & Nearest Neighbors detection & 16XXX \\
\textit{ggpubr} & Combining plots & 11  \\
\textit{glmnet} & Penalized regressions & 6 \\
\textit{iml} & Interpretability tools & 14 \\
\textit{keras} & Neural networks  & 8 \\
\textit{lime} & Interpretability & 14 \\
\textit{lmtest} & Granger causality & 15 \\
\textit{lubridate} & Handling dates & All (or many) \\
\textit{MlBayesOpt} & Bayesian hyperparameter tuning & 11 \\
\textit{naivebayes} & Naive Bayes classifier & 10 \\
\textit{quadprog} & Quadratic programming & 12 \\
\textit{quantmod} & Data extraction & 4, 12\\
\textit{randomForest} & Random forests & 7 \\
\textit{ReinforcementLearning} & Reinforcement Learning & 17 \\
\textit{rpart} and \textit{rpart.plot} & Simple decision trees &7 \\
\textit{spBayes} & Bayesian linear regression & 10 \\
\textit{tidyverse} & Environment for data science, data wrangling & All \\
\textit{xgboost} & Boosted trees  & 7 \\
\textit{xtable} & Table formatting & 4 \\ \hline
\end{tabular}
\end{center}
\caption{List of all packages used in the book. \label{tab:packages}}
\end{table}

Of all of these packages (or collections thereof), the
\textbf{tidyverse} and \textbf{lubridate} are compulsory in almost all
sections of the book.

\hypertarget{acknowledgements}{%
\section{Acknowledgements}\label{acknowledgements}}

The core of the book was prepared for a lecture given by one of the
authors to students of Masters Degrees in Finance at EMLYON Business
School and at the Imperial College Business School in the Spring of
2019. We thank Eric André, Bertand Tavin, and Aurélie Brossard for
friendly reviews; Christophe Dervieux for his help with bookdown; John
Kimmel for making this happen and Jonathan Regenstein for his
availability, no matter the topic.

\hypertarget{future-developments}{%
\section{Future developments}\label{future-developments}}

The fields of machine learning and factor modelling are developing at a
fast pace. The content of this book will always constitute a solid
background but it is naturally destined to obsolescence. As much as we
can, we will update it with the latest ongoing research.\footnote{We
  again thank any reader for sharing useful references and sending them
  to guillaume.coqueret(at)gmail.com.} Until then, all errors are ours.

\hypertarget{notdata}{%
\chapter{Notations and data}\label{notdata}}

\hypertarget{notations}{%
\section{Notations}\label{notations}}

Bold notations indicate vectors and matrices. We use capital letters for
matrices and lower case letters for vectors. \(\mathbf{v}'\) and
\(\mathbf{M}'\) denote the transposes of \(\mathbf{v}\) and
\(\mathbf{M}\). \(\mathbf{M}=[m]_{i,j}\), where \(i\) is the row index
and \(j\) the column index.

We will work with two notations in parallel. The first one is the pure
machine learning notation in which the \textbf{labels} (also called
\textbf{output} or \textbf{dependent} variables) \(\mathbf{y}=y_i\) are
approximated by functions of features
\(\mathbf{X}_i=(x_{i,1},\dots,x_{i,K})\). The dimension of the feature
matrix \(\mathbf{X}\) is \(I\times K\): there are \(I\)
\textbf{instances}, \textbf{records}, or \textbf{observations} and each
one of them has \(K\) \textbf{attributes}, \textbf{features},
\textbf{inputs}, or \textbf{predictors} which will serve as
\textbf{independent} and \textbf{explanatory} variables (all these terms
will be used interchangeably). Sometimes, to ease notations, we will
write \(\textbf{x}_i\) for one instance (one row) of \(\textbf{X}\) or
\(\textbf{x}_k\) for one (feature) vector of \(\textbf{X}\).

The second field is finance and will directly relate to the first. We
will often work with discrete returns \(r_{t,n}=p_{t,n}/p_{t-1,n}-1\)
computed from price data. Here \(t\) is the time index and \(n\) the
asset index. Unless specified otherwise, the return is always computed
over one period, though this period can sometimes be one month or one
year. Whenever confusion is possible, we will specify other notations
for returns.

In line with our previous conventions, the number of return dates will
be \(T\) and the number of assets, \(N\). The features or
characteristics of assets will be denoted with \(x_{t,n}^{(k)}\): it is
the time-\(t\) value of the \(k^{th}\) attribute of firm or asset \(n\).
Moreover, \(\mathbf{r}_t\) stands for all returns at time \(t\) while
\(\mathbf{r}_n\) stand for all returns of asset \(n\). Often, returns
will play the role of the dependent variable, or label (in ML terms).
For the riskless asset, we will use the notation \(r_{t,f}\).

The link between the two notations will most of the time be the
following. One instance \(i\) will consist one one couple (\(t,n\)) of
one particular date and one particular firm (if the data is perfectly
rectangular with no missing field, \(I=T\times N\)). The label will
usually be some performance measure of the firm computed over some
future period while the features will consist of the firm attributes at
time \(t\). Hence, the purpose of the machine learning engine in factor
investing will be to determine the model that maps the time-\(t\)
characteristics of firms into their future performance.

In terms of canonical matrices: \(\mathbf{I}_N\) will denote the
\((N\times N)\) identity matrix.

From the probabilistic literature, we employ the expectation operator
\(\mathbb{E}[\cdot]\) and the conditional expectation
\(\mathbb{E}_t[\cdot]\), where the corresponding filtration
\(\mathcal{F}_t\) corresponds to all information available at time
\(t\). More precisely,
\(\mathbb{E}_t[\cdot]=\mathbb{E}[\cdot | \mathcal{F}_t]\).
\(\mathbb{V}[\cdot]\) will denote the variance operator. Depending on
the context, probabilities will be written simply \(P\), but sometimes
we will use the heavier notation \(\mathbb{P}\). Probability
distribution functions (pdfs) will be denoted with lowercase letters
(\(f\)) and cumulative distribution functions (cdfs) with uppercase
letters (\(F\)). We will write equality in distribution as
\(X \overset{d}{=}Y\), which is equivalent to \(F_X(z)=F_Y(z)\) for all
\(z\) on the support of the variables.

Sometimes, asymptotic behaviours will be characterized with the usual
Landau notation \(o(\cdot)\) and \(O(\cdot)\). The symbol \(\propto\)
refers to proportionality: \(x\propto y\) means that \(x\) is
proportional to \(y\). With respect to derivatives, we use the standard
notation \(\frac{\partial}{\partial x}\) when differentiating with
respect to \(x\). We resort to the symbol \(\nabla\) when all
derivatives are computed (gradient vector).

Finally, we turn to functions. We list a few below:\\
- \(1_{\{x \}}\): the indicator function of the condition \(x\), which
is equal to one if \(x\) is true and to zero otherwise. -
\(\phi(\cdot)\) and \(\Phi(\cdot)\) are the standard Gaussian pdf and
cdf.\\
- card\((\cdot)\) is the cardinal function: it evaluates the number of
elements in a given set.\\
- \(\lfloor \cdot \rfloor\) is the integer part function.\\
- for a real number \(x\), \([x]^+\) is the positive part of \(x\), that
is \(\max(0,x)\).\\
- tanh\((\cdot)\) is the hyperbolic tangent:
tanh\((x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}\).\\
- ReLu\((\cdot)\) is the rectified linear unit: ReLu\((x)=\max(0,x)\).\\
- s\((\cdot)\) will be the softmax function:
\(s(\textbf{x})_i=\frac{e^{x_i}}{\sum_{j=1}^Je^{x_j}}\), where the
subscript \(i\) refers to the \(i^{th}\) element of the output.

\hypertarget{dataset}{%
\section{Dataset}\label{dataset}}

Throughout the book, we will illustrate the concepts we present with
examples of implementation based on a single financial dataset. This
dataset comprises information on 1,207 stocks listed in the US (possibly
originating from Canada or Mexico). The time range starts in November
1998 and ends in March 2019. For each point in time, 96 characteristics
describe the firms in the sample. These attributes cover a wide range of
topics:

\begin{itemize}
\tightlist
\item
  valuation (earning yields, accouting ratios)\\
\item
  profitability and quality (return on equity)\\
\item
  momentum and technical analysis (12-1 monthly returns, relative
  strength index)\\
\item
  risk (volatilities)\\
\item
  estimates (earnings-per-share)\\
\item
  volume and liquidity (share turnover)
\end{itemize}

The sample is not perfectly rectangular: there are no missing points but
the number of firms and their attributes is not constant through time.
This makes the computations in the backtest more tricky, but also more
realistic.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)        }\CommentTok{# Activate the data science package}
\KeywordTok{library}\NormalTok{(lubridate)        }\CommentTok{# Activate the date management package}
\KeywordTok{load}\NormalTok{(}\StringTok{"data_ml.RData"}\NormalTok{)                          }\CommentTok{# Load the data}
\NormalTok{data_ml <-}\StringTok{ }\NormalTok{data_ml }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{filter}\NormalTok{(date }\OperatorTok{>}\StringTok{ "1999-12-31"}\NormalTok{,                }\CommentTok{# Keep the date with sufficient data points}
\NormalTok{           date }\OperatorTok{<}\StringTok{ "2019-01-01"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{arrange}\NormalTok{(stock_id, date)                    }\CommentTok{# Order the data}
\NormalTok{data_ml[}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{]                              }\CommentTok{# Sample values}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 6
##   stock_id date       Advt_12M_Usd Advt_3M_Usd Advt_6M_Usd Asset_Turnover
##      <int> <date>            <dbl>       <dbl>       <dbl>          <dbl>
## 1        1 2000-01-31         0.41        0.39        0.42           0.19
## 2        1 2000-02-29         0.41        0.39        0.4            0.19
## 3        1 2000-03-31         0.4         0.37        0.37           0.2 
## 4        1 2000-04-30         0.39        0.36        0.37           0.2 
## 5        1 2000-05-31         0.4         0.42        0.4            0.2 
## 6        1 2000-06-30         0.41        0.47        0.42           0.21
\end{verbatim}

\normalsize

The data has 99 columns and 268336 rows. The first two columns indicate
the stock identifier and the date. The points are sampled at the monthly
frequency. There are four immediate labels in the dataset: R1M\_Usd,
R3M\_Usd, R6M\_Usd and R12M\_Usd, which correspond to the 1 month, 3
month, 6 month and 12 month future/forward returns of the stocks. These
labels are located in the last 4 columns. We provide their descriptive
statistics below.

\footnotesize

\normalsize

In anticipation for future models, we keep the name of the predictors in
memory. In addition, we also keep a much shorter list of predictors.
\footnotesize

\normalsize

The original labels are numerical and will be used for regression
exercises, that is, when the objective is to predict a scalar real
number. Sometimes, the exercises can be different and the purpose is to
forecast a category, like ``buy'', ``hold'' or ``sell''. In order to be
able to perform this type of analysis, we create additional labels that
are categorical.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data_ml <-}\StringTok{ }\NormalTok{data_ml }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{group_by}\NormalTok{(date) }\OperatorTok{%>%}\StringTok{                                   }\CommentTok{# Group by date}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{R1M_Usd_C =}\NormalTok{ R1M_Usd }\OperatorTok{>}\StringTok{ }\KeywordTok{median}\NormalTok{(R1M_Usd),        }\CommentTok{# Create the categorical labels}
           \DataTypeTok{R12M_Usd_C =}\NormalTok{ R1M_Usd }\OperatorTok{>}\StringTok{ }\KeywordTok{median}\NormalTok{(R12M_Usd)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{ungroup}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate_if}\NormalTok{(is.logical, as.factor)}
\end{Highlighting}
\end{Shaded}

\normalsize

The labels are binary: they are equal to 1 (true) if the original return
is above that of the median return over the considered period and to 0
(false) if not. Hence, at each point in time, half of the sample is
either equal to zero or one: some stocks overperforms and others
underperform.

In machine learning, models are estimated on one portion of data
(training set) and then tested on another portion of the data (testing
set) to assess their quality. We split our sample accordingly.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{separation_date <-}\StringTok{ }\KeywordTok{as.Date}\NormalTok{(}\StringTok{"2014-01-15"}\NormalTok{)}
\NormalTok{training_sample <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(data_ml, date }\OperatorTok{<}\StringTok{ }\NormalTok{separation_date)}
\NormalTok{testing_sample <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(data_ml, date }\OperatorTok{>=}\StringTok{ }\NormalTok{separation_date)}
\end{Highlighting}
\end{Shaded}

\normalsize

We also keep in memory a few key variables, like the list of asset
identifiers and a rectangular version of returns. For simplicity, in the
computation of the latter, we shrink the investment universe to keep
only the stocks for which we have the maximum number of points.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{stock_ids <-}\StringTok{ }\KeywordTok{levels}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(data_ml}\OperatorTok{$}\NormalTok{stock_id)) }\CommentTok{# A list of all stock_ids}
\NormalTok{stock_days <-}\StringTok{ }\NormalTok{data_ml }\OperatorTok{%>%}\StringTok{                        }\CommentTok{# Compute the number of data points per stock}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(stock_id) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{nb =} \KeywordTok{n}\NormalTok{()) }
\NormalTok{stock_ids_short <-}\StringTok{ }\NormalTok{stock_ids[}\KeywordTok{which}\NormalTok{(stock_days}\OperatorTok{$}\NormalTok{nb }\OperatorTok{==}\StringTok{ }\KeywordTok{max}\NormalTok{(stock_days}\OperatorTok{$}\NormalTok{nb))] }\CommentTok{# Stocks with full data}
\NormalTok{returns <-}\StringTok{ }\NormalTok{data_ml }\OperatorTok{%>%}\StringTok{                           }\CommentTok{# Compute returns, in matrix format, in 3 steps:}
\StringTok{    }\KeywordTok{filter}\NormalTok{(stock_id }\OperatorTok{%in%}\StringTok{ }\NormalTok{stock_ids_short) }\OperatorTok{%>%}\StringTok{    }\CommentTok{# 1. Filtering the data}
\StringTok{    }\KeywordTok{select}\NormalTok{(date, stock_id, R1M_Usd) }\OperatorTok{%>%}\StringTok{          }\CommentTok{# 2. Keep returns along with dates & firm names}
\StringTok{    }\KeywordTok{spread}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ stock_id, }\DataTypeTok{value =}\NormalTok{ R1M_Usd)      }\CommentTok{# 3. Put in matrix shape }
\end{Highlighting}
\end{Shaded}

\normalsize

\hypertarget{intro}{%
\chapter{Introduction}\label{intro}}

\hypertarget{context}{%
\section{Context}\label{context}}

The blossoming of machine learning in factor investing has it source at
the confluence of three favorable developments: data availability,
computational capacity, and economic groundings.

First, the data. Nowadays, classical providers, such as Bloomberg and
Reuters have seen their playing field invaded by niche players and
aggregation platforms.\footnote{We refer to
  \href{https://alternativedata.org/data-providers/}{https://alternativedata.org/data-providers/}
  for a list of alternative data providers. Moreover, we recall that
  Quandl, an alt-data hub was acquired by Nasdaq in December 2018.} In
addition, high-frequency data and derivative quotes have become
mainstream. Hence, firm-specific attributes are easy and often cheap to
compile. This means that the size of \(\mathbf{X}\) in \eqref{eq:ML} is
now sufficiently large to be plugged into ML algorithms. The order of
magnitude (in 2019) that can be reached is the following: a few hundred
monthly observations over several thousand stocks (US listed at least)
covering a few hundred attributes. This makes a dataset of dozens of
millions of points. While it is a reasonably high figure, we highlight
that the chronological depth is probably the weak point and will remain
so for decade to come because accounting figures are only released on a
quarterly basis. Needless to say that this drawback does not hold for
high-frequency strategies.

Second, computational power, both through hardware and software. Storage
and processing speed are no technical hurdles anymore and models can
even be run on the cloud thanks to services hosted by major actors
(Amazon, Microsoft, IBM and Google) and by smaller players (Rackspace,
Techila). On the software side, open source has become the norm, funded
by corporations (TensorFlow \& Keras by Google, Pytorch by Facebook,
h2o, etc.), universities (Scikit-Learn by INRIA, NLPCore by Stanford,
NLTK by UPenn) or individual or groups of researchers (caret, xgboost,
tidymodels to list but a pair of frameworks). Consequently, ML is no
longer the private turf of a handful of expert computer scientist, but
is on the contrary accessible to anyone willing to learn and code.

Finally, economic framing. Machine learning applications in finance were
initially introduced by computer scientists and information system
experts (e.g., \citet{braun1987predicting}, \citet{white1988economic})
and exploited shortly after by academics in financial economics
(\citet{bansal1993no}). Nonlinear relationships then became more
mainstream in asset pricing (\citet{freeman1992nonlinear},
\citet{bansal1993new}). These contributions started to pave the way for
the more brute-force approaches that have blossomed since the 2010
decade.

In the synthetic proposal of \citet{arnott2019backtesting}, the first
piece of advice is to rely on a model that makes sense economically.
While we agree with this stance, the only assumption that we make in
this book is that future returns depend on firm characteristics. The
relationship between these features and performance is largely unknown
and probably time-varying. This is why ML can be useful: to detect some
hidden patterns beyond the documented asset pricing anomalies.

\begin{equation}
\mathbf{y}=f(\mathbf{X})+\mathbf{\epsilon}
\label{eq:ML}
\end{equation}

\hypertarget{portfolio-construction-the-workflow}{%
\section{Portfolio construction: the
workflow}\label{portfolio-construction-the-workflow}}

\begin{figure}[H]

{\centering \includegraphics[width=16.54in]{images/scheme2} 

}

\caption{Simplified workflow in portfolio construction.}\label{fig:figscheme2}
\end{figure}

\hypertarget{machine-learning-is-no-magic-wand}{%
\section{Machine Learning is no Magic
Wand}\label{machine-learning-is-no-magic-wand}}

By definition, the curse of predictions is that they rely on past data
to infer patterns about subsequent fluctuations. The more or less
explicit hope of any forecaster is that the past will turn out to be a
good approximation of the future. Needless to say, this is a pious wish:
in general, predictions fare badly. Surprisingly, this does not depend
much on the sophistication of the econometric tool. In fact, heuristic
guesses are often hard to beat.

Hard to translate computer vision or textual ML into financial
predictions.

To illustrate this sad truth, the baseline algorithms that we detail in
chapters \ref{lasso} to \ref{NN} yield at best mediocre results.

The attentive reader will have noticed that\ldots{}

\hypertarget{factor}{%
\chapter{Factor investing and asset pricing anomalies}\label{factor}}

Asset pricing anomalies are the foundations of factor investing. In this
chapter the aim is twofold:

\begin{itemize}
\tightlist
\item
  present simple ideas and concepts: basic factor models, common
  empirical facts (time-varying nature of returns and risk premia);\\
\item
  provide the reader with articles that go much deeper.
\end{itemize}

The purpose of this chapter is not to provide a full treatment of the
many topics related to factor investing. Rather, it is intended to give
a broad overview and cover the essential themes so that the reader is
guided towards the relevant references. As such, it can serve as a
short, non-exhaustive, review of the literature. The subject of factor
modelling in finance is incredibly vast and the number of papers
dedicated to it is substantial and still rapidly increasing.

Several monographs are already dedicated to the topic of style
allocation (a synonym of factor investing) To cite but a few, we
mention:

\begin{itemize}
\tightlist
\item
  \citet{ilmanen2011expected}: an exhaustive excursion into risk premia,
  across many asset classes, with a large spectrum of descriptive
  statistics (across factors and periods),\\
\item
  \citet{ang2014asset}: covers factor investing with a strong focus on
  the money management industry,\\
\item
  \citet{bali2016empirical}, very complete book on the cross-section of
  signals with statistical analyses (univariate metrics, correlations,
  persistence, etc.),\\
\item
  \citet{jurczenko2017factor}: a tour on various topics given by field
  experts (factor purity, predictability, selection vs weighting, factor
  timing, etc.).
\end{itemize}

Finally, we mention a few wide-scope papers on this topic:
\citet{goyal2012empirical}, \citet{cazalet2014facts} and
\citet{baz2015dissecting}.

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

The topic of factor investing, though a decades-old academic theme, has
gained traction concurrently with the rise of Equity Traded Funds (ETFs)
as vectors of investment. Both have gathered momentum in the 2010
decade. Not so surprisingly, the feedback loop between practical
financial engineering and academic research has stimulated both sides in
a mutually beneficial manner. Practitioners rely on key scholarly
findings (e.g., asset pricing anomalies) while researchers dig deeper
into pragmatic topics (e.g., factor exposure or transaction costs).
Recently, researchers have also tried to quantify and qualify the impact
of factor indices on financial markets. For instance,
\citet{krkoska2019herding} analyze herding behaviors while
\citet{cong2019rise} show that the introducting of composite securities
increase volatilty and cross-asset correlations.

The core aim of factor models is to understand the \textbf{drivers of
asset prices}. Broadly speaking, the rationale behind factor investing
is that the financial performance of firms depend on factors, whether
they be latent and inobservable, or related to intrinsic characteristics
(like accounting ratios for instance). As such, factor models can be
viewed as special cases of the arbitrage pricing theory (APT) of
\citet{ross1976arbitrage}, which assumes that the return on an asset
\(n\) can be modelled as a linear combination of underlying factors
\(f_k\): \begin{equation}
\label{eq:apt}
r_{t,n}= \alpha_n+\sum_{k=1}^K\beta_{n,k}f_{t,k}+\epsilon_{t,n}, 
\end{equation}

where the usual econometric constraints on linear models hold:
\(\mathbb{E}[\epsilon_{t,n}]=0\),
\(\text{cov}(\epsilon_{t,n},\epsilon_{t,m})=0\) for \(n\neq m\) and
\(\text{cov}(f_n,\epsilon_n)=0\). If such factors do exist, then they
are in contradiction with the cornerstone model in asset pricing: the
capital asset pricing model (CAPM) of \citet{sharpe1964capital},
\citet{lintner1965valuation} and \citet{mossin1966equilibrium}. Indeed,
according to the CAPM, the only driver of returns is the market
portfolio. This explains why factors are also called `anomalies'.

Empirical evidence of asset pricing anomalies has accumulated since the
dual publication of \citet{fama1992cross} and \citet{fama1993common}.
This seminal work has paved the way for a blossoming stream of
literature that has its meta-studies (e.g., \citet{green2013supraview},
\citet{harvey2016and} and \citet{mclean2016does}). The regression
\eqref{eq:apt} can be evaluated once (unconditionally) or sequentially
over different time frames. In the latter case, the parameters
(coefficient estimates) change and the models are thus called
\emph{conditional} (we refer to \citet{ang2012testing} and to
\citet{cooper2018new} for recent results on this topic as well as for a
detailed review on the related research). Conditional models are more
flexible because they acknowledge that the drivers of asset prices may
not be constant, which seems like a reasonable postulate.

\hypertarget{detecting-anomalies}{%
\section{Detecting anomalies}\label{detecting-anomalies}}

Obviously, a crucial step is to be able to identify an anomaly and the
complexity of this task should not be underestimated. Given the
publication bias towards positive results (see, e.g.,
\citet{harvey2017presidential} in Finance), researchers are often
tempted to report partial results that are sometimes invalidated by
further studies. The need for replication is therefore high and many
findings have no tomorrow (\citet{linnainmaa2018history}). Some
researcher document fading effects because of publication: once the
anomaly becomes public, agents invest in it, which pushes prices up and
the anomaly disappears. \citet{mclean2016does} document this effect in
the US but \citet{jacobs2019anomalies} find that all other countries
experience sustained post-publication factor returns. With a different
methodology, \citet{chen2019publication} introduce a publication bias
adjustment for returns and the authors note that this (negative)
adjustment is in fact rather small. \citet{penasse2018understanding}
recommends the notion of \(alpha decay\) to study the persistence or
attenuation of anomalies.

The destruction of factor premia may due to herding
(\citet{krkoska2019herding}) and could be accelerated by the
democritization of so-called smart-beta products (Equity Traded Funds
(ETFs) notably) that allow investors to directly invest in particular
styles (value, low volatility, etc.).

This subsection was inspired from \citet{baker2017detecting} and
\citet{harvey2017lucky}.

\hypertarget{simple-portfolio-sorts}{%
\subsection{Simple portfolio sorts}\label{simple-portfolio-sorts}}

This is the most common procedure and the one used in
\citet{fama1992cross}. The idea is simple. On one date,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  rank firms according to a particular criterion (e.g., size,
  book-to-market ratio);\\
\item
  form \(J\ge 2\) portfolios (i.e.~homogeneous groups) consisting of the
  same number of stocks according to the ranking (usually, \(J=2\),
  \(J=3\), \(J=5\) or \(J=10\) portfolios are built, based on the
  median, terciles, quintiles or deciles of the criterion);\\
\item
  the weight of stocks inside the portfolio is either uniform (equal
  weights), or proportional to market capitalisation;
\item
  at a future date (usually one month), report the returns of the
  portfolios.\\
  Then, iterate the procedure until the chronological end of the sample
  is reached.
\end{enumerate}

The outcome is a time-series of portfolio returns \(r_t^j\) for each
grouping \(j\). An anomaly is identified if the \(t\)-test between the
first (\(j=1\)) and the last group (\(j=J\)) unveils a significant
difference in average returns. A strong limitation of this approach is
that the sorting criterion could have a non monotonous impact on returns
and the simple \(t\)-statistic would not detect it. Several articles
address this concern: \citet{patton2010monotonicity} and
\citet{romano2013testing} for instance.

Instead of focusing on only one criterion, it is possible to group asset
according to more characteristics. The original paper
\citet{fama1992cross} also combines market capitalization with
book-to-market ratios. Each characteristic is divided into 10 buckets,
which makes 100 portfolios in total. Beyond data availability, there is
no upper bound on the number of features that can be included the
sorting process. In fact, some authors investigate more complex sorting
algorithms that can manage a potentially large number of characteristics
(see e.g., \citet{feng2019deep} and \citet{bryzgalova2019forest}).

Finally, we refer to \citet{ledoit2018efficient} for refinements that
take into account the covariance structure of asset returns and to
\citet{cattaneo2019characteristic} for a theoretical study on the
statistical properties of the sorting procedure. Notably, thelatter
paper discusses the optimal number of portfolios.

In the code and Figure \ref{fig:factportsort} below, we compute size
portfolios (equally weighted: above versus below the median
capitalization). According to the size anomaly, the firms with below
median market cap should earn higher returns on average. This is
verified whenever the red bar in the plot is above the green one (it
happens most of the time).

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data_ml }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(date) }\OperatorTok{%>%}\StringTok{                                            }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{large =}\NormalTok{ Mkt_Cap_12M_Usd }\OperatorTok{>}\StringTok{ }\KeywordTok{median}\NormalTok{(Mkt_Cap_12M_Usd)) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# Creates the cap sort}
\StringTok{    }\KeywordTok{ungroup}\NormalTok{() }\OperatorTok{%>%}\StringTok{                                                 }\CommentTok{# Ungroup}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{year =}\NormalTok{ lubridate}\OperatorTok{::}\KeywordTok{year}\NormalTok{(date)) }\OperatorTok{%>%}\StringTok{                      }\CommentTok{# Creates a year variable}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(year, large) }\OperatorTok{%>%}\StringTok{                                     }\CommentTok{# Analyze by year & cap}
\StringTok{    }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{avg_return =} \KeywordTok{mean}\NormalTok{(R1M_Usd)) }\OperatorTok{%>%}\StringTok{                     }\CommentTok{# Compute average return}
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ year, }\DataTypeTok{y =}\NormalTok{ avg_return, }\DataTypeTok{fill =}\NormalTok{ large)) }\OperatorTok{+}\StringTok{         }\CommentTok{# Plot!}
\StringTok{    }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}\StringTok{                                }\CommentTok{# Bars side-to-side}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.8}\NormalTok{, }\FloatTok{0.2}\NormalTok{)) }\OperatorTok{+}\StringTok{                        }\CommentTok{# Legend location}
\StringTok{    }\KeywordTok{coord_fixed}\NormalTok{(}\DecValTok{124}\NormalTok{) }\OperatorTok{+}\StringTok{                                            }\CommentTok{# x/y aspect ratio}
\StringTok{    }\KeywordTok{scale_fill_manual}\NormalTok{(}\DataTypeTok{values=}\KeywordTok{c}\NormalTok{(}\StringTok{"#F87E1F"}\NormalTok{, }\StringTok{"#0570EA"}\NormalTok{))             }\CommentTok{# Colors}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{ML_factor_files/figure-latex/factportsort-1} 

}

\caption{The size factor: average returns of small (=FALSE) versus large (=TRUE) firms.}\label{fig:factportsort}
\end{figure}

\normalsize

\hypertarget{factors}{%
\subsection{Factors}\label{factors}}

The construction of so-called factors follows the same lines as above.
Portfolios are based on one characteristic and the factor is a
long-short ensemble of one extreme portfolio minus the opposite extreme
(small minus large for the size factor or high book-to-market ratio
minus low book-to-market ratio for the value factor). Sometimes,
subtleties include forming bivariate sorts and aggregating several
portfolios together, as in the original contribution of
\citet{fama1993common}. The most common factors are listed below, along
with a few references. We refer to the books listed at the beginning of
the chapter for a more exhaustive treatment of factor idiosyncrasies.
For most anomalies, theoretical justifications have been brought
forward, whether risk-based or behavioural. We list the most frequently
cited factors below:

\begin{itemize}
\tightlist
\item
  size (\textbf{SMB} = small firms minus large firms):
  \citet{banz1981relationship}, \citet{fama1992cross},
  \citet{fama1993common}, \citet{van2011size}, \citet{asness2018size}
  and \citet{astakhov2019firm}.\\
\item
  value (\textbf{HM} = high minus low: undervalued minus `growth'
  firms): \citet{fama1992cross}, \citet{fama1993common},
  \citet{asness2013value}.\\
\item
  momentum (\textbf{WML} = winners minus looser):
  \citet{jegadeesh1993returns}, \citet{asness2013value}. The winners are
  the assets that have experienced the highest returns over the last
  year (sometimes the computation of the return is truncated to omit the
  last month). Cross-sectional momentum is linked, but not equivalent,
  to time-series momentum (trend following), see e.g.,
  \citet{moskowitz2012time} and \citet{lemperiere2014two}.\\
\item
  profitability (\textbf{RMW} = robust minus weak profits):
  \citet{fama2015five}, \citet{bouchaud2019sticky}. In the former
  reference, profitability is measured as (revenues - (cost and
  expenses))/equity.\\
\item
  investment (\textbf{CMA} = conservative minus aggressive):
  \citet{fama2015five}, \citet{hou2015digesting}. Investment is measures
  via the growth of total assets (divided by total assets). Aggressive
  firms are those that experience the largest growth in assets.\\
\item
  low `risk' (sometimes: \textbf{BAB} = betting against beta):
  \citet{ang2006cross}, \citet{baker2011benchmarks},
  \citet{frazzini2014betting}, \citet{boloorforoosh2019beta}. In this
  case, the computation of risk changes from one article to the other
  (simple volatility, market beta, idiosyncratic volatility, etc.)
\end{itemize}

With the notable exception of the low risk premium, the most mainstream
anomalies are kept and updated in the data library of Kenneth French
(\url{https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html}).
Of course, the computation of the factors follows a particular set of
rules, but they are generally accepted in the academic sphere. Another
source of data is the AQR repository:
\url{https://www.aqr.com/Insights/Datasets}.

Below, we import data from Ken French's data library. We will use it
later on in the chapter.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(quantmod)                         }\CommentTok{# Package for data extraction}
\KeywordTok{library}\NormalTok{(xtable)                           }\CommentTok{# Package for LaTeX exports }
\NormalTok{min_date <-}\StringTok{ "1963-07-31"}                  \CommentTok{# Start date}
\NormalTok{max_date <-}\StringTok{ "2019-11-28"}                  \CommentTok{# Stop date}
\NormalTok{temp <-}\StringTok{ }\KeywordTok{tempfile}\NormalTok{()}
\NormalTok{KF_website <-}\StringTok{ "http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/"}
\NormalTok{KF_file <-}\StringTok{ "ftp/F-F_Research_Data_5_Factors_2x3_CSV.zip"}
\NormalTok{link <-}\StringTok{ }\KeywordTok{paste0}\NormalTok{(KF_website,KF_file)        }\CommentTok{# Link of the file}
\KeywordTok{download.file}\NormalTok{(link, temp, }\DataTypeTok{quiet =} \OtherTok{TRUE}\NormalTok{)   }\CommentTok{# Download!}
\NormalTok{FF_factors <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\KeywordTok{unz}\NormalTok{(temp, }\StringTok{"F-F_Research_Data_5_Factors_2x3.CSV"}\NormalTok{), }
                       \DataTypeTok{skip =} \DecValTok{3}\NormalTok{) }\OperatorTok{%>%}\StringTok{          }\CommentTok{# Check the number of lines to skip!}
\StringTok{    }\KeywordTok{rename}\NormalTok{(}\DataTypeTok{date =}\NormalTok{ X1, }\DataTypeTok{MKT_RF =} \StringTok{`}\DataTypeTok{Mkt-RF}\StringTok{`}\NormalTok{) }\OperatorTok{%>%}\StringTok{  }\CommentTok{# Change the name of the first column}
\StringTok{    }\KeywordTok{mutate_at}\NormalTok{(}\KeywordTok{vars}\NormalTok{(}\OperatorTok{-}\NormalTok{date), as.numeric) }\OperatorTok{%>%}\StringTok{                 }\CommentTok{# Convert values to number}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{date =} \KeywordTok{ymd}\NormalTok{(}\KeywordTok{parse_date_time}\NormalTok{(date, }\StringTok{"%Y%m"}\NormalTok{))) }\OperatorTok{%>%}\StringTok{  }\CommentTok{# Date in right format}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{date =} \KeywordTok{rollback}\NormalTok{(date }\OperatorTok{+}\StringTok{ }\KeywordTok{months}\NormalTok{(}\DecValTok{1}\NormalTok{)))              }\CommentTok{# End of month date}
\NormalTok{FF_factors <-}\StringTok{ }\NormalTok{FF_factors }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{MKT_RF =}\NormalTok{ MKT_RF }\OperatorTok{/}\StringTok{ }\DecValTok{100}\NormalTok{, }\CommentTok{# Scale returns}
                                    \DataTypeTok{SMB =}\NormalTok{ SMB }\OperatorTok{/}\StringTok{ }\DecValTok{100}\NormalTok{,}
                                    \DataTypeTok{HML =}\NormalTok{ HML }\OperatorTok{/}\StringTok{ }\DecValTok{100}\NormalTok{,}
                                    \DataTypeTok{RMW =}\NormalTok{ RMW }\OperatorTok{/}\StringTok{ }\DecValTok{100}\NormalTok{,}
                                    \DataTypeTok{CMA =}\NormalTok{ CMA }\OperatorTok{/}\StringTok{ }\DecValTok{100}\NormalTok{,}
                                    \DataTypeTok{RF =}\NormalTok{ RF}\OperatorTok{/}\DecValTok{100}\NormalTok{) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{filter}\NormalTok{(date }\OperatorTok{>=}\StringTok{ }\NormalTok{min_date, date }\OperatorTok{<=}\StringTok{ }\NormalTok{max_date)             }\CommentTok{# Finally, keep only recent points}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\KeywordTok{head}\NormalTok{(FF_factors),  }\DataTypeTok{booktabs =} \OtherTok{TRUE}\NormalTok{,}
             \DataTypeTok{caption =} \StringTok{"Sample of monthly factor values."}\NormalTok{) }\CommentTok{# A look at the data (see table)                   }
\end{Highlighting}
\end{Shaded}

\begin{table}[t]

\caption{\label{tab:factorImport}Sample of monthly factor values.}
\centering
\begin{tabular}{lrrrrrr}
\toprule
date & MKT\_RF & SMB & HML & RMW & CMA & RF\\
\midrule
1963-07-31 & -0.0039 & -0.0047 & -0.0083 & 0.0066 & -0.0115 & 0.0027\\
1963-08-31 & 0.0507 & -0.0079 & 0.0167 & 0.0039 & -0.0040 & 0.0025\\
1963-09-30 & -0.0157 & -0.0048 & 0.0018 & -0.0076 & 0.0024 & 0.0027\\
1963-10-31 & 0.0253 & -0.0129 & -0.0010 & 0.0275 & -0.0224 & 0.0029\\
1963-11-30 & -0.0085 & -0.0084 & 0.0171 & -0.0045 & 0.0222 & 0.0027\\
\addlinespace
1963-12-31 & 0.0183 & -0.0189 & -0.0012 & 0.0008 & -0.0030 & 0.0029\\
\bottomrule
\end{tabular}
\end{table}

\normalsize

Posterior to the discovery of these stylised facts, some contributions
have aimed at building theoretical models that capture these properties.
We cite a handful below:

\begin{itemize}
\tightlist
\item
  size and value: \citet{berk1999optimal},
  \citet{daniel2001overconfidence}, \citet{barberis2003style},
  \citet{gomes2003equilibrium}, \citet{carlson2004corporate},
  arnott2014can;
\item
  momentum: \citet{johnson2002rational}, \citet{grinblatt2005prospect},
  \citet{vayanos2013institutional}, \citet{choi2014momentum}.
\end{itemize}

In addition, recent bridges have been built between risk-based factor
representations and behavioural theories. We refer essentially to
\citet{barberis2016prospect} and \citet{daniel2019short} and the
references therein.

While these factors (i.e., long/short portfolios) exhibit time-varying
risk-premia, it is well-documented (and accepted) that they deliver
positive returns over long horizons. We refer to
\citet{gagliardini2016time} and to the survey
\citet{gagliardini2019estimation}, as well as to the related
bibliography for technical details on estimation procedures of risk
premia and the corresponding empirical results. A large sample study
that documents regime changes in factor premia was also carried out by
\citet{ilmanen2019factor}.

In Figure \ref{fig:riskpremiaFF}, we plot the average monthly return
aggregated over each calendar year for five common factors. The risk
free rate (which is not a factor per se) is the most stable while the
market factor (aggregate market returns minus the risk-free rate) is the
most volatile. This makes sense because it is the only long equity
factor among the five series.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{FF_factors }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{date =} \KeywordTok{year}\NormalTok{(date)) }\OperatorTok{%>%}\StringTok{                       }\CommentTok{# Turn date into year}
\StringTok{    }\KeywordTok{gather}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ factor, }\DataTypeTok{value =}\NormalTok{ value, }\OperatorTok{-}\StringTok{ }\NormalTok{date) }\OperatorTok{%>%}\StringTok{     }\CommentTok{# Put in tidy shape}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(date, factor) }\OperatorTok{%>%}\StringTok{                          }\CommentTok{# Group by year and factor}
\StringTok{    }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{value =} \KeywordTok{mean}\NormalTok{(value)) }\OperatorTok{%>%}\StringTok{                  }\CommentTok{# Compute average return}
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ date, }\DataTypeTok{y =}\NormalTok{ value, }\DataTypeTok{color =}\NormalTok{ factor)) }\OperatorTok{+}\StringTok{  }\CommentTok{# Plot}
\StringTok{    }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{coord_fixed}\NormalTok{(}\DecValTok{500}\NormalTok{)                      }\CommentTok{# Fix x/y ratio}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{ML_factor_files/figure-latex/riskpremiaFF-1.pdf}
\caption{\label{fig:riskpremiaFF}Average returns of common anomalies
(1963-2020). Source: Ken French library.}
\end{figure}

\normalsize

The individual attributes of investor who allocate towards particular
factors is a blossoming topic. We list a few references below, even
though, they somewhat lie out of the scope of this book.
\citet{betermier2017value} show that value investors are older,
wealthier and face lower income risk compared to growth investors: they
are those in the best position to take financial risks. The study
\citet{cronqvist2015value} leads to different conclusions: it finds that
the propensity to invest in value versus growth assets has roots in
genetics and in life events (the latter effect being confirmed in
\citet{cocco2019evidence} and the former being further detailed in a
more general context in \citet{cronqvist2015fetal}). Psychological
traits can also explain some factors: when agents extrapolates, they are
likely to fuel momentum (this topic is thoroughly reviewed in
\citet{barberis2018psychology}). Micro- and macro-economic consequences
of these preferences are detailed in \citet{bhamra2019does}. To conclude
this paragraph, we mention that theoretical models have also been
proposed that link agents' preferences and beliefs to market anomalies
(see for instance \citet{barberis2019prospect}).

Finally, we highlight the need of replicability of factor premia. As is
shown by \citet{linnainmaa2018history} and \citet{hou2019replicating},
many proclaimed factors are in fact very much data-dependent and often
fail to deliver sustained profitability when the investment universe is
altered.

Campbell Harvey, in a series of papers, tried to synthesize the research
on factors: \citet{harvey2016and}, \citet{harvey2017lucky},
\citet{harvey2019census}. His work underlines the need to set high bars
for an anomaly to be called a `true' factor. Increasing thresholds for
\(p\)-values is only a partial answer as it is always possible to resort
to data snooping in order to find an optimized strategy that will fail
out-of-sample but that will deliver a \(t\)-statistic larger than three
(or even four). \citet{harvey2017presidential} recommends to resort to a
Bayesian approach which blends data-based significance with a prior into
a so-called Bayesanised \emph{p}-value (see subsection below).

Following this work, researchers have continued to explore the richness
of this zoo. \citet{bryzgalova2019bayesian} propose a tractable Bayesian
estimation of large-dimensional factor models and evaluate all possible
combinations of more than 50 factors, yielding an incredibly large
number of coefficients. This combined with a Bayesianized
\citet{fama1973risk} procedure allows to distinguish between pervasive
and superfluous factors.

\hypertarget{predictive-regressions-sorts-and-p-value-issues}{%
\subsection{Predictive regressions, sorts, and p-value
issues}\label{predictive-regressions-sorts-and-p-value-issues}}

For simplicity, assume a simple form: \begin{equation}
\label{eq:factsimple}
\textbf{r} = a+b\textbf{x}+\textbf{e},
\end{equation} where the vector \(\textbf{r}\) stacks all returns of all
stocks and \(\textbf{x}\) is a lagged variable so that the regression is
indeed predictive. If the estimate \(\hat{b}\) is significant given a
specified threshold, then it can be tempting to conclude that
\(\textbf{x}\) does a good job at predicting returns. Hence, long-short
portfolios related to extreme values of \(\textbf{x}\) (mind the sign of
\(\hat{b}\)) are expected to generate profits. This is unfortunately
often false because \(\hat{b}\) gives information on the \emph{past}
ability of \(\textbf{x}\) to forecast returns. What happens in the
future may be another story.

Statistical tests are also used for portfolio sorts. Assume two extreme
portfolios are expected to yield very different average returns (like
very small cap versus very large cap, or strong winners versus bad
losers). The portfolio returns are written \(r_t^+\) and \(r_t^-\). The
simplest test for the mean is
\(t=\sqrt{T}\frac{m_{r_+}-m_{r_-}}{\sigma_{r_+-r_-}}\), where \(T\) is
the number of points and \(m_{r_\pm}\) denote the means of returns and
\(\sigma_{r_+-r_-}\) is the standard deviation of the difference between
the two series, i.e., the volatility of the long/short portfolio. In
short, the statistic can be viewed as a scaled Sharpe ratio (though
usually these ratios are computed for long-only portfolios) and can in
turn be used to compute \(p\)-values to assess the robustness of an
anomaly. As is shown in \citet{linnainmaa2018history} and
\citet{hou2019replicating}, many factors discovered by reasearchers fail
to survive in out-of-sample tests.

One reason why people are overly optimistic about anomalies they detect
is the widespread reverse interpretation of the \emph{p}-value. Often,
it is thought of as the probability of one hypothesis (e.g., my anomaly
exists) given the data. In fact, it's the opposite: it's the likelihood
of your data sample, knowing that the anomaly holds. \begin{align*}
p-\text{value} &= P[D|H] \\
\text{target prob.}& = P[H|D]=\frac{P[D|H]}{P[D]}\times P[H],
\end{align*} where \(H\) stands for hypothesis and \(D\) for data. The
equality in the second row is a plain application of Bayes' identity:
the interesting probability is in fact a transform of the \(p\)-value.

Two articles (at least) discuss this idea.
\citet{harvey2017presidential} introduces \textbf{Bayesianized}
\(p\)-\textbf{values}: \begin{equation}
\label{eq:Bpv}
\text{Bayesianized } p-\text{value}=\text{Bpv}= e^{-t^2/2}\times\frac{\text{prior}}{1+e^{-t^2/2}\times \text{prior}} ,
\end{equation} where \(t\) is the \(t\)-statistic obtained from the
regression (i.e., the one that defines the \emph{p}-value) and prior is
the analyst's estimation of the odds that the hypothesis (anomaly) is
true. The prior is coded as follows. Suppose there is a p\% chance that
the null holds (i.e (1-p)\% for the anomaly). The odds are coded as
\(p/(1-p)\). Thus, if the \emph{t}-statistic is equal to 2
(corresponding to a \emph{p}-value of 5\% roughly) and the prior odds
are equal to 6, then the Bpv is equal to
\(e^{-2}\times 6 \times(1+e^{-2}\times 6)^{-1}\approx 0.448\) and there
is a 44.8\% chance that the null is true. This interpretation stands in
sharp contrast with the original \(p\)-value which cannot be viewed as a
probability that the null holds. Of course, one drawback is that the
level of the prior is crucial and solely user-specified.

The work of \citet{chinco2019estimating} is very different but shares
some key concepts, like the introduction of Bayesian priors in
regression outputs. They show that introducing an \(L^2\) constraint on
the predictive regression (see the ridge regression in Chapter
\ref{lasso}) amounts to introduce views on what the true distribution of
\(b\) is. The stronger the constraint, the more the estimate \(\hat{b}\)
will the shrunk towards zero. One key idea in their work is the
introduction of a distribution of the true \(b\) across many anomalies.
It is assumed to be Gaussian and centered. The interesting parameter is
the standard deviation: the larger it is, the more significant anomalies
are discovered. Notably, the authors show that this parameter changes
through time and we refer to the original paper for more details on this
subject.

\hypertarget{fama-macbeth-regressions}{%
\subsection{Fama-Macbeth regressions}\label{fama-macbeth-regressions}}

Another detection method was proposed by \citet{fama1973risk} through a
two-stage regression analysis of risk premia. The first stage is a
simple estimation of the relationship (\ref{eq:apt}): the regressions
are run on a stock-by-stock basis over the corresponding time-series.
The resulting estimates \(\hat{\beta}_{i,k}\) are then plugged into a
second series of regressions: \begin{equation}
r_{t,n}= \gamma_{t,0} + \sum_{k=1}^K\gamma_{t,k}\hat{\beta}_{n,k} + \varepsilon_{t,n},
\end{equation} which are ran date-by-date on the cross-section of
assets.\footnote{Originally, \citet{fama1973risk} work with the market
  beta only: \(r_{t,n}=\alpha_n+\beta_nr_{t,M}+\epsilon_{t,n}\) and the
  second pass included nonlinear terms:
  \(r_{t,n}=\gamma_{n,0}+\gamma_{t,1}\hat{\beta}_{n}+\gamma_{t,2}\hat{\beta}^2_n+\gamma_{t,3}\hat{s}_n+\eta_{t,n}\),
  where the \(\hat{s}_n\) are risk estimates for the assets that are not
  related to the betas. It is then possible to perform asset pricing
  tests to infer some properties. For instance, test whether betas have
  a linear influence on returns or not (\(\mathbb{E}[\gamma_{t,2}]=0\)),
  or test the validity of the CAPM (which implies
  \(\mathbb{E}[\gamma_{t,0}]=0\)).} Theoretically, the betas would be
known and the regression would be run on the \(\beta_{n,k}\) instead of
their estimated values. The \(\hat{\gamma}_{t,k}\) estimate the premia
of factor \(k\) at time \(t\). Under suitable distributional assumptions
over the \(\varepsilon_{t,n}\), statistical tests can be perform to
determine whether these premia are significant or not. Typically, the
statistic on the time-aggregated (average) premia
\(\hat{\gamma}_k=\frac{1}{T}\sum_{t=1}^T\hat{\gamma}_{t,k}\):
\[t_k=\frac{\hat{\gamma}_k}{\hat{\sigma_k}/\sqrt{T}}\] is often used in
pure Gaussian contexts to assess whether or not the factor is
significant (\(\hat{\sigma}_k\) is the standard deviation of the
\(\hat{\gamma}_{t,k}\)).

We refer to \citet{jagannathan1998asymptotic} and
\citet{petersen2009estimating} for technical discussions on the biases
and losses in accuracy that can be induced by standard OLS estimations.
Moreover, as the \(\hat{\beta}_{i,k}\) in the second-pass regression are
\emph{estimates}, a second level of errors can arise (the so-called
errors in variables). The interested reader will find some extensions
and solutions in \citet{shanken1992estimation}, \citet{ang2018using} and
\citet{jegadeesh2019empirical}.

Below, we perform \citet{fama1973risk} regressions on our sample. We
start by thr first pass: individual estimation of betas. We build a
dedicated function below and use some functional programming to automate
the process.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nb_factors <-}\StringTok{ }\DecValTok{5}                                                   \CommentTok{# Number of factors}
\NormalTok{data_FM <-}\StringTok{ }\KeywordTok{left_join}\NormalTok{(data_ml }\OperatorTok{%>%}\StringTok{                                  }\CommentTok{# Join the 2 datasets}
\StringTok{                         }\KeywordTok{select}\NormalTok{(date, stock_id, R1M_Usd) }\OperatorTok{%>%}\StringTok{      }\CommentTok{# (with returns...}
\StringTok{                         }\KeywordTok{filter}\NormalTok{(stock_id }\OperatorTok{%in%}\StringTok{ }\NormalTok{stock_ids_short),   }\CommentTok{# ... over some stocks)}
\NormalTok{                     FF_factors, }
                     \DataTypeTok{by =} \StringTok{"date"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{R1M_Usd =} \KeywordTok{lag}\NormalTok{(R1M_Usd)) }\OperatorTok{%>%}\StringTok{                            }\CommentTok{# Lag returns}
\StringTok{    }\KeywordTok{na.omit}\NormalTok{() }\OperatorTok{%>%}\StringTok{                                                 }\CommentTok{# Remove missing points}
\StringTok{    }\KeywordTok{spread}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ stock_id, }\DataTypeTok{value =}\NormalTok{ R1M_Usd)}
\NormalTok{models <-}\StringTok{ }\KeywordTok{lapply}\NormalTok{(}\KeywordTok{paste0}\NormalTok{(}\StringTok{"`"}\NormalTok{, stock_ids_short, }
                        \StringTok{'` ~  MKT_RF + SMB + HML + RMW + CMA'}\NormalTok{),           }\CommentTok{# Model spec}
                 \ControlFlowTok{function}\NormalTok{(f)\{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{as.formula}\NormalTok{(f), }\DataTypeTok{data =}\NormalTok{ data_FM,           }\CommentTok{# Call lm(.)}
                                 \DataTypeTok{na.action=}\StringTok{"na.exclude"}\NormalTok{) }\OperatorTok{%>%}\StringTok{       }
\StringTok{                         }\KeywordTok{summary}\NormalTok{() }\OperatorTok{%>%}\StringTok{                                    }\CommentTok{# Gather the output}
\StringTok{                         "$"}\NormalTok{(coef) }\OperatorTok{%>%}\StringTok{                                    }\CommentTok{# Keep only coefs}
\StringTok{                         }\KeywordTok{data.frame}\NormalTok{() }\OperatorTok{%>%}\StringTok{                                 }\CommentTok{# Convert to dataframe}
\StringTok{                         }\KeywordTok{select}\NormalTok{(Estimate)\}                                }\CommentTok{# Keep the estimates}
\NormalTok{                 )}
\NormalTok{betas <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{unlist}\NormalTok{(models), }\DataTypeTok{ncol =}\NormalTok{ nb_factors }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{byrow =}\NormalTok{ T) }\OperatorTok{%>%}\StringTok{     }\CommentTok{# Extract the betas}
\StringTok{    }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{row.names =}\NormalTok{ stock_ids_short)                               }\CommentTok{# Format: row names}
\KeywordTok{colnames}\NormalTok{(betas) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Constant"}\NormalTok{, }\StringTok{"MKT_RF"}\NormalTok{, }\StringTok{"SMB"}\NormalTok{, }\StringTok{"HML"}\NormalTok{, }\StringTok{"RMW"}\NormalTok{, }\StringTok{"CMA"}\NormalTok{)    }\CommentTok{# Format: col names}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\KeywordTok{head}\NormalTok{(betas }\OperatorTok{%>%}\StringTok{ }\KeywordTok{round}\NormalTok{(}\DecValTok{3}\NormalTok{)),  }\DataTypeTok{booktabs =} \OtherTok{TRUE}\NormalTok{,}
             \DataTypeTok{caption =} \StringTok{"Sample of beta values (row numbers are stock IDs)."}\NormalTok{) }\CommentTok{# Betas (table) }
\end{Highlighting}
\end{Shaded}

\begin{table}[t]

\caption{\label{tab:FMreg}Sample of beta values (row numbers are stock IDs).}
\centering
\begin{tabular}{lrrrrrr}
\toprule
  & Constant & MKT\_RF & SMB & HML & RMW & CMA\\
\midrule
1 & 0.008 & 1.438 & 0.525 & 0.567 & 1.006 & -0.301\\
3 & -0.002 & 0.791 & 1.085 & 0.854 & 0.220 & -0.423\\
4 & 0.005 & 0.364 & 0.305 & -0.072 & 0.589 & 0.267\\
7 & 0.006 & 0.414 & 0.682 & 0.284 & 0.294 & 0.119\\
9 & 0.005 & 0.811 & 0.683 & 1.046 & 0.009 & 0.148\\
\addlinespace
11 & 0.000 & 0.937 & 0.120 & 0.494 & -0.246 & 0.086\\
\bottomrule
\end{tabular}
\end{table}

\normalsize

We then reformat these betas to prepare the second pass. Each line
corresponds to one asset: the first 5 columns are the estimated factor
loadings and the remaining ones are the asset returns (date by date).

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{loadings <-}\StringTok{ }\NormalTok{betas }\OperatorTok{%>%}\StringTok{                            }\CommentTok{# Start from loadings (betas)}
\StringTok{    }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{Constant) }\OperatorTok{%>%}\StringTok{                        }\CommentTok{# Remove constant}
\StringTok{    }\KeywordTok{data.frame}\NormalTok{()                                 }\CommentTok{# Convert to dataframe             }
\NormalTok{ret <-}\StringTok{ }\NormalTok{returns }\OperatorTok{%>%}\StringTok{                               }\CommentTok{# Start from returns}
\StringTok{    }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{date) }\OperatorTok{%>%}\StringTok{                            }\CommentTok{# Keep the returns only}
\StringTok{    }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{row.names =}\NormalTok{ returns}\OperatorTok{$}\NormalTok{date) }\OperatorTok{%>%}\StringTok{     }\CommentTok{# Set row names}
\StringTok{    }\KeywordTok{t}\NormalTok{()                                          }\CommentTok{# Transpose}
\NormalTok{FM_data <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(loadings, ret)                  }\CommentTok{# Aggregate both}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\KeywordTok{head}\NormalTok{(FM_data[,}\DecValTok{1}\OperatorTok{:}\DecValTok{8}\NormalTok{] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{round}\NormalTok{(}\DecValTok{3}\NormalTok{)),  }\DataTypeTok{booktabs =} \OtherTok{TRUE}\NormalTok{,    }\CommentTok{# The betas (see table)}
             \DataTypeTok{caption =} \StringTok{"Sample of reformatted beta values (ready for regression)."}\NormalTok{)  }
\end{Highlighting}
\end{Shaded}

\begin{table}[t]

\caption{\label{tab:betaformat}Sample of reformatted beta values (ready for regression).}
\centering
\begin{tabular}{lrrrrrrrr}
\toprule
  & MKT\_RF & SMB & HML & RMW & CMA & 2000-01-31 & 2000-02-29 & 2000-03-31\\
\midrule
1 & 1.438 & 0.525 & 0.567 & 1.006 & -0.301 & -0.036 & 0.263 & 0.031\\
3 & 0.791 & 1.085 & 0.854 & 0.220 & -0.423 & 0.077 & -0.024 & 0.018\\
4 & 0.364 & 0.305 & -0.072 & 0.589 & 0.267 & -0.016 & 0.000 & 0.153\\
7 & 0.414 & 0.682 & 0.284 & 0.294 & 0.119 & -0.009 & 0.027 & 0.000\\
9 & 0.811 & 0.683 & 1.046 & 0.009 & 0.148 & 0.032 & 0.076 & -0.025\\
\addlinespace
11 & 0.937 & 0.120 & 0.494 & -0.246 & 0.086 & 0.144 & 0.258 & 0.049\\
\bottomrule
\end{tabular}
\end{table}

\normalsize

We observe that the values of the first column (market betas) revolve
around one, which is what we would expect. Finally, we are ready for the
second round of regressions.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{models <-}\StringTok{ }\KeywordTok{lapply}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"`"}\NormalTok{, returns}\OperatorTok{$}\NormalTok{date, }\StringTok{"`"}\NormalTok{, }\StringTok{' ~  MKT_RF + SMB + HML + RMW + CMA'}\NormalTok{, }\DataTypeTok{sep =} \StringTok{""}\NormalTok{),}
\ControlFlowTok{function}\NormalTok{(f)\{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{as.formula}\NormalTok{(f), }\DataTypeTok{data =}\NormalTok{ FM_data) }\OperatorTok{%>%}\StringTok{                        }\CommentTok{# Call lm(.)}
\StringTok{                         }\KeywordTok{summary}\NormalTok{() }\OperatorTok{%>%}\StringTok{                                    }\CommentTok{# Gather the output}
\StringTok{                         "$"}\NormalTok{(coef) }\OperatorTok{%>%}\StringTok{                                    }\CommentTok{# Keep only the coefs}
\StringTok{                         }\KeywordTok{data.frame}\NormalTok{() }\OperatorTok{%>%}\StringTok{                                 }\CommentTok{# Convert to dataframe}
\StringTok{                         }\KeywordTok{select}\NormalTok{(Estimate)\}                                }\CommentTok{# Keep only estimates}
\NormalTok{                 )}
\NormalTok{gammas <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{unlist}\NormalTok{(models), }\DataTypeTok{ncol =}\NormalTok{ nb_factors }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{, }\DataTypeTok{byrow =}\NormalTok{ T) }\OperatorTok{%>%}\StringTok{    }\CommentTok{# Switch to dataframe}
\StringTok{    }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{row.names =}\NormalTok{ returns}\OperatorTok{$}\NormalTok{date)                                  }\CommentTok{# & set row names}
\KeywordTok{colnames}\NormalTok{(gammas) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Constant"}\NormalTok{, }\StringTok{"MKT_RF"}\NormalTok{, }\StringTok{"SMB"}\NormalTok{, }\StringTok{"HML"}\NormalTok{, }\StringTok{"RMW"}\NormalTok{, }\StringTok{"CMA"}\NormalTok{)   }\CommentTok{# Set col names}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\KeywordTok{head}\NormalTok{(gammas }\OperatorTok{%>%}\StringTok{ }\KeywordTok{round}\NormalTok{(}\DecValTok{3}\NormalTok{)),  }\DataTypeTok{booktabs =} \OtherTok{TRUE}\NormalTok{,    }\CommentTok{# The gammas (see table)}
             \DataTypeTok{caption =} \StringTok{"Sample of gamma (premia) values."}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{table}[t]

\caption{\label{tab:FamaMacBeth2b}Sample of gamma (premia) values.}
\centering
\begin{tabular}{lrrrrrr}
\toprule
  & Constant & MKT\_RF & SMB & HML & RMW & CMA\\
\midrule
2000-01-31 & -0.031 & 0.037 & 0.227 & -0.157 & -0.276 & 0.044\\
2000-02-29 & 0.020 & 0.081 & -0.134 & 0.050 & 0.089 & -0.027\\
2000-03-31 & 0.007 & -0.011 & -0.016 & 0.054 & 0.036 & 0.039\\
2000-04-30 & 0.127 & -0.132 & -0.104 & 0.088 & 0.117 & -0.002\\
2000-05-31 & 0.042 & -0.005 & 0.075 & -0.113 & -0.080 & -0.045\\
\addlinespace
2000-06-30 & 0.028 & -0.029 & -0.019 & 0.054 & 0.045 & 0.017\\
\bottomrule
\end{tabular}
\end{table}

\normalsize

Visually, the estimated premia are also very volatile. We plot their
estimated values for the market, SMB and HML factors. \footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gammas }\OperatorTok{%>%}\StringTok{                                                          }\CommentTok{# Take gammas:}
\StringTok{    }\KeywordTok{select}\NormalTok{(MKT_RF, SMB, HML) }\OperatorTok{%>%}\StringTok{                                    }\CommentTok{# Select 3 factors}
\StringTok{    }\KeywordTok{bind_cols}\NormalTok{(}\DataTypeTok{date =}\NormalTok{ data_FM}\OperatorTok{$}\NormalTok{date) }\OperatorTok{%>%}\StringTok{                              }\CommentTok{# Add date}
\StringTok{    }\KeywordTok{gather}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ factor, }\DataTypeTok{value =}\NormalTok{ gamma, }\OperatorTok{-}\NormalTok{date) }\OperatorTok{%>%}\StringTok{                  }\CommentTok{# Put in tidy shape}
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ date, }\DataTypeTok{y =}\NormalTok{ gamma, }\DataTypeTok{color =}\NormalTok{ factor)) }\OperatorTok{+}\StringTok{              }\CommentTok{# Plot}
\StringTok{    }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{facet_grid}\NormalTok{( factor}\OperatorTok{~}\NormalTok{. ) }\OperatorTok{+}\StringTok{                          }\CommentTok{# Lines & facets}
\StringTok{    }\KeywordTok{scale_color_manual}\NormalTok{(}\DataTypeTok{values=}\KeywordTok{c}\NormalTok{(}\StringTok{"#F87E1F"}\NormalTok{, }\StringTok{"#0570EA"}\NormalTok{, }\StringTok{"#F81F40"}\NormalTok{)) }\OperatorTok{+}\StringTok{ }\CommentTok{# Colors}
\StringTok{    }\KeywordTok{coord_fixed}\NormalTok{(}\DecValTok{980}\NormalTok{)                                                }\CommentTok{# Fix x/y ratio}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{ML_factor_files/figure-latex/premiaplot-1.pdf}
\caption{\label{fig:premiaplot}Time-series plot of gammas (premia) in
Fama-Macbeth regressions}
\end{figure}

\normalsize

The two spikes at the end of the sample signal potential colinearity
issues: two factors seem to compensate in an unclear aggregate effect.
This underlines the usefulness of penalized estimates (see Chapter
\ref{lasso}).

\hypertarget{factor-competition}{%
\subsection{Factor competition}\label{factor-competition}}

The core purpose of factors is to explain the cross-section of stock
returns. For theoretical and practical reasons, it is preferable if
redundancies within factors are avoided. Indeed, redundancies imply
collinearity which is known to perturb estimates
(\citet{belsley2005regression}). In addition, when asset managers
decompose the performance of their returns into factors, overlap between
factors yield exposures that are less interpretable often because
positive and negative exposures compensate each other spuriously.

A simple protocol to sort out redundant factors is to run regressions of
each factor against all others: \begin{equation}
 \label{eq:faccompet}
f_{t,k} = a_k +\sum_{j\neq k} \delta_{k,j} f_{t,j} + \epsilon_{t,k}.
\end{equation} The interesting metric is then the test statistic
associated to the estimation of \(a_k\). If \(a_k\) is significantly
different from zero, then the cross-section of (other) factors fails to
explain exhaustively the average return of factor \(k\). Otherwise, the
return of the factor can be captured be exposures to the other factors
and is thus redundant.

One mainstream application of this technique was performed in
\citet{fama2015five}, in which the authors show that the HML factor is
redundant when taking into account four other factors (Market, SMB, RMW
and CMA). Below, we reproduce their analysis on an updated sample. We
start our analysis directly with the database maintained by Kenneth
French.

We can run the regressions that determine the redundancy of factors via
the procedure defined in Equation \eqref{eq:faccompet}.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{factors <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"MKT_RF"}\NormalTok{, }\StringTok{"SMB"}\NormalTok{, }\StringTok{"HML"}\NormalTok{, }\StringTok{"RMW"}\NormalTok{, }\StringTok{"CMA"}\NormalTok{)}
\NormalTok{models <-}\StringTok{ }\KeywordTok{lapply}\NormalTok{(}\KeywordTok{paste}\NormalTok{(factors, }\StringTok{' ~  MKT_RF + SMB + HML + RMW + CMA-'}\NormalTok{,factors),}
 \ControlFlowTok{function}\NormalTok{(f)\{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{as.formula}\NormalTok{(f), }\DataTypeTok{data =}\NormalTok{ FF_factors) }\OperatorTok{%>%}\StringTok{               }\CommentTok{# Call lm(.)}
\StringTok{                         }\KeywordTok{summary}\NormalTok{() }\OperatorTok{%>%}\StringTok{                               }\CommentTok{# Gather the output}
\StringTok{                         "$"}\NormalTok{(coef) }\OperatorTok{%>%}\StringTok{                               }\CommentTok{# Keep only the coefs}
\StringTok{                         }\KeywordTok{data.frame}\NormalTok{() }\OperatorTok{%>%}\StringTok{                            }\CommentTok{# Convert to dataframe}
\StringTok{                         }\KeywordTok{filter}\NormalTok{(}\KeywordTok{rownames}\NormalTok{(.) }\OperatorTok{==}\StringTok{ "(Intercept)"}\NormalTok{) }\OperatorTok{%>%}\StringTok{    }\CommentTok{# Keep only the Intercept}
\StringTok{                         }\KeywordTok{select}\NormalTok{(Estimate,}\StringTok{`}\DataTypeTok{Pr...t..}\StringTok{`}\NormalTok{)\}                }\CommentTok{# Keep the coef & p-value}
\NormalTok{                 )}
\NormalTok{alphas <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{unlist}\NormalTok{(models), }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{, }\DataTypeTok{byrow =}\NormalTok{ T) }\OperatorTok{%>%}\StringTok{       }\CommentTok{# Switch from list to dataframe}
\StringTok{    }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{row.names =}\NormalTok{ factors)}
\CommentTok{# alphas # To see the alphas (optional)}
\end{Highlighting}
\end{Shaded}

\normalsize

We obtain the vector of \(\alpha\) values from equation
(\ref{eq:faccompet}). Below, we format these figures along with
\(p\)-value thresholds and export them in a summary table. The
significance levels of coefficients is coded as follows:
\(0<(***)<0.001<(**)<0.01<(*)<0.05\).

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =} \KeywordTok{length}\NormalTok{(factors), }\DataTypeTok{ncol =} \KeywordTok{length}\NormalTok{(factors) }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)   }\CommentTok{# Coefs}
\NormalTok{signif  <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =} \KeywordTok{length}\NormalTok{(factors), }\DataTypeTok{ncol =} \KeywordTok{length}\NormalTok{(factors) }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)   }\CommentTok{# p-values}
\ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(factors))\{}
\NormalTok{    form <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(factors[j],}
                  \StringTok{' ~  MKT_RF + SMB + HML + RMW + CMA-'}\NormalTok{,factors[j])         }\CommentTok{# Build model}
\NormalTok{    fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(form, }\DataTypeTok{data =}\NormalTok{ FF_factors) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{summary}\NormalTok{()                        }\CommentTok{# Estimate model}
\NormalTok{    coef <-}\StringTok{ }\NormalTok{fit}\OperatorTok{$}\NormalTok{coefficients[,}\DecValTok{1}\NormalTok{]                                            }\CommentTok{# Keep coefficients}
\NormalTok{    p_val <-}\StringTok{ }\NormalTok{fit}\OperatorTok{$}\NormalTok{coefficients[,}\DecValTok{4}\NormalTok{]                                           }\CommentTok{# Keep p-values}
\NormalTok{    results[j,}\OperatorTok{-}\NormalTok{(j}\OperatorTok{+}\DecValTok{1}\NormalTok{)] <-}\StringTok{ }\NormalTok{coef                                               }\CommentTok{# Fill matrix}
\NormalTok{    signif[j,}\OperatorTok{-}\NormalTok{(j}\OperatorTok{+}\DecValTok{1}\NormalTok{)] <-}\StringTok{ }\NormalTok{p_val}
\NormalTok{\}}
\NormalTok{signif[}\KeywordTok{is.na}\NormalTok{(signif)] <-}\StringTok{ }\DecValTok{1}                                                  \CommentTok{# Kick out NAs}
\NormalTok{results <-}\StringTok{ }\NormalTok{results }\OperatorTok{%>%}\StringTok{ }\KeywordTok{round}\NormalTok{(}\DecValTok{3}\NormalTok{)  }\OperatorTok{%>%}\StringTok{ }\KeywordTok{data.frame}\NormalTok{()                           }\CommentTok{# Basic formatting}
\NormalTok{results[signif}\OperatorTok{<}\FloatTok{0.001}\NormalTok{] <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(results[signif}\OperatorTok{<}\FloatTok{0.001}\NormalTok{],}\StringTok{" (***)"}\NormalTok{)              }\CommentTok{# 3 star signif}
\NormalTok{results[signif}\OperatorTok{>}\FloatTok{0.001}\OperatorTok{&}\NormalTok{signif}\OperatorTok{<}\FloatTok{0.01}\NormalTok{] <-}\StringTok{                                        }\CommentTok{# 2 star signif}
\StringTok{    }\KeywordTok{paste}\NormalTok{(results[signif}\OperatorTok{>}\FloatTok{0.001}\OperatorTok{&}\NormalTok{signif}\OperatorTok{<}\FloatTok{0.01}\NormalTok{],}\StringTok{" (**)"}\NormalTok{)}
\NormalTok{results[signif}\OperatorTok{>}\FloatTok{0.01}\OperatorTok{&}\NormalTok{signif}\OperatorTok{<}\FloatTok{0.05}\NormalTok{] <-}\StringTok{                                         }\CommentTok{# 1 star signif}
\StringTok{    }\KeywordTok{paste}\NormalTok{(results[signif}\OperatorTok{>}\FloatTok{0.01}\OperatorTok{&}\NormalTok{signif}\OperatorTok{<}\FloatTok{0.05}\NormalTok{],}\StringTok{" (*)"}\NormalTok{)     }

\NormalTok{results <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\KeywordTok{as.character}\NormalTok{(factors), results)                            }\CommentTok{# Add dep. variable}
\KeywordTok{colnames}\NormalTok{(results) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Dep. Variable"}\NormalTok{,}\StringTok{"Intercept"}\NormalTok{, factors)                }\CommentTok{# Add column names}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(results,  }\DataTypeTok{booktabs =} \OtherTok{TRUE}\NormalTok{,}
             \DataTypeTok{caption =} \KeywordTok{paste0}\NormalTok{(}\StringTok{"Factor competition among the Fama French 2015 five factors. "}\NormalTok{,}
                              \StringTok{"The sample starts in "}\NormalTok{,}\KeywordTok{substr}\NormalTok{(min_date,}\DecValTok{1}\NormalTok{,}\DecValTok{7}\NormalTok{), }
                              \StringTok{" and ends in "}\NormalTok{, }\KeywordTok{substr}\NormalTok{(max_date,}\DecValTok{1}\NormalTok{,}\DecValTok{7}\NormalTok{),}
                              \StringTok{". The regressions are run on monthly returns. "}
\NormalTok{                       )}
\NormalTok{       ) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{kable_styling}\NormalTok{(}\DataTypeTok{font_size =} \DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}[t]

\caption{\label{tab:faccompet2}Factor competition among the Fama French 2015 five factors. The sample starts in 1963-07 and ends in 2019-11. The regressions are run on monthly returns. }
\centering
\fontsize{9}{11}\selectfont
\begin{tabular}{lllllll}
\toprule
Dep. Variable & Intercept & MKT\_RF & SMB & HML & RMW & CMA\\
\midrule
MKT\_RF & 0.008  (***) & NA & 0.252  (***) & 0.095 & -0.378  (***) & -0.916  (***)\\
SMB & 0.003  (**) & 0.13  (***) & NA & 0.066 & -0.434  (***) & -0.128\\
HML & 0 & 0.026 & 0.035 & NA & 0.143  (***) & 1.011  (***)\\
RMW & 0.004  (***) & -0.099  (***) & -0.221  (***) & 0.137  (***) & NA & -0.291  (***)\\
CMA & 0.002  (***) & -0.112  (***) & -0.03 & 0.453  (***) & -0.136  (***) & NA\\
\bottomrule
\end{tabular}
\end{table}

\normalsize

We confirm that the HML factor remains redundant when the four other are
present in the asset pricing model. The figures we obtain are very close
to the ones in the original paper (\citet{fama2015five}), which makes
sense, since we only add 5 years to their initial sample.

At a more macro-level, researchers also try to figure out which models
(i.e., combinations of factors) are the most likely, given the data
empirically observed (and possibly given priors formulated by the
econometrician). For instance, this stream of literature seeks to
quantify to which extent the 3 factor model of \citet{fama1993common}
outperforms the 5 factors in \citet{fama2015five}. In this direction,
\citet{de2015comparing} introduce a novel computation for
\emph{p}-values that compare the relative likelihood that two models
pass a zero-alpha test. More generally, the Bayesian method of
\citet{barillas2018comparing} was subsequently improved by
\citet{chib2019comparing}.

\hypertarget{advanced-techniques}{%
\subsection{Advanced techniques}\label{advanced-techniques}}

The ever increasing number of factors combined to their importance in
asset management has led researchers to craft more subtle methods in
order to ``organise'' the so-called \emph{factor zoo} and more
importantly, to detect spurious anomalies and compare different asset
pricing model specifications. We list a few of them below.

\begin{itemize}
\tightlist
\item
  \citet{feng2019taming} combine LASSO selection with Fama-MacBeth
  regressions to test if new factor models are worth it. They quantify
  the gain of adding one new factor to a set of predefined factors and
  show that many factors reported in paper published in the 2010 decade
  do not add much incremental value;\\
\item
  \citet{harvey2017lucky} (in a similar vein) use bootstrap on
  orthogonalised factors. They make the case that correlations among
  predictors is a major issue and their method aims at solving this
  problem. Their lengthy procedure seeks to test if maximal additional
  contribution of a candidate variable is significant;\\
\item
  \citet{fama2018choosing} compare asset pricing models through squared
  maximum Sharpe ratios;\\
\item
  \citet{giglio2018asset} estimate factor risk premia using a three-pass
  method based on principal component analysis;\\
\item
  \citet{pukthuanthong2018protocol} disentangle priced and non-priced
  factors via a combination of principal component analysis and
  \citet{fama1973risk} regressions.\\
\item
  \citet{gospodinov2019too} warn against factor misspecification (when
  spurious factors are included in the list of regressors). Traded
  factors (\(resp.\) macro-economic factors) seem more likely (\(resp.\)
  less likely) to yield robust identifications (see also
  \citet{bryzgalova2019spurious}).
\end{itemize}

There is obviously no infaillible method, but the number of
contributions in the field highlights the need for robustness. This is
evidently a major concern when crafting investment decisions based on
factor intuitions. One major hurdle for short-term strategies is the
likely time-varying feature of factors. We refer for instance to
\citet{ang2012testing} and \citet{cooper2018new} for practical results
and to \citet{gagliardini2016time} and \citet{ma2018testing}) for more
theoretical treatments (with additional empirical results).

\hypertarget{factors-or-characteristics}{%
\section{Factors or characteristics?}\label{factors-or-characteristics}}

The decomposition of returns into linear factor models is convenient
because of its simple interpretation. There is nonetheless a debate in
the academic literature about whether firm returns are indeed explained
by exposure to macro-economic factors or simply by the characteristics
of firms. In their early study, \citet{lakonishok1994contrarian} argue
that one explanation of the value premium comes from incorrect
extrapolation of past earning growth rates. Investors are overly
optimistic about firms subject to recent profitability. Consequently,
future returns are (also) driven by the core (accounting) features of
the firm. The question is then to disentangle which effect is the most
pronounced when explaining returns: characteristics versus exposures to
macro-economic factors?

In their seminal contribution on this topic, \citet{daniel1997evidence}
provide evidence in favour of the former (two follow-up papers are
\citet{daniel2001explaining} and \citet{daniel2012testing}). They show
that firms with high book-to-market ratios or small capitalisations
display higher average returns, even if they are negatively loaded on
the HML or SMB factors. Therefore, it seems that it is indeed the
intrinsic characteristics that matter, and not the factor exposure. For
further material on characteristics' role in return explanation or
prediction, we refer to the following contributions:

\begin{itemize}
\tightlist
\item
  Section 2.5.2. in \citet{goyal2012empirical} surveys pre-2010 results
  on this topic;\\
\item
  \citet{chordia2015cross} find that characteristics explain a larger
  proportion of variation in estimated expected returns than factor
  loadings;\\
\item
  \citet{kozak2018interpreting} reconcile factor-based explanations of
  premia to a theoretical model in which some agents' demands are
  sentiment driven;\\
\item
  \citet{han2018firm} show with penalised regressions that 20 to 30
  characteristics (out of 94) are useful for the prediction of monthly
  returns of US stocks. Their methodology is interesting: they regress
  returns against characteristics to build forecasts and then regress
  the returns on the forecast to assess if the forecasts are reliable.
  The latter regression uses a LASSO-type penalization (see Chapter
  \ref{lasso}) so that useless characteristics are excluded from the
  model. The penalization is extended to elasticnet in
  \citet{rapach2019time}.\\
\item
  both \citet{kelly2019characteristics} and \citet{kim2019arbitrage}
  estimate models in which \emph{factors} are \emph{latent} but loadings
  (betas) and possibly alphas depend on characteristics. In contrast,
  \citet{lettau2018estimating} and \citet{lettau2018factors} estimate
  latent factors without any link to particular characteristics (and
  provide large sample asymptotic properties of their methods).\\
\item
  in the same vein as \citet{hoechle2018correcting},
  \citet{gospodinov2019too} and \citet{bryzgalova2019spurious} and
  discuss potential errors that arise when working with portfolio sorts
  that yield long-short returns. The authors show that in some cases,
  tests based on this procedure may be deceitful. This happens when the
  characteristic chosen to perform the sort is correlated with an
  external (unobservable) factor. They propose a novel regression-based
  approach aimed at bypassing this problem.
\end{itemize}

More recently and in a separate stream of literature,
\citet{koijen2019demand} have introduced a demand model in which
investors for their portfolios according to their preferences towards
particular firm characteristics. They show that this allows them to
mimic the portfolios of large institutional investors. In their model,
aggregate demands (and hence, prices) are directly linked to
characteristics, not to factors. In a follow-up paper,
\citet{koijen2019investors} show that a few set of characteristics
suffice to predict future returns. They also show that, based on
institutional holdings from the UK and the US, the largest investors are
those who are the most influencial in the formation of prices. In a
similar vein, \citet{betermier2019supply} derive an elegant
(theoretical) general equilibrium model that generates some well
documented anomalies (size, book-to-market). Finally, in
\citet{martin2019market}, characteristics influence returns via the role
they play in the predictability of dividend growth. This paper discussed
the asymptotic case when the number of assets and the number of
characteristics are proportional and both increase to infinity.

\hypertarget{the-link-with-machine-learning}{%
\section{The link with machine
learning}\label{the-link-with-machine-learning}}

Given the exponential increase in data availability, the obvious
temptation of any asset manager is to try to infer future returns from
the abundance of attributes available at the firm level. We allude to
classical data like accounting ratios and to alternative data, like
sentiment. This task is precisely the aim of Machine Learning. Given a
large set of predictor variables (\(\mathbf{X}\)), the goal is to
predict a proxy for future performance \(\mathbf{y}\) through a model of
the form \eqref{eq:ML}.

Some attempts toward this direction have already been made (e.g.,
\citet{brandt2009parametric}, \citet{ammann2016characteristics},
\citet{martin2018transaction}), but not with any ML intent or focus.

\hypertarget{a-short-list-of-recent-references}{%
\subsection{A short list of recent
references}\label{a-short-list-of-recent-references}}

Independently of a characteristics-based approach, ML applications in
Finance have blossomed, initially working with price data only and later
on integrating firm characteristics as predictors. We cite a few
references below, grouped by methodological approach:

\begin{itemize}
\tightlist
\item
  penalised quadratic programming: \citet{goto2015improving},
  \citet{ban2016machine} and \citet{perrin2019machine},
\item
  regularised predictive regressions: \citet{rapach2013international}
  and \citet{chinco2019sparse},
\item
  support vector machines: \citet{cao2003support} (and the references
  therein),
\item
  model comparison and/or aggregation: \citet{kim2003financial},
  \citet{huang2005forecasting}, \citet{matias2012forecasting},
  \citet{reboredo2012nonlinearity}, \citet{dunis2013hybrid},
  \citet{gu2018empirical} and \citet{guida2018machine}. The latter two
  more recent articles work with a large cross-section of
  characteristics.
\end{itemize}

We provide more detailed lists for tree-based methods, neural networks
and reinforcement learning techniques in Chapters \ref{trees}, \ref{NN}
and \ref{RL}, respectively. Moreover, we refer to
\citet{ballings2015evaluating} for a comparison of classifiers and to
\citet{henrique2019literature} for a survey on ML-based forecasting
techniques.

\hypertarget{explicit-connexions-with-asset-pricing-models}{%
\subsection{Explicit connexions with asset pricing
models}\label{explicit-connexions-with-asset-pricing-models}}

The first and obvious link between factor investing and asset pricing is
(average) return prediction. The main canonical academic reference is
\citet{gu2018empirical}. Let us first write the general equation and
then comment on it: \begin{equation}
\label{eq:genML}
r_{t+1,n}=g(\textbf{x}_{t,n}) + \epsilon_{t+1}.
\end{equation}

The interesting discussion lies in the differences between the above
model and that of Equation \eqref{eq:apt}. The first obvious difference is
the introduction of the nonlinear function \(g\): indeed, there is no
reason (beyond simplicity and interpretability) why we should restrict
the model to linear relationships. One early reference for
nonlinearities in asset pricing kernels is \citet{bansal1993no}.

More importantly, the second difference between \eqref{eq:genML} and
\eqref{eq:apt} is the shift in the time index. Indeed, from an investor's
perspective, the interest is to be able to \emph{predict} some
information about the structure of the cross-section of assets.
Explaining asset returns with synchronous factors is not useful because
the realization of factor values are not known in advance. Hence, if one
seeks to extract value from the model, there needs to be a time interval
between the observation of the state space (which we call
\(\textbf{x}_{t,n}\)) and the occurrence of the returns. Once the model
\(\hat{g}\) is estimated, the time-\(t\) (measurable) value
\(g(\textbf{x}_{t,n})\) will give a forecast for the (average) future
returns. These predictions can then serve as signal in the crafting of
portfolio weights (see Chapter \ref{backtest} for more on that topic).

While most studies do work with returns on the l.h.s. of \eqref{eq:genML},
there is no reason why other indicators should not be used. Returns are
straightforward and simple to compute, but they could very well be
replaced by more sophisticated metrics, like the Sharpe ratio, for
instance. The firms' features would then be used to predict a
risk-adjusted performance rather than simple returns.

Beyond the explicit form of Equation \eqref{eq:genML}, several other
ML-related tools can also be used to estimate asset pricing models. This
can be achieved in several ways, some of which we list below.

First, one mainstream problems in asset pricing is to characterize the
stochastic discount factor (SDF) \(M_t\), which satisfies
\(\mathbb{E}_t[M_{t+1}(r_{t+1,n}-r_{t+1,f})]=0\) for any asset \(n\)
(see \citet{cochrane2009asset}). This equation is a natural playing
field for the generalized method of moment (\citet{hansen1982large}):
\(M_t\) must be such that \begin{equation}
\label{eq:SDFGMM}
\mathbb{E}[M_{t+1}R_{t+1,n}g(V_t)]=0,
\end{equation} where the instrumental variables \(V_t\) are
\(\mathcal{F}_t\)-measurable (i.e., are known at time \(t\)) and the
capital \(R_{t+1,n}\) denotes the excess return of asset \(n\). In order
to reduce and simplify the estimation problem, it is customary to define
the SDF as a portfolio of assets (see Chapter 3 in
\citet{back2010asset}). In \citet{chen2019deep}, the authors use a
generative adversarial network (GAN, see Section
\ref{generative-aversarial-networks}) to estimate the weights of the
portfolios that are the closest to satisfy \eqref{eq:SDFGMM} under a
strongly penalizing form.

A second approach is to try to model asset returns as linear
combinations of factors, just as in \eqref{eq:apt}. We write in compact
notation
\[r_{t,n}=\alpha_n+\boldsymbol{\beta}_{t,n}'\textbf{f}_t+\epsilon_{t,n},\]
and we allow the loadings \(\boldsymbol{\beta}_{t,n}\) to be
time-dependent. The trick is then to introduce the firm characteristics
in the above equation. Traditionally, the characteristics are present in
the definition of factors (as in the seminal definition of
\citet{fama1993common}). The decomposition of the return is made
according to the exposition of the firm's return to these factors
constructed according to market size, accounting ratios, past
performance etc. Given the exposures, the performance of the stock is
attributed to particular style profiles (e.g., small stock, or value
stock, etc.).

Habitually, the factors are heuristic portfolios constructed from simple
rules like thresholding. For instance, firms below the 1/3 quantile in
book-to-market are growth firms and those above the 2/3 quantile are the
value firms. A value factor can then be defined by the long-short
portfolio of these two sets, with uniform weights. Note that
\citet{fama1993common} use a more complex approach which also take
market capitalization into account both in the weighting scheme and also
in the composition of the portfolios.

One of the advances enabled by machine learning is to automate the
construction of the factors. It is for instance the approach of
\citet{feng2019deep}. Instead of building the factors heuristically, the
authors optimize the construction to maximize the fit in the
cross-section of returns. The optimization is performed via a relatively
deep feed-forward neural network and the feature space is lagged so that
the relationship is indeed predictive, as in Equation \eqref{eq:genML}.
Theoretically, the resulting factors help explain a substantially larger
porportion of the in-sample variance in the returns. The prediction
ability of the model depends on how well it generalizes out-of-sample.

A third approach is that of \citet{kelly2019characteristics} (though the
statistical treatment is not machine learning per se).\footnote{In the
  same spirit, see also \citet{lettau2018estimating} and
  \citet{lettau2018factors}.} Their idea is the opposite: factors are
latent (unobserved) and it is the betas (loadings) that depend on the
characteristics. This allows many degrees of freedom because in
\(r_{t,n}=\alpha_n+(\boldsymbol{\beta}_{t,n}(\textbf{x}_{t-1,n}))'\textbf{f}_t+\epsilon_{t,n},\)
only the characteristics \(\textbf{x}_{t-1,n}\) are known and both the
factors \(\textbf{f}_t\) and the functional forms
\(\boldsymbol{\beta}_{t,n}(\cdot)\) must be estimated. In their article,
\citet{kelly2019characteristics} work with a linear form, which is
naturally more tractable.

Lastly, a fourth approach (introduced in \citet{gu2019autoencoder}) goes
even further and combines two neural network architectures. The first
neural network takes characteristics \(\textbf{x}_{t-1}\) as inputs and
generates factor loadings
\(\boldsymbol{\beta}_{t-1}(\textbf{x}_{t-1})\). The second network
transforms returns \(\textbf{r}_t\) into factor values
\(\textbf{f}_t(\textbf{r}_t)\) (in \citet{feng2019deep}). The aggregate
model can then be written: \begin{equation}
\label{eq:AEearly}
\textbf{r}_t=\boldsymbol{\beta}_{t-1}(\textbf{x}_{t-1})'\textbf{f}_t(\textbf{r}_t)+\boldsymbol{\epsilon}_t.
\end{equation}

The above specification is quite special because the output (on the left
hand side) is also present as input (in the right hand side). In machine
learning, autoencoders (see Section \ref{autoencoders}) share the same
property. Their aim, just like in principal component analysis, is to
find a parsimonious nonlinear representation form for a dataset (in this
case: returns). In Equation \eqref{eq:AEearly}, the input is
\(\textbf{r}_t\) and the output function is
\(\boldsymbol{\beta}_{t-1}(\textbf{x}_{t-1})'\textbf{f}_t(\textbf{r}_t)\).
The aim is to minimize the difference between the two just as is any
regression-like model.

Autoencoders are neural networks which have outputs as close as possible
to the inputs with an objective of dimensional reduction. The innovation
in \citet{gu2019autoencoder} is that the pure autoencoder part is merged
with a vanilla perceptron used to model the loadings. The structure of
the neural network is summarized below.

\[\left. \begin{array}{rl}
\text{returns } (\textbf{r}_t) & \overset{NN_1}{\longrightarrow} \quad \text{ factors } (\textbf{f}_t=NN_1(\textbf{r}_t)) \\
\text{characteristics } (\textbf{x}_{t-1}) & \overset{NN_2}{\longrightarrow} \quad \text{ loadings } (\boldsymbol{\beta}_{t-1}=NN_2(\textbf{x}_{t-1}))
\end{array} \right\} \longrightarrow \text{ returns } (r_t)\]

A simple autoencoder would consist in only the first line of the model.

As a conclusion of this chapter, it appears undeniable that the
intersection between the two fields of asset pricing and machine
learning offers a rich variety of applications. The literature is
already exhaustive and it is often hard to disentangle the noise from
the great ideas in the continuous flow of publications on these topics.
Practice and implementation is the only way forward to extricate value
from hype.

\hypertarget{coding-exercises}{%
\section{Coding exercises}\label{coding-exercises}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute annual returns of the growth versus value portfolios, that is,
  the average return of firms with above median price-to-book ratio (Pb
  variable in the dataset).\\
\item
  Same exercise, but compute the monthly returns and plot the value
  (through time) of the corresponding portfolios.\\
\item
  Instead of a unique threshold, compute simply sorted portfolios based
  on quartiles of market capitalization.
\end{enumerate}

\hypertarget{Data}{%
\chapter{Data preprocessing}\label{Data}}

The methods we describe in this chapter are driven by financial
applications. For an introduction to non-financial data processing, we
recommend two references: Chapter 3 from the general purpose ML book
\citet{boehmke2019hands} and the monograph on this dedicated subject:
\citet{kuhn2019feature}.

\hypertarget{know-your-data}{%
\section{Know your data}\label{know-your-data}}

The first step, as in any quantitative study, is obviously to make sure
the data is trustworthy, i.e., comes from a reliable provider. The
landscape in financial data provision is vast to say the least: some
providers are well established (e.g., Bloomberg, Thomson-Reuters,
Datastream, CRSP, Morningstar), some are more recent (e.g., Capital IQ,
Ravenpack) and some focus on alternative data niches (see
\url{https://alternativedata.org/data-providers/} for an exhaustive
list). Unfortunately, and to the best of our knowledge, no study has
been published that evaluates a large spectrum of these providers in
terms of data reliability.

The second step is to have a look at \textbf{summary statistics}: ranges
(minimum and maximum values), and averages and medians. Histograms or
plots of time-series carry of course more information but cannot be
analyzed properly in high dimensions. They are nonetheless sometimes
useful to track local patterns or errors for a given stock and a
particular feature. Beyond first order moments, second order quantities
(variances and covariances/correlations) also matter because they help
spot colinearities. When two features are highly correlated, problems
may arise in some models (e.g., simple regressions, see Section
\ref{corpred}).

Often, the number of predictors is so large that it is unpractical to
look at these simple metrics. A minimal verification is recommended. To
further ease the analysis:

\begin{itemize}
\tightlist
\item
  focus on a subset of predictors, e.g., the ones linked to the most
  common factors (market-capitalization, price-to-book or
  book-to-market, momentum (past returns), profitability, asset growth,
  volatility);\\
\item
  track outliers in the summary statistics (when the maximum/median or
  median/minimum ratios seem suspicious).
\end{itemize}

More importantly, when seeking to work with supervised learning (as we
will do most of the time), the link of some features with the dependent
variable can be further characterized by the smoothed
\textbf{conditional average} because it shows how the features impact
the label. The use of the conditional average has a deep theoretical
grounding. Suppose there is only one feature \(X\) and that we seek a
model \(Y=f(X)+\text{error}\), where variables are real-valued. The
function \(f\) that minimizes the average squared error
\(\mathbb{E}[(Y-f(X))^2]\) is the so-called regression function (see
Section 2.4 in \citet{friedman2009elements}): \begin{equation}
\label{eq:regfun}
f(x)=\mathbb{E}[Y|X=x].
\end{equation}

In Figure \ref{fig:regfun}, we plot two illustrations of this function
when the dependent variable is the one month ahead return. The first one
pertains to the average market capitalization over the past year and the
second to the volatility over the past year as well. Both predictors
have been uniformized (see Section \ref{scaling} below) so that their
values are uniformly distributed in the cross-section of assets for any
given time period. Thus, the range of features is \([0,1]\) and is shown
on the \(x\)-axis of the plot. The grey corridors around the lines show
95\% level confidence interval for the computation of the mean.
Essentially, it is narrow when both i) many data points are available
and ii) these points are not too dispersed.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data_ml }\OperatorTok{%>%}\StringTok{                                                      }\CommentTok{# From dataset:}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ R1M_Usd)) }\OperatorTok{+}\StringTok{                                     }\CommentTok{# Plot}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Mkt_Cap_12M_Usd, }\DataTypeTok{color =} \StringTok{"Market Cap"}\NormalTok{)) }\OperatorTok{+}\StringTok{  }\CommentTok{# Cond. Exp. Mkt_cap}
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Vol1Y_Usd, }\DataTypeTok{color =} \StringTok{"Volatility"}\NormalTok{)) }\OperatorTok{+}\StringTok{        }\CommentTok{# Cond. Exp. Vol}
\StringTok{  }\KeywordTok{scale_color_manual}\NormalTok{(}\DataTypeTok{values=}\KeywordTok{c}\NormalTok{(}\StringTok{"#F87E1F"}\NormalTok{, }\StringTok{"#0570EA"}\NormalTok{)) }\OperatorTok{+}\StringTok{           }\CommentTok{# Change color}
\StringTok{  }\KeywordTok{coord_fixed}\NormalTok{(}\DecValTok{10}\NormalTok{)                                                }\CommentTok{# Change x/y ratio}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{ML_factor_files/figure-latex/regfun-1.pdf}
\caption{\label{fig:regfun}Conditional expecations: average returns as
smooth functions of features.}
\end{figure}

\normalsize

The two variables have a close to monotonic impact on future returns.
Returns, on average, decrease with market capitalization (thereby
corroborating the so-called \emph{size} effect). The reverse pattern is
less pronounced for volatility: the curve is rather flat is the first
half of volatility scores and progressively increases, especially over
the last quintile of volatility values (thereby contradicting the
low-vol anomaly).

\hypertarget{missing-data}{%
\section{Missing data}\label{missing-data}}

Similarly to any empirical discipline, portfolio management is bound to
face missing data issues. The topic is well known and several books
detail solutions to this problem (e.g., \citet{allison2001missing},
\citet{enders2010applied}, \citet{little2014statistical} and
\citet{van2018flexible}). While researchers continuously propose new
methods to cope with absent points (\citet{honaker2010missing} or
\citet{che2018recurrent} to cite but a few), we believe that a simple,
heuristic, treatment is usually sufficient as long as some basic
cautious safeguards are enforced.

First of all, there are mainly two ways to deal with missing data:
removal and imputation. Removal is agnostic but costly, especially if
one whole instance is eliminated because of only one missing feature.
Imputation is often prefered but relies on some underlying and
potentially erroneous assumption.

A simplified classification of imputation is the following:

\begin{itemize}
\tightlist
\item
  A basic imputation choice is the median (or mean) of the feature for
  the stock over the past available values. If there is a trend in the
  time series, this will nonetheless alter the trend. Relatedly, this
  method is forward looking.\\
\item
  In time-series contexts with views towards backtesting, the most
  simple imputation comes from previous values: if \(x_t\) is missing,
  replace it with \(x_{t-1}\). This makes sense most of the time because
  past values are all that is available and are by definition backward
  looking. However, in some particular cases, this be a very bad choice
  (see words of caution below).
\item
  Medians and means can also be computed over the cross-section of
  assets. This roughly implies that the missing feature value will be
  relocated in the bulk of observed values. When many values are
  missing, this creates an atom in the distribution of the feature and
  alters the original distribution. One advantage is that this
  imputation is not forward looking.\\
\item
  Many techniques rely on some modelling assumptions for the data
  generating process. We refer to nonparametric approaches
  (\citet{stekhoven2011missforest} and \citet{shah2014comparison}, which
  both rely on random forests, see Chapter \ref{trees}), Bayesian
  imputation (\citet{schafer1999multiple}), maximum likelihood
  approaches (\citet{enders2001primer}, \citet{enders2010applied}),
  interpolation or extrapolation and nearest neighbor algorithms
  (\citet{garcia2009k}). More generally, the four books cited at the
  begining of the subsection detail many such imputation processes.
  Advanced techniques are much more demanding computationally.
\end{itemize}

A few words of caution:

\begin{itemize}
\tightlist
\item
  Interpolation should be avoided at all cost. Accounting values or
  ratios that are released every quarter must never be linearly
  interpolated for the simple reason that this is forward looking. If
  numbers are disclosed in January and April, then interpolating
  February and March requires the knowledge of the April figure, which,
  in live trading will not be known. Resorting to past values is a
  better way to go.\\
\item
  Nevertheless, there are some feature types for which imputation from
  past values should be avoided. First of all, returns should not be
  replicated. By default, a superior choice is to set missing return
  indicators to zero (which is often close to the average or the
  median). A good indicator that can help the decision is the
  persistence of the feature through time. If it is highly
  autocorrelated (and the time-series plot create a smooth curve, like
  for marjet capitalization), then imputation from the past can make
  sense. If not, then it should be avoided.\\
\item
  There are some cases that can require more attention. Let us consider
  the following fictitious sample of dividend yield:
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\toprule
Date & Original yield & Replacement value\tabularnewline
\midrule
\endhead
2015-02 & NA & (preceding (if it exists)\tabularnewline
2015-03 & 0.02 & untouched (none)\tabularnewline
2015-04 & NA & 0.02 (previous)\tabularnewline
2015-05 & NA & 0.02 (previous)\tabularnewline
2015-06 & NA & \(\textcolor{red}{\leftarrow}\)
\textbf{Problem}!\tabularnewline
\bottomrule
\end{longtable}

In this case, the yield is released quarterly, in March, June,
September, etc. But in June, the value is missing. The problem is that
we cannot know if it is missing because of a genuine data glitch, or
because the firm simply did not pay any dividends in June. Thus,
imputation from past value may be erroneous here. There is no perfect
solution but a decision must nevertheless be taken. For dividend data,
three options are:\\
1) Keep the previous value.\\
2) Extrapolate from previous observations (this is very different from
\textbf{inter}polation): for instance, evaluate a trend and pursue that
trend. 3) Set the value to zero. This is tempting but may be sub-optimal
due to dividend smoothing practices from executives (see for instance
\citet{leary2011determinants} and \citet{chen2012dividend} for details
on the subject). For persistent time-series, the first two options are
probably better.

Tests can be perform to evaluate the relative performance of each
option. It is also important to \textbf{remember} these design choices.
There are so many of them that they are easy to forget. Keeping track of
them is obviously compulsory. In the ML pipeline, the scripts pertaining
to data preparation are often key because they do not serve only once!

\hypertarget{outlier-detection}{%
\section{Outlier detection}\label{outlier-detection}}

The topic of outlier detection is also well documented and has its own
surveys (\citet{hodge2004survey}, \citet{chandola2009anomaly} and
\citet{gupta2014outlier}) and a few dedicated books
(\citet{aggarwal2013outlier} and \citet{rousseeuw2005robust}, though the
latter is very focused on regression analysis).

Again, incredibly sophisticated methods may require a lot of effort for
possibly limited gain. Simple heuristic methods, as long as they are
documented in the process may suffice. They often rely on `hard'
thresholds:

\begin{itemize}
\tightlist
\item
  for one given feature (possibly filtered in time), any point outside
  the interval \([\mu-m\sigma, \mu+m\sigma]\) can be deemed an outlier.
  Here \(\mu\) is the mean of the sample and \(\sigma\) the standard
  deviation. The multiple value \(m\) usually belongs to the set
  \(\{3, 5, 10\}\), which is of course arbitrary.
\item
  likewise, if the largest value is above \(m\) times the
  second-to-largest, then is can also be classified as an outlier (the
  same reasoning applied for the other side of the tail).
\item
  finally, for a given small threshold \(q\), any value outside the
  \([q,1-q]\) quantile range can be considered outliers.
\end{itemize}

This latter idea was popularized by winsorization. Winsorizing amounts
to setting to \(x^{(q)}\) all values below \(x^{(q)}\) and to
\(x^{(1-q)}\) all values above \(x^{(1-q)}\). The winsorised variable
\(\tilde{x}\) is: \[\tilde{x}_i=\left\{\begin{array}{ll}
x_i & \text{ if }  x_i \in [x^{(q)},x^{(1-q)}] \quad \text{ (unchanged)}\\
x^{(q)} & \text{ if }  x_i < x^{(q)} \\
x^{(1-q)} & \text{ if }  x_i > x^{(1-q)}
 \end{array} \right. .\]

The range for \(q\) is usually \((0.5\%, 5\%)\) with 1\% and 2\% being
the most often used.

The winsorization stage must be performed on a feature-by-feature and a
data-by-date basis. However, keeping a time-series perspective is also
useful. For instance, a 800B\$ market capitalization may seems out of
range, except when looking at the history of Apple's capitalization.

We conclude this subsection by recalling that \emph{true} outliers (i.e,
extreme points that are not due to data extraction errors) are valuable
because they are likely to carry important information.

\hypertarget{feateng}{%
\section{Feature engineering}\label{feateng}}

Feature engineering is a very important step of the portfolio
construction process. Computer scientistic often refer to the saying
``garbage in, garbage out''. It is thus paramount to prevent the ML
engine of the allocation to be trained on ill-designed variables. We
invite the interested reader to have a look at the recent work of
\citet{kuhn2019feature} on this topic. The (shorter) academic reference
is \citet{guyon2003introduction}.

\hypertarget{feature-selection}{%
\subsection{Feature selection}\label{feature-selection}}

The first step is selection. Given a large set of predictors, it seems a
sound idea to filter out unwanted or redundant exogenous variables.
Heuristically, simple methods include:

\begin{itemize}
\tightlist
\item
  computing the correlation matrix of all features and making sure that
  no (absolute) value is above a threshold (0.7 is a common value) so
  that redundant variables do not pollute the learning engine;\\
\item
  carrying out a linear regression and removing the non significant
  variables (e.g., those with \(p\)-value above 0.05).
\end{itemize}

Both these methods are somewhat reductive and overlook nonlinear
relationships. Another approach would be to fit a decision tree (or a
random forest) and retain only the features that have a high variable
importance. These topics will be developed in Chapter \ref{trees}.

\hypertarget{scaling}{%
\subsection{Scaling the predictors}\label{scaling}}

The premise of the need to pre-process the data comes from the large
variety of scales in financial data:

\begin{itemize}
\tightlist
\item
  returns are most of the time smaller than one in absolute value;
\item
  stock volatility lies usually between 5\% and 80\%;
\item
  market capitalisation is expressed in million or billion units of a
  particular currency;
\item
  accounting values as well;
\item
  accounting ratios have inhomogeneous units;
\item
  synthetic attributes like sentiment also have their idiosyncrasies.
\end{itemize}

While it is widely considered that monotonic transformation of the
features have a marginal impact on prediction outcomes,
\citet{galili2016splitting} show that this is not always the case (see
also section \ref{impact-of-rescaling-toy-example}). Hence, the choice
of normalisation may in fact very well matter.

If we write \(x_i\) for the raw input and \(\tilde{x}_i\) for the
transformed data, common scaling practices include:

\begin{itemize}
\tightlist
\item
  \textbf{standardization}: \(\tilde{x}_i=(x_i-m_x)/\sigma_x\), where
  \(m_x\) and \(\sigma_x\) are the mean and standard deviation of \(x\),
  respectively;
\item
  \textbf{min-max} rescaling over {[}0,1{]}:
  \(\tilde{x}_i=(x_i-\min(\mathbf{x}))/(\max(\mathbf{x})-\min(\mathbf{x}))\);
\item
  \textbf{min-max} rescaling over {[}-1,1{]}:
  \(\tilde{x}_i=2\frac{x_i-\min(\mathbf{x})}{\max(\mathbf{x})-\min(\mathbf{x})}-1\);
\item
  \textbf{uniformization}: \(\tilde{x}_i=F_\mathbf{x}(x_i)\), where
  \(F_\mathbf{x}\) is the empirical c.d.f. of \(\mathbf{x}\). In this
  case, the vector \(\tilde{\mathbf{x}}\) is defined to follow a uniform
  distribution over {[}0,1{]}.
\end{itemize}

Sometimes, it is possible to apply a logarithmic transform of variables
with both large values (market capitalization) and large outliers. The
scaling can come after this transformation. Obviously, this technique is
prohibited for features with negative values.

It is often advised to scale inputs so that they range in {[}0,1{]}
before sending them through the training of neural networks for
instance. The dataset that we use in this book is based on variables
that have been uniformized. In factor investing, the scaling of features
must be \textbf{operated separately for each date and each feature}.
This point is critical. It makes sure that for every rebalancing date,
the predictors will have a similar shape and do carry information on the
cross-section of stocks.

Scaling features across dates should be proscribed. Take for example the
case of market capitalization. On the long run (market crashes
notwithstanding), this feature increases through time. Thus, scaling
across dates, would lead to small values at the beginning of the sample
and large values at the end of the sample. This would completely alter
and dilute the cross-sectional content of the features.

\hypertarget{labelling}{%
\section{Labelling}\label{labelling}}

\hypertarget{simple-labels}{%
\subsection{Simple labels}\label{simple-labels}}

There are several ways to define labels when constructing portfolio
policies. Of course, the finality is the portfolio weight, but it is
rarely considered as the best choice for the label.\footnote{Some
  methodologies do map firm attributes into final weights, e.g.,
  \citet{brandt2009parametric} and \citet{ammann2016characteristics} but
  these are outside the scope of the book.}

Usual labels in factor investing are the following:

\begin{itemize}
\tightlist
\item
  raw asset returns;\\
\item
  future relative returns (versus some benchmark: market-wide, or
  sector-based for instance);\\
\item
  the probability of positive return (or of return above a specified
  threshold);\\
\item
  the probability of outperforming a benchmark;\\
\item
  the binary version of the above: YES (outperforming) versus NO
  (underperforming);\\
\item
  risk-adjusted versions of the above: Sharpe ratios, information ratios
  (see Section \ref{perfmet}).
\end{itemize}

As we will discuss later in this chapter, these choices still leave room
for additional degrees of freedom. Should the labels be rescaled, just
like features are processed? What is the best time horizon on which to
compute performance metrics?

\hypertarget{categorical-labels}{%
\subsection{Categorical labels}\label{categorical-labels}}

In a typical ML analysis, when \(y\) is a proxy for future performance,
the ML engine will try to minimize some distance between the predicted
value and the realized values. For mathematical convenience, the sum of
squared error (\(L^2\) norm) is used because it has the simplest
derivative and makes gradient descent accessible and easy to compute.

Sometimes, it can be interesting not to focus on raw performance
proxies, like returns or Sharpe ratios, but on investment decisions -
which can be derived from these proxies. A simple example (decision
rule) is the following:

\[y_{t,i}=\left\{  \begin{array}{rll}
-1 & \text{ if } & \hat{r}_{t,i} < r_- \\
0 & \text{ if } & \hat{r}_{t,i} \in [r_,r_+] \\
+1 & \text{ if } & \hat{r}_{t,i} > r_+ \\
\end{array} \right.,\] where \(\hat{r}_{t,i}\) is the performance proxy
and \(r_\pm\) are the decision thresholds. When the predicted
performance is below \(r_-\), the decision is -1 (e.g., \emph{sell}),
when it is above \(r_+\), the decision is +1 (e.g., \emph{buy}) and when
it is in the middle (the model is neither very optimistic nor very
pessimistic), then the decision is neutral (e.g., \emph{hold}). The
performance proxy can of course be relative to some benchmark so that
the decision is directly related to this benchmark. The thresholds
\(r_\pm\) should be chosen such that the three categories are relatively
balanced, that is, have a comparable number of instances.

In this case, the final output can be both considered as categorical or
numerical because it belongs to an important subgroup of categorical
variables: the ordered categorical (\textbf{ordinal}) variables. If
\(y\) is taken as a number, then the usual regression tools apply. The
transformation of the initial output into a new format is similar to
what is performed when engineering features.

When \(y\) is treated as non-ordered (\textbf{nominal}) categorical
variable, then a new layer of processing is required because ML tools
only work with numbers. Hence, the categories must be recoded into
digits. The mapping that is most often used is called `\textbf{one-hot
encoding}'. The vector of classes is split in a sparse matrix in which
each column is dedicated to one class. The matrix is filled with zeros
and ones. A one is allocated to the column corresponding to the class of
the instance. We provide a simple illustration in the table below.

\begin{longtable}[]{@{}llll@{}}
\toprule
Initial data & & One-hot encoding &\tabularnewline
\midrule
\endhead
Position & Sell & Hold & Buy\tabularnewline
buy & 0 & 0 & 1\tabularnewline
buy & 0 & 0 & 1\tabularnewline
hold & 0 & 1 & 0\tabularnewline
sell & 1 & 0 & 0\tabularnewline
buy & 0 & 0 & 1\tabularnewline
\bottomrule
\end{longtable}

In classification tasks, the output has a larger dimension. For each
instance, it gives the probability of belonging to each class assigned
by the model. As we will see in Chapters \ref{trees} and \ref{NN}, this
is easily handled via the softmax function.

\hypertarget{the-triple-barrier-method}{%
\subsection{The triple barrier method}\label{the-triple-barrier-method}}

We conclude this section with an advanced labelling technique mentioned
in \citet{de2018advances}. The idea is to consider the full dynamics of
a trading strategy and not a simple performance proxy. The rationale for
this extension is that often money managers implement P\&L triggers that
cash in when gains are sufficient or opt out to stop their losses. Upon
inception of the strategy, three barriers are fixed (see Figure
\ref{fig:triplebarrier}):

\begin{itemize}
\tightlist
\item
  one above the current level of the asset (majenta line), which
  measures a reasonable expected profit;\\
\item
  one below the current level of the asset (cyan line), which acts as a
  stop-loss signal to prevent large negative returns;\\
\item
  and finally, one that fixes the horizon of the strategy after which it
  will be terminated (black line).
\end{itemize}

If the strategy hits the first (\emph{resp}. second) barrier, the output
is +1 (\emph{resp}. -1) and if it hits the last barrier, the output is
equal to zero or to some linear interpolation (between -1 and +1) that
represents the position of the terminal value relative to the two
horizontal barriers. Computationally, this method is \textbf{much} more
demanding as it evaluates a whole trajectory for each instance. It is
nonetheless considered as more realistic because trading strategies are
often accompanied with automatic triggers such as stop-loss, etc.

\begin{figure}[H]

{\centering \includegraphics{ML_factor_files/figure-latex/triplebarrier-1} 

}

\caption{ Illustration of the triple barrier method.}\label{fig:triplebarrier}
\end{figure}

\hypertarget{filtering-the-sample}{%
\subsection{Filtering the sample}\label{filtering-the-sample}}

One of the main challenges in Machine Learning is to extract as much
\textbf{signal} as possible. By signal, we mean patterns that will hold
out-of-sample. Intuitively, it may seem reasonable to think that the
more data we gather, the more signal we can extract. This is in fact
false in all generality because more data also means more noise.
Surprisingly, filtering the training samples can improve performance.
This idea was for example implemented successfully in
\citet{fu2018machine}, \citet{guida2019big} and
\citet{guida2018machine}.

In our paper \citet{coqueret2019training}, we investigate why smaller
samples may lead to superior out-of-sample accuracy for a particular
type of ML algorithm: decision trees (see Chapter \ref{trees}). We focus
on a particular kind of filter: we exclude the labels (i.e., returns)
that are not extreme and retain the 20\% values that are the smallest
and the 20\% that are the largest (the bulk of the distribution is
removed). In doing so, we alter the structure of trees in two ways:\\
- when the splitting points are altered, they are always closer to the
center of the distribution of the splitting variable (i.e., the
resulting clusters are more balanced);\\
- the choice of splitting variables is (sometimes) pushed towards the
features that have a monotonous impact on the label.\\
These two properties are desirable. The first reduces the risk of
fitting to small groups of instances that may be spurious. The second
gives more importance to features that appear globally more relevant in
explaning the returns. However, the filtering must not be too intense.
If, instead of retaining 20\% of each tail of the predictor, we keep
just 10\%, then the loss in signal becomes too severe and the
performance deteriorates.

\hypertarget{horizons}{%
\subsection{Return horizons}\label{horizons}}

This subsection deals with one of the least debated issue in
factor-based machine learning models.

\citet{jegadeesh1993returns} HORIZONS ??????? XXXX

\hypertarget{pers}{%
\section{Discussion on persistence}\label{pers}}

While we have separated the steps of feature engineering and labelling
in two different subsections, it is probably wiser to consider them
jointly. One important property of the dataset processed by the ML
algorithm should be the consistency of persistence between features and
labels. Intuitively, the autocorrelation patterns between the label
\(y_{t,n}\) (future performance) and the features \(x_{t,n}^{(k)}\)
should not be too distant.

One problematic example is when the dataset is sampled at the monthly
frequency (not unusual in the money management industry) with the labels
being monthly returns and the features being risk-based or fundamental
attributes. In this case, the label is very weakly autocorrelated, while
the features are often highly autocorrelated. In this situation, most
sophisticated forecasting tools will arbitrage between features which
will probably result in a lot of noise. In linear predictive models,
this configuration is known to generate bias in estimates (see the study
of \citet{stambaugh1999predictive} and the review by
\citet{gonzalo2018predictive}).

Among other more technical options, there are two simple solutions when
facing this issue. Either introduce autocorrelation into the label, or
remove it from the features. Both are rather easy econometrically:

\begin{itemize}
\tightlist
\item
  to increase the autocorrelation of the label, compute performance over
  longer time ranges. For instance, when working with monthly data,
  considering annual or biennial returns will do the trick.\\
\item
  to get rid of autocorrelation, the shortest route is to resort to
  difference/variations:
  \(\Delta x_{t,n}^{(k)}=x_{t,n}^{(k)}-x_{t-1,n}^{(k)}\). One advantage
  of this procedure is that it makes sense, economically: variations in
  features may be better drivers of performance, compared to raw levels.
\end{itemize}

The crucial choice is whether to work \emph{mostly} with persistent or
oscillating variables. A mix between the two in the feature space is
also possible, but both types should be well represented.

\hypertarget{extensions}{%
\section{Extensions}\label{extensions}}

\hypertarget{transforming-features}{%
\subsection{Transforming features}\label{transforming-features}}

The feature space can easily be augmented through simple operations. One
of them is lagging, that is, considering older values of features and
assuming some memory effect for their impact on the label. This is
naturally useful mostly if the features are oscillating (adding a layer
of memory on persistent features can be somewhat redudant). New
variables are defined by \(\breve{x}_{t,n}^{(k)}=x_{t-1,n}^{(k)}\).

In some cases (e.g., insufficient number of features), it is possible to
consider ratios or products between features. Accounting ratios like
price-to-book, book-to-market, debt-to-equity are examples of functions
of raw features that make sense. The gains brought by a larger spectrum
of features are not obvious. The risk of overfitting increases, just
like in a simple linear regression adding variables mechanically
increases the \(R^2\). The choices must make sense, economically.

Another way to increase the feature space (mentioned above) is to
consider variations. Variations in sentiment, variations in
book-to-market ratio, etc., can be relevant predictors because
sometimes, the change is more important than the level. In this case, a
new predictor is
\(\breve{x}_{t,n}^{(k)}=x_{t,n}^{(k)}-x_{t-1,n}^{(k)}\).

\hypertarget{macro-economic-variables}{%
\subsection{Macro-economic variables}\label{macro-economic-variables}}

Finally, we discuss a very important topic. The data should never be
seperated from the context it comes from (its environment). In classical
financial terms, this means that a particular model is likely to depend
on the overarching situation which is often proxied by macro-economic
indicators. One way to take this into account at the data level is
simply to multiply the feature by a exogenous indicator \(z_{t}\) and in
this case, the new predictor is
\(\breve{x}_{t,n}^{(k)}=z_t \times x_{t,n}^{(k)}\). This technique is
used by \citet{gu2018empirical} who use 8 economic indicators (plus the
original predictors (\(z_t=1\))). This increase the feature space
ninefold.

conditional feature engineering XXX regime dependent\\
XXXXXXX features released at different frequencies

\hypertarget{code-and-results}{%
\section{Code and results}\label{code-and-results}}

\hypertarget{impact-of-rescaling-graphical-representation}{%
\subsection{Impact of rescaling: graphical
representation}\label{impact-of-rescaling-graphical-representation}}

We start with a simple illustration of the different scaling methods. We
generate an arbitrary series and then rescale it. The series is not
random so that each time the code chunk is executed, the output remains
the same.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Length <-}\StringTok{ }\DecValTok{100}                                 \CommentTok{# Length of the sequence}
\NormalTok{x <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\KeywordTok{sin}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{Length))                       }\CommentTok{# Original data}
\NormalTok{data <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{index =} \DecValTok{1}\OperatorTok{:}\NormalTok{Length, }\DataTypeTok{x =}\NormalTok{ x)   }\CommentTok{# Data framed into dataframe}
\KeywordTok{ggplot}\NormalTok{(data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ index, }\DataTypeTok{y =}\NormalTok{ x)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat =} \StringTok{"identity"}\NormalTok{) }\CommentTok{# Plot}
\end{Highlighting}
\end{Shaded}

\includegraphics{ML_factor_files/figure-latex/scale_ex-1.pdf}

\normalsize

We define and plot the scaled variables below.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{norm_unif <-}\StringTok{  }\ControlFlowTok{function}\NormalTok{(v)\{  }\CommentTok{# This is a function that uniformalises a vector.}
\NormalTok{    v <-}\StringTok{ }\NormalTok{v }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{()}
    \KeywordTok{return}\NormalTok{(}\KeywordTok{ecdf}\NormalTok{(v)(v))}
\NormalTok{\}}

\NormalTok{norm_}\DecValTok{0}\NormalTok{_}\DecValTok{1}\NormalTok{ <-}\StringTok{  }\ControlFlowTok{function}\NormalTok{(v)\{  }\CommentTok{# This is a function that uniformalises a vector.}
    \KeywordTok{return}\NormalTok{((v}\OperatorTok{-}\KeywordTok{min}\NormalTok{(v))}\OperatorTok{/}\NormalTok{(}\KeywordTok{max}\NormalTok{(v)}\OperatorTok{-}\KeywordTok{min}\NormalTok{(v)))}
\NormalTok{\}}

\NormalTok{data_norm <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(                        }\CommentTok{# Formatting the data}
    \DataTypeTok{index =} \DecValTok{1}\OperatorTok{:}\NormalTok{Length,                           }\CommentTok{# Index of point/instance}
    \DataTypeTok{standard =}\NormalTok{ (x }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(x)) }\OperatorTok{/}\StringTok{ }\KeywordTok{sd}\NormalTok{(x),           }\CommentTok{# Standardisation}
    \DataTypeTok{norm_0_1 =} \KeywordTok{norm_0_1}\NormalTok{(x),                     }\CommentTok{# [0,1] reduction}
    \DataTypeTok{unif =} \KeywordTok{norm_unif}\NormalTok{(x)) }\OperatorTok{%>%}\StringTok{                    }\CommentTok{# Uniformisation}
\StringTok{    }\KeywordTok{gather}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ Type, }\DataTypeTok{value =}\NormalTok{ value, }\OperatorTok{-}\NormalTok{index)   }\CommentTok{# Putting in tidy format}
\KeywordTok{ggplot}\NormalTok{(data_norm, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ index, }\DataTypeTok{y =}\NormalTok{ value, }\DataTypeTok{fill =}\NormalTok{ Type)) }\OperatorTok{+}\StringTok{   }\CommentTok{# Plot!}
\StringTok{    }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat =} \StringTok{"identity"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{facet_grid}\NormalTok{(Type}\OperatorTok{~}\NormalTok{.)          }\CommentTok{# This option creates 3 concatenated graphs to ease comparison}
\end{Highlighting}
\end{Shaded}

\includegraphics{ML_factor_files/figure-latex/data_norm-1.pdf}

\normalsize

Finally, we look at the histogram of the newly created variables.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(data_norm, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ value, }\DataTypeTok{fill =}\NormalTok{ Type)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ML_factor_files/figure-latex/data_norm_dist-1.pdf}

\normalsize

With respect to shape, the green and red distributions are close to the
original one. It is only the support that changes: the min/max rescaling
ensures all values lie in the \([0,1]\) interval. In both cases, the
smallest values (on the left) display a spike in distribution. By
construction, this spike disappears under the uniformisation: the points
are evenly distributed over the unit interval.

\hypertarget{impact-of-rescaling-toy-example}{%
\subsection{Impact of rescaling: toy
example}\label{impact-of-rescaling-toy-example}}

To illustrate the impact of chosing one particular rescaling
method,\footnote{For a more thorough technical discussion on the impact
  of feature engineering, we refer to \citet{galili2016splitting}.} we
build a simple dataset, comprising 3 firms and 3 dates.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{firm <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{))             }\CommentTok{# Firms (3 lines for each)}
\NormalTok{date <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{),}\DecValTok{3}\NormalTok{)                             }\CommentTok{# Dates}
\NormalTok{cap <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{,                               }\CommentTok{# Market capitalisation}
         \DecValTok{15}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{15}\NormalTok{, }
         \DecValTok{200}\NormalTok{, }\DecValTok{120}\NormalTok{, }\DecValTok{80}\NormalTok{)}
\NormalTok{return <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.06}\NormalTok{, }\FloatTok{0.01}\NormalTok{, }\FloatTok{-0.06}\NormalTok{,                      }\CommentTok{# Return values}
            \FloatTok{-0.03}\NormalTok{, }\FloatTok{0.00}\NormalTok{, }\FloatTok{0.02}\NormalTok{,}
            \FloatTok{-0.04}\NormalTok{, }\FloatTok{-0.02}\NormalTok{,}\FloatTok{0.00}\NormalTok{)}
\NormalTok{data_toy <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(firm, date, cap, return)     }\CommentTok{# Aggregation of data}
\NormalTok{data_toy <-}\StringTok{ }\NormalTok{data_toy }\OperatorTok{%>%}\StringTok{                            }\CommentTok{# Transformation of data}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(date) }\OperatorTok{%>%}\StringTok{                            }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{cap_0_1 =} \KeywordTok{norm_0_1}\NormalTok{(cap), }\DataTypeTok{cap_u =} \KeywordTok{norm_unif}\NormalTok{(cap))}
\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(data_toy, }\DataTypeTok{digits =} \DecValTok{3}\NormalTok{,                  }\CommentTok{# Display the data}
             \DataTypeTok{caption =} \StringTok{"Sample data for a toy example."}\NormalTok{)   }
\end{Highlighting}
\end{Shaded}

\begin{table}[t]

\caption{\label{tab:fakedata}Sample data for a toy example.}
\centering
\begin{tabular}{r|r|r|r|r|r}
\hline
firm & date & cap & return & cap\_0\_1 & cap\_u\\
\hline
1 & 1 & 10 & 0.06 & 0.000 & 0.333\\
\hline
1 & 2 & 50 & 0.01 & 0.364 & 0.667\\
\hline
1 & 3 & 100 & -0.06 & 1.000 & 1.000\\
\hline
2 & 1 & 15 & -0.03 & 0.026 & 0.667\\
\hline
2 & 2 & 10 & 0.00 & 0.000 & 0.333\\
\hline
2 & 3 & 15 & 0.02 & 0.000 & 0.333\\
\hline
3 & 1 & 200 & -0.04 & 1.000 & 1.000\\
\hline
3 & 2 & 120 & -0.02 & 1.000 & 1.000\\
\hline
3 & 3 & 80 & 0.00 & 0.765 & 0.667\\
\hline
\end{tabular}
\end{table}

\normalsize

Let's briefly comment on this synthetic data. We assume that dates are
ordered chronologically and far away: each date stands for a year or the
beginning of a decade, but the (forward) returns are computed on a
monthly basis. The first firm is hugely successful and multiplies its
cap ten times over the periods. The second firms remains stable
cap-wise, while the third one plummets. If we look at `local' future
returns, they are strongly negatively related to size for the first and
third firms. For the second one, there is no clear pattern.

Date-by-date, the analysis is fairly similar, though slightly nuanced.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  On date 1, the smallest firm has the largest return and the two others
  have negative returns.\\
\item
  On date 2, the biggest firm has a negative return while the two
  smaller firms do not.\\
\item
  On date 3, returns are decreasing with size.
\end{enumerate}

While the relationship is not always perfectly monotonous, there seems
to be a link between size and return and typically, investing in the
smallest firm would be a very good strategy with this sample.

Now let us look at the output of simple regressions.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lm}\NormalTok{(return }\OperatorTok{~}\StringTok{ }\NormalTok{cap_}\DecValTok{0}\NormalTok{_}\DecValTok{1}\NormalTok{, }\DataTypeTok{data =}\NormalTok{ data_toy) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# First regression (min-max rescaling)}
\StringTok{    }\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{'Regression output when the independent var. comes }
\StringTok{                 from min-max rescaling'}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{table}[t]

\caption{\label{tab:datatoyreg}Regression output when the independent var. comes 
                 from min-max rescaling}
\centering
\begin{tabular}{l|r|r|r|r}
\hline
term & estimate & std.error & statistic & p.value\\
\hline
(Intercept) & 0.0162778 & 0.0137351 & 1.185121 & 0.2746390\\
\hline
cap\_0\_1 & -0.0497032 & 0.0213706 & -2.325777 & 0.0529421\\
\hline
\end{tabular}
\end{table}

\normalsize

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lm}\NormalTok{(return }\OperatorTok{~}\StringTok{ }\NormalTok{cap_u, }\DataTypeTok{data =}\NormalTok{ data_toy) }\OperatorTok{%>%}\StringTok{   }\CommentTok{# Second regression (uniformised feature)}
\StringTok{    }\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{() }\OperatorTok{%>%}\StringTok{   }
\StringTok{    }\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\DataTypeTok{caption =} \StringTok{'Regression output when the independent var. comes from uniformisation'}\NormalTok{)   }
\end{Highlighting}
\end{Shaded}

\begin{table}[t]

\caption{\label{tab:datatoyreg2}Regression output when the independent var. comes from uniformisation}
\centering
\begin{tabular}{l|r|r|r|r}
\hline
term & estimate & std.error & statistic & p.value\\
\hline
(Intercept) & 0.06 & 0.0198139 & 3.028170 & 0.0191640\\
\hline
cap\_u & -0.10 & 0.0275162 & -3.634219 & 0.0083509\\
\hline
\end{tabular}
\end{table}

\normalsize In terms of \emph{p}-\textbf{value} (last column), the first
estimation for the cap coefficient is above 5\% (in Table
\ref{tab:datatoyreg}) while the second is below 1\% (in Table
\ref{tab:datatoyreg2}). One possible explanation for this discrepancy is
the standard deviation of the variables. The deviations are equal to
0.47 and 0.29 for cap\_0 and cap\_u, respectively. Values like market
capitalisations can have very large ranges and are thus subject to
substantial deviations (even after scaling). Working with uniformised
variables reduces dispersion and can help solve this problem.

Note that this is a \textbf{double-edged sword}: while it can help avoid
\textbf{false negatives}, it can also lead to \textbf{false positives}.

\hypertarget{coding-exercises-1}{%
\section{Coding exercises}\label{coding-exercises-1}}

\hypertarget{lasso}{%
\chapter{Penalized regressions and sparse hedging for minimum variance
portfolios}\label{lasso}}

In this chapter, we introduce the widespread concept of regularisation
for linear models. There are in fact several possible applications for
these models. The first one is straightforward: resort to penalizations
to improve the robustness of factor-based predictive regressions. The
outcome can then be used to fuel an allocation scheme. For instance,
\citet{han2018firm} and \citet{rapach2019time} use penalized regressions
to improve stock return prediction when combining forecasts that emanate
from individual characteristics.

Similar ideas can be devloped for macroeconomic predictions for
instance, as in \citet{uematsu2019high}. The second application stems
from a lesser known results which originates from
\citet{stevens1998inverse}. It links the weights of optimal
mean-variance portfolios to particular cross-sectional regressions. The
idea is then different and the purpose is to improve the quality of
mean-variance driven portfolio weights. We present the two approach
below after an introduction on regularization techniques for linear
models.

Other examples of financial applications of penalization can be found in
\citet{d2011identifying}, \citet{ban2016machine} and
\citet{kremer2019sparse}. In any case, the idea is the same as in the
seminal paper \citet{tibshirani1996regression}: standard (unconstrained)
optimization programs may lead to noisy estimates, thus adding a
structuring constraint helps remove some noise (at the cost of a
possible bias). For instance, \citet{kremer2019sparse} use this concept
to build more robust mean-variance (\citet{markowitz1952portfolio})
portfolios.

\hypertarget{penalised-regressions}{%
\section{Penalised regressions}\label{penalised-regressions}}

\hypertarget{simple-regressions}{%
\subsection{Simple regressions}\label{simple-regressions}}

The ideas behind linear models are at least two centuries old
(\citet{legendre1805nouvelles} is an early reference on least squares
optimization). Given a matrix of predictors \(\textbf{X}\), we seek to
decompose the output vector \(\textbf{y}\) as a linear function of the
columns of \(\textbf{X}\) (written \(\textbf{X}\boldsymbol{\beta}\))
plus an error term \(\boldsymbol{\epsilon}\):
\(\textbf{y}=\textbf{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}\).

The best choice of \(\boldsymbol{\beta}\) is naturally the one that
minimizes the error. For analytical tractability, it is the sum of
squared error that is minimized:
\(L=\boldsymbol{\epsilon}'\boldsymbol{\epsilon}\). The loss \(L\) is
called the sum of squared residuals (SSR). In order to find the optimal
\(\boldsymbol{\beta}\), it is imperative to differentiate this loss
\(L\) with respect to \(\boldsymbol{\beta}\) because the first order
condition requires that the gradient be equal to zero: \begin{align*}
\nabla_{\boldsymbol{\beta}} L&=\frac{\partial}{\partial \boldsymbol{\beta}}(\textbf{y}-\textbf{X}\boldsymbol{\beta})'(\textbf{y}-\textbf{X}\boldsymbol{\beta})=\frac{\partial}{\partial \boldsymbol{\beta}}\boldsymbol{\beta}'\textbf{X}'\textbf{X}\boldsymbol{\beta}-2\textbf{y}'\textbf{X}\boldsymbol{\beta} \\
&=2\textbf{X}'\textbf{X}\boldsymbol{\beta}  -2\textbf{X}'\textbf{y}
\end{align*} so that the first order condition
\(\nabla_{\boldsymbol{\beta}}=\textbf{0}\) is satisfied if
\begin{equation}
\label{eq:regbeta}
\boldsymbol{\beta}^*=(\textbf{X}'\textbf{X})^{-1}\textbf{X}'\textbf{y},
\end{equation} which is known as the standard \textbf{ordinary least
squares} (OLS) solution of the linear model. If the matrix
\(\textbf{X}\) has dimensions \(I \times K\), then the
\(\textbf{X}'\textbf{X}\) can only be inverted if the number of rows
\(I\) is strictly superior to the number of columns \(K\). In some
cases, that may not hold: there are more predictors than instances and
there is no unique value of \(\boldsymbol{\beta}\) that minimizes the
loss. If \(\textbf{X}'\textbf{X}\) is nonsingular (or positive
definite), then the second order condition ensures that
\(\boldsymbol{\beta}^*\) yields a global minimum for the loss \(L\) (the
second order derivative of \(L\) with respect to \(\boldsymbol{\beta}\)
is \(\textbf{X}'\textbf{X}\)).

Up to now, we have made no distributional assumption on any of the above
quantities. Standard assumptions are the following:\\
- \(\mathbb{E}[\textbf{y}|\textbf{X}]=\textbf{X}\boldsymbol{\beta}\):
\textbf{linear shape for the regression function};\\
- \(\mathbb{E}[\boldsymbol{\epsilon}|\textbf{X}]=\textbf{0}\): errors
are \textbf{independent of predictors};\\
-
\(\mathbb{E}[\boldsymbol{\epsilon}\boldsymbol{\epsilon}'| \textbf{X}]=\sigma^2\textbf{I}\):
\textbf{homoscedasticity}: errors are uncorrelated and have identical
variance;\\
- the \(\epsilon_i\) are normally distributed.

Under these hypotheses, it is possible to perform statistical tests
related to the \(\hat{\boldsymbol{\beta}}\) coefficients. We refer to
Chapters 2 to 4 in \citet{greene2018econometric} for a thorough
treatment on linear models as well as to Chapter 5 of the same book for
details on the corresponding tests.

\hypertarget{forms-of-penalizations}{%
\subsection{Forms of penalizations}\label{forms-of-penalizations}}

Penalised regressions have been popularised since the seminal work of
\citet{tibshirani1996regression}. The idea is to impose a constraint on
the coefficients of the regression, namely that their total magnitude be
restrained. In his original paper, \citet{tibshirani1996regression}
proposes to estimate the following model (LASSO): \begin{equation}
\label{eq:lasso1}
y_i = \sum_{j=1}^J \beta_jx_{i,j} + \epsilon_i, \quad i =1,\dots,I, \quad \text{s.t.} \quad \sum_{j=1}^J |\beta_j| < \delta, 
\end{equation} for some strictly positive constant \(\delta\). Under
least square minimisation, this amounts to solve the Lagrangian
formulation: \begin{equation}
\label{eq:lasso2}
\underset{\mathbf{\beta}}{\min} \, \left\{ \sum_{i=1}^I\left(y_i - \sum_{j=1}^J \beta_jx_{i,j} \right)^2+\lambda \sum_{j=1}^J |\beta_j| \right\},
\end{equation} for some value \(\lambda>0\). This specification seems
close to the ridge regression (\(L^2\) regularisation), which is in fact
anterior to the Lasso: \begin{equation}
\label{eq:ridge}
\underset{\mathbf{\beta}}{\min} \, \left\{ \sum_{i=1}^I\left(y_i - \sum_{j=1}^J\beta_jx_{i,j} \right)^2+\lambda \sum_{j=1}^J \beta_j^2 \right\},
\end{equation} which is equivalent to estimating the following model
\begin{equation}
\label{eq:ridge6}
y_i = \sum_{j=1}^J \beta_jx_{i,j} + \epsilon_i, \quad i =1,\dots,I, \quad \text{s.t.} \quad \sum_{j=1}^J \beta_j^2 < \delta, 
\end{equation} but the outcome is in fact quite different, which
justifies a separate treatment. Mechanically, as \(\lambda\) increases
(or as \(\delta\) in \eqref{eq:ridge6} \emph{decreases}), the coefficients
of the ridge regression all slowly decrease in magnitude towards zero.
In the case of the LASSO, the convergence is somewhat more brutal as
some coefficients shrink to zero very quickly. For \(\lambda\)
sufficiently large, only one coefficient will remain nonzero, while in
the ridge regression, the zero value is only reached asymptotically for
all coefficients.

To depict the difference between the Lasso and the ridge regression, let
us consider the case of \(K=2\) predictors which is shown in Figure
\ref{fig:lassoridge}. The optimal unconstrained solution
\(\boldsymbol{\beta}^*\) is pictured in red in the middle of the space.
The problem is naturally that it does not satisfy the imposed
conditions. These constraints are shown in light grey: they take the
shape of a square \(|\beta_1|+|\beta_2| \le \delta\) in the case of the
Lasso and a circle \(\beta_1^2+\beta_2^2 \le \delta\) for the ridge
regression. In order to satisfy these constraints, the optimization need
to look in the vicinity of \(\boldsymbol{\beta}^*\) by allowing for
larger error levels. These error levels are shown as orange ellipsoids
in the figure. When the requirement on the error is loose enough, one
ellipsoid touches the acceptable boundary (in grey) and this is where
the constrained solution is located.

\begin{figure}[H]

{\centering \includegraphics[width=800px]{images/lassoridge} 

}

\caption{Schematic view of Lasso versus ridge regressions.}\label{fig:lassoridge}
\end{figure}

Both methods work when the number of exogenous variables surpasses that
of observations, i.e., in the case where classical regressions are
ill-defined. This is easy to see in the case of the ridge regression for
which the OLS solution is simply
\[\hat{\boldsymbol{\beta}}=(\mathbf{X}'\mathbf{X}+\lambda \mathbf{I}_N)^{-1}\mathbf{X}'\mathbf{Y}.\]
The additional term \(\lambda \mathbf{I}_N\) compared to Equation
\eqref{eq:regbeta} ensures that the inverse matrix is well-defined
whenever \(\lambda>0\). As \(\lambda\) increases, the magnitudes of the
\(\hat{\beta}_i\) decrease, which explains why penalizations are
sometimes referred to as \textbf{shrinkage} methods.

\citet{zou2005regularization} propose to benefit from the best of both
worlds when combining both penalisations in a convex manner
(elasticnet): \begin{equation}
\label{eq:elasticnet}
y_i = \sum_{j=1}^J \beta_jx_{i,j} + \epsilon_i, \quad \text{s.t.} \quad (1-\alpha)\sum_{j=1}^J |\beta_j| +\alpha\sum_{j=1}^J \beta_j^2< \delta, \quad i =1,\dots,N,
\end{equation} which is the solved as \begin{equation}
\label{eq:elastic}
\underset{\mathbf{\beta}}{\min} \, \left\{ \sum_{i=1}^I\left(y_i - \sum_{j=1}^J\beta_jx_{i,j} \right)^2+\lambda \left(\alpha\sum_{j=1}^J |\beta_j|+ (1-\alpha)\sum_{j=1}^J \beta_j^2\right) \right\},
\end{equation}

The main advantage of the LASSO compared to the ridge regression is its
selection capability. Indeed, given a very large number of variables (or
predictors), the LASSO will progressively rule-out those that are the
least relevant. The elasticnet preserves this selection ability and
\citet{zou2005regularization} argue that in some cases, it is even more
effective that the LASSO. The parameter \(\alpha \in [0,1]\) tunes the
smoothness of convergence (of the coefficients) towards zero. The closer
\(\alpha\) is to zero the smoother the convergence.

\hypertarget{illustrations}{%
\subsection{Illustrations}\label{illustrations}}

We start with simple illustrations of penalized regressions. We start
with the LASSO. The original implementation by the authors is in R,
which is practical. The syntax is slightly different, compared to usual
linear models. The illustrations are run on the whole dataset. First, we
estimate the coefficients. By default, the function chooses a large
array of penalization values so that the results for different
intensities can be shown immediately.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(glmnet)}
\NormalTok{y_penalized <-}\StringTok{ }\NormalTok{data_ml}\OperatorTok{$}\NormalTok{R1M_Usd                              }\CommentTok{# Dependent variable}
\NormalTok{x_penalized <-}\StringTok{ }\NormalTok{data_ml }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(features) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{() }\CommentTok{# Predictors}
\NormalTok{fit_lasso <-}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(x_penalized, y_penalized, }\DataTypeTok{alpha =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\normalsize

Once the coefficients are computed, they require some wrangling before
plotting.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lasso_res <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(fit_lasso}\OperatorTok{$}\NormalTok{beta)                      }\CommentTok{# Extract LASSO coefs}
\NormalTok{lambda <-}\StringTok{ }\NormalTok{fit_lasso}\OperatorTok{$}\NormalTok{lambda                                }\CommentTok{# Values of the penalisation constant}
\NormalTok{lasso_res}\OperatorTok{$}\NormalTok{Lambda <-}\StringTok{ }\NormalTok{lambda[lasso_res}\OperatorTok{$}\NormalTok{j]                   }\CommentTok{# Put the labels where they belong}
\NormalTok{lasso_res}\OperatorTok{$}\NormalTok{Feat <-}\StringTok{ }\NormalTok{features[lasso_res}\OperatorTok{$}\NormalTok{i] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.factor}\NormalTok{()   }\CommentTok{# Add names of variables to the output}
\NormalTok{lasso_res[}\DecValTok{1}\OperatorTok{:}\DecValTok{120}\NormalTok{,] }\OperatorTok{%>%}\StringTok{                                     }\CommentTok{# Take the first 150 estimates}
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Lambda, }\DataTypeTok{y =}\NormalTok{ x, }\DataTypeTok{color =}\NormalTok{ Feat)) }\OperatorTok{+}\StringTok{        }\CommentTok{# Plot!}
\StringTok{    }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{coord_fixed}\NormalTok{(}\FloatTok{0.25}\NormalTok{) }\OperatorTok{+}\StringTok{                     }\CommentTok{# Change aspect ratio of graph}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{8}\NormalTok{))           }\CommentTok{# Reduce legend font size}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
\includegraphics[width=400px]{ML_factor_files/figure-latex/lassoresults-1} \caption{LASSO model. The dependent variable is the 1 month ahead return.}\label{fig:lassoresults}
\end{figure}

\normalsize

The graph plots the evolution of coefficients as the penalization
intensity, \(\lambda\), increases. For some characteristics, like
Ebit\_Ta (in orange), the convergence to zero is rapid. Other variables
resist the penalization longer, like Mkt\_Cap\_3M\_Usd, which is the
last one to vanish. Essentially, this means that at the first order,
this variable is an important driver of future 1 month returns - in our
sample. Moreover, the negative sign of its coefficient are a
confirmation (again, in this sample) of the size anomaly, according to
which small firms experience higher future returns compared to their
larger counterparts.

Next, we turn to ridge regressions.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit_ridge <-}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(x_penalized, y_penalized, }\DataTypeTok{alpha =} \DecValTok{0}\NormalTok{)}
\NormalTok{ridge_res <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(fit_ridge}\OperatorTok{$}\NormalTok{beta)                                 }\CommentTok{# Extract ridge coefs}
\NormalTok{lambda <-}\StringTok{ }\NormalTok{fit_ridge}\OperatorTok{$}\NormalTok{lambda                                           }\CommentTok{# Values of penalisation constant}
\NormalTok{ridge_res}\OperatorTok{$}\NormalTok{Feat <-}\StringTok{ }\NormalTok{features[ridge_res}\OperatorTok{$}\NormalTok{i] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.factor}\NormalTok{()}
\NormalTok{ridge_res}\OperatorTok{$}\NormalTok{Lambda <-}\StringTok{ }\NormalTok{lambda[ridge_res}\OperatorTok{$}\NormalTok{j]                              }\CommentTok{# Put labels where they belong}
\NormalTok{ridge_res }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{filter}\NormalTok{(Feat }\OperatorTok{%in%}\StringTok{ }\KeywordTok{levels}\NormalTok{(}\KeywordTok{droplevels}\NormalTok{(lasso_res}\OperatorTok{$}\NormalTok{Feat[}\DecValTok{1}\OperatorTok{:}\DecValTok{120}\NormalTok{]))) }\OperatorTok{%>%}\StringTok{  }\CommentTok{# Keep the same features as above}
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Lambda, }\DataTypeTok{y =}\NormalTok{ x, }\DataTypeTok{color =}\NormalTok{ Feat)) }\OperatorTok{+}\StringTok{                   }\CommentTok{# Plot!}
\StringTok{    }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{scale_x_log10}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{coord_fixed}\NormalTok{(}\DecValTok{45}\NormalTok{) }\OperatorTok{+}\StringTok{                }\CommentTok{# Aspect ratio of + logscale}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.text =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{8}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{ML_factor_files/figure-latex/sparseridge-1.pdf}
\caption{\label{fig:sparseridge}Ridge regression. The dependent variable is
the 1 month ahead return.}
\end{figure}

\normalsize

In the above graph, the convergence to zero is much smoother. We
underline that the x-axis (penalization intensities) have a log-scale.
This allows to see the early patterns (close to zero, to the left) more
clearly. As in the previous figure, the Mkt\_Cap\_3M\_Usd predictor
clearly dominates, with again large negative coefficients. Nonetheless,
as \(\lambda\) increases, its domination over the other predictor fades.

By definition, the elasticnet will produce curves that behave like a
blend of the two above approaches. Nonetheless, as long as
\(\alpha >0\), the selective property of the LASSO will be preserved:
some features will see their coefficients shrink rapidly to zero. In
fact, the strength of the LASSO is such that a balanced mix of the two
penalization is not reached at \(\alpha = 1/2\), but rather at a much
smaller value (possibly below 0.1).

\hypertarget{sparse-hedging-for-minimum-variance-portfolios}{%
\section{Sparse hedging for minimum variance
portfolios}\label{sparse-hedging-for-minimum-variance-portfolios}}

\hypertarget{presentation-and-derivations}{%
\subsection{Presentation and
derivations}\label{presentation-and-derivations}}

The idea of constructing sparse portfolios is not new per se (see, e.g.,
\citet{brodie2009sparse}, \citet{fastrich2015constructing}) and the link
with the selective property of the LASSO is rather straightforward in
classical quadratic programs. Note that the choice of the \(L^1\) norm
is imperative because when enforcing a simple \(L^2\) norm, the
diversification of the portfolio increases (see
\citet{coqueret2015diversified}).

The idea behind this section stems from \citet{goto2015improving} but
the cornerstone result was first published by \citet{stevens1998inverse}
and we present it below. We provide details because the derivations are
not commonplace in the literature.

In usual mean-variance allocations, one core ingredient is the inverse
covariance matrix of assets \(\mathbf{\Sigma}^{-1}\). For instance, the
maximum Sharpe ratio portfolio is given by

\begin{equation}
\label{eq:MSR}
\mathbf{w}^{\text{MSR}}=\frac{\mathbf{\Sigma}^{-1}\boldsymbol{\mu}}{\mathbf{1}'\mathbf{\Sigma}^{-1}\boldsymbol{\mu}},
\end{equation} where \(\mathbf{\mu}\) is the vector of expected (excess)
returns. Taking \(\mathbf{\mu}=\mathbf{1}\) yields the minimum variance
portfolio, which is agnostic in terms of the first moment of expected
returns (and, as such, usually more robust than most alternatives).

If we decompose the matrix \(\mathbf{\Sigma}\) into:
\[\mathbf{\Sigma}= \left[\begin{array}{cc} \sigma^2 & \mathbf{c}' \\
\mathbf{c}& \mathbf{C}\end{array} \right],\] classical partitioning
results (e.g., Schur complements) imply
\[\small \mathbf{\Sigma}^{-1}= \left[\begin{array}{cc} (\sigma^2 -\mathbf{c}'\mathbf{C}^{-1}\mathbf{c})^{-1} & - (\sigma^2 -\mathbf{c}'\mathbf{C}^{-1}\mathbf{c})^{-1}\mathbf{c}'\mathbf{C}^{-1} \\
- (\sigma^2 -\mathbf{c}'\mathbf{C}^{-1}\mathbf{c})^{-1}\mathbf{C}^{-1}\mathbf{c}& \mathbf{C}^{-1}+ (\sigma^2 -\mathbf{c}'\mathbf{C}^{-1}\mathbf{c})^{-1}\mathbf{C}^{-1}\mathbf{cc}'\mathbf{C}^{-1}\end{array} \right].\]
We are interested in the first line, which has 2 components: the factor
\((\sigma^2 -\mathbf{c}'\mathbf{C}^{-1}\mathbf{c})^{-1}\) and the line
vector \(\mathbf{c}'\mathbf{C}^{-1}\). \(\mathbf{C}\) is the covariance
matrix of assets \(2\) to \(N\) and \(\mathbf{c}\) is the covariance
between the first asset and all other assets. The first line of
\(\mathbf{\Sigma}^{-1}\) is \begin{equation}
\label{eq:sparse1}
(\sigma^2 -\mathbf{c}'\mathbf{C}^{-1}\mathbf{c})^{-1} \left[1 \quad  \underbrace{-\mathbf{c}'\mathbf{C}^{-1}}_{N-1 \text{ terms}} \right]. 
\end{equation}

We now consider an alternative setting. We regress the returns of the
first asset on those of all other assets: \begin{equation}
\label{eq:sparseeq}
r_{1,t}=a_1+\sum_{n=2}^N\beta_{1|n}r_{n,t}+\epsilon_t, \quad \text{ i.e., } \quad  \mathbf{r}_1=a_1\mathbf{1}_T+\mathbf{R}_{-1}\mathbf{\beta}_1+\epsilon_1,
\end{equation} where \(\mathbf{R}_{-1}\) gathers the returns of all
assets except the first one. The OLS estimator for \(\mathbf{\beta}_1\)
is \begin{equation}
\label{eq:sparse2}
\hat{\mathbf{\beta}}_{1}=\mathbf{C}^{-1}\mathbf{c},
\end{equation}

and this is the partitioned formed (when a constant is included to the
regression) stemming from the Frisch-Waugh-Lovell theorem (see chapter 3
in \citet{greene2018econometric}).

In addition, \begin{equation}
\label{eq:sparse3}
(1-R^2)\sigma_{\mathbf{r}_1}^2=\sigma_{\mathbf{r}_1}^2- \mathbf{c}'\mathbf{C}^{-1}\mathbf{c} =\sigma^2_{\epsilon_1}.
\end{equation} The proof of this last fact is given below.

With \(\mathbf{X}\) being the concatenation of \(\mathbf{1}_T\) with
returns \(\mathbf{R}_{-1}\) and with \(\mathbf{y}=\mathbf{r}_1\), the
classical expression of the \(R^2\) is
\[R^2=1-\frac{\mathbf{\epsilon}'\mathbf{\epsilon}}{T\sigma_Y^2}=1-\frac{\mathbf{y}'\mathbf{y}-\hat{\mathbf{\beta}'}\mathbf{X}'\mathbf{X}\hat{\mathbf{\beta}}}{T\sigma_Y^2}=1-\frac{\mathbf{y}'\mathbf{y}-\mathbf{y}'\mathbf{X}\hat{\mathbf{\beta}}}{T\sigma_Y^2},\]
with fitted values
\(\mathbf{X}\hat{\mathbf{\beta}}= \hat{a_1}\mathbf{1}_T+\mathbf{R}_{-1}\mathbf{C}^{-1}\mathbf{c}\).
Hence, \begin{align*}
T\sigma_{\mathbf{r}_1}^2R^2&=T\sigma_{\mathbf{r}_1}^2-\mathbf{r}'_1\mathbf{r}_1+\hat{a_1}\mathbf{1}'_T\mathbf{r}_1+\mathbf{r}'_1\mathbf{R}_{-1}\mathbf{C}^{-1}\mathbf{c} \\
T(1-R^2)\sigma_{\mathbf{r}_1}^2&=\mathbf{r}'_1\mathbf{r}_1-\hat{a_1}\mathbf{1}'_T\mathbf{r}_1-\left(\mathbf{\tilde{r}}_1+\frac{\mathbf{1}_T\mathbf{1}'_T}{T}\mathbf{r}_1\right)'\left(\tilde{\mathbf{R}}_{-1}+\frac{\mathbf{1}_T\mathbf{1}'_T}{T}\mathbf{R}_{-1}\right)\mathbf{C}^{-1}\mathbf{c} \\
T(1-R^2)\sigma_{\mathbf{r}_1}^2&=\mathbf{r}'_1\mathbf{r}_1-\hat{a_1}\mathbf{1}'_T\mathbf{r}_1-T\mathbf{c}'\mathbf{C}^{-1}\mathbf{c} -\mathbf{r}'_1\frac{\mathbf{1}_T\mathbf{1}'_T}{T}\mathbf{R}_{-1} \mathbf{C}^{-1}\mathbf{c} \\
T(1-R^2)\sigma_{\mathbf{r}_1}^2&=\mathbf{r}'_1\mathbf{r}_1-\frac{(\mathbf{1}'_T\mathbf{r}_1)^2}{T}- T\mathbf{c}'\mathbf{C}^{-1}\mathbf{c} \\
(1-R^2)\sigma_{\mathbf{r}_1}^2&=\sigma_{\mathbf{r}_1}^2- \mathbf{c}'\mathbf{C}^{-1}\mathbf{c} 
\end{align*} where in the fourth equality we have plugged
\(\hat{a_1}=\frac{\mathbf{1'}_T}{T}(\mathbf{r}_1-\mathbf{R}_{-1}\mathbf{C}^{-1}\mathbf{c})\).
Note: there is probably a simpler proof - see e.g.~Section 3.5 in
\citet{greene2018econometric}.

Combining (\ref{eq:sparse1}), (\ref{eq:sparse2}) and (\ref{eq:sparse3}),
we get that the first line of \(\mathbf{\Sigma}^{-1}\) is equal to
\begin{equation}
\label{eq:sparsehedgeeq2}
\frac{1}{\sigma^2_{\epsilon_1}}\times \left[ 1 \quad  -\hat{\boldsymbol{\beta}}_1'\right].
\end{equation}

Given the first line of \(\mathbf{\Sigma}^{-1}\), it suffices to
multiply by \(\boldsymbol{\mu}\) to get the portfolio weight in the
first asset (up to a scaling constant).

There is a nice economic intuition behind the above results which
justifies the term ``sparse hedging''. We take the case of the minimum
variance portfolio, for which \(\boldsymbol{\mu}=\boldsymbol{1}\). In
Equation \eqref{eq:sparseeq}, we try to explain the return of asset 1 with
that of all other assets. In the above equation, up to a scaling
constant, the portfolio has a unit position in the first asset and
\(-\hat{\boldsymbol{\beta}}_1\) positions in all other assets. Hence,
the purpose of all other assets is clearly to hedge the return of the
first one. In fact, these positions are aimed at minimizing the squared
errors of the aggregate portfolio for asset one (these errors a exactly
\(\mathbf{\epsilon}_1\)). Moreover, the scaling factor
\(\sigma^{-2}_{\epsilon_1}\) is also simple to interpret: the more we
trust the regression output (because of a small
\(\sigma^{2}_{\epsilon_1}\)), the more we invest in the hedging
portfolio of the asset.

This reasoning is easily generalized for any line of
\(\mathbf{\Sigma}^{-1}\), which can be obtained by regressing the
returns of asset \(i\) on the returns of all other assets. If the
allocation scheme has the form (\ref{eq:MSR}) for given values of
\(\boldsymbol{\mu}\), then the pseudo-code for the sparse portfolio
strategy is the following.

At each date (which we omit for notational convenience),

\begin{itemize}
\item For all stocks $i$,
\begin{enumerate}
\item estimate the elastic-net regression over the $t=1,\dots,T$ samples to get the $i^{th}$ line of $\hat{\mathbf{\Sigma}}^{-1}$:
\begin{align*} \small \left[\hat{\mathbf{\Sigma}}^{-1}\right]_{i,\cdot}= \underset{\mathbf{\beta}_{i|}}{\text{argmin}}\, \left\{\sum_{t=1}^T\left( r_{i,t}-a_i+\sum_{n\neq i}^N\beta_{i|n}r_{n,t}\right)^2+\lambda \alpha ||  \mathbf{\beta}_{i|}||_1+\lambda (1-\alpha)||\mathbf{\beta}_{i|}||_2^2\right\}
\end{align*}
\item to get the weights of asset $i$, we compute the $\mathbf{\mu}$-weighted sum: $w_i= \sigma_{\epsilon_i}^{-2}\left(\mu_i- \sum_{j\neq i}\mathbf{\beta}_{i|j}\mu_j\right)$
\end{enumerate}
\end{itemize}

The introduction of the penalization norms is the new ingredient,
compared to the original approach of \citet{stevens1998inverse}. The
benefits are twofold: first, introducing constraints yields weights that
are more robust and less subject to errors in the estimates of
\(\mathbf{\mu}\); second, because of sparsity, weights are more stable,
less leveraged and thus the strategy is less impacted by transaction
costs. Before we turn to numerical applications, we mention a more
direct route to the estimation of a robust inverse covariance matrix:
the Graphical LASSO. We refer to the original article
\citet{friedman2008sparse} for more details on this subject.

\hypertarget{sparseex}{%
\subsection{Example}\label{sparseex}}

The interest of sparse hedging portfolios is to proposed a robust
approach to the estimation of minimum variance policies. Indeed, since
the vector of expected returns \(\boldsymbol{\mu}\) is usually very
noisy, a simple solution is to adopt an agnostic view by setting
\(\boldsymbol{\mu}=\boldsymbol{1}\). In order to test the added value of
the sparsity constraint, we must resort to a full backtest. In doing so,
we anticipate the content of chapter \ref{backtest}.

We first prepare the variables. Sparse portfolios are based on returns
only; we thus base our analysis on the dedicated variable in
matrix/rectangular format (\emph{returns}) which were created at the end
of Chapter \ref{notdata}.

Then, we initialize the output variables: portfolio weights and
portfolio returns. We want to compare three strategies: an equally
weighted (EW) benchmark of all stocks, the classical global minimum
variance portfolio (GMV) and the sparse-hedging approach to minimum
variance.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t_oos <-}\StringTok{ }\NormalTok{returns}\OperatorTok{$}\NormalTok{date[returns}\OperatorTok{$}\NormalTok{date }\OperatorTok{>}\StringTok{ }\NormalTok{separation_date] }\OperatorTok{%>%}\StringTok{            }\CommentTok{# Out-of-sample dates }
\StringTok{    }\KeywordTok{unique}\NormalTok{() }\OperatorTok{%>%}\StringTok{                                                     }\CommentTok{# Remove duplicates}
\StringTok{    }\KeywordTok{as.Date}\NormalTok{(}\DataTypeTok{origin =} \StringTok{"1970-01-01"}\NormalTok{)                                   }\CommentTok{# Transform in date format}
\NormalTok{Tt <-}\StringTok{ }\KeywordTok{length}\NormalTok{(t_oos)                                                  }\CommentTok{# Nb of dates, avoid T }
\NormalTok{nb_port <-}\StringTok{ }\DecValTok{3}                                                         \CommentTok{# Nb of portfolios/strats.}
\NormalTok{portf_weights <-}\StringTok{ }\KeywordTok{array}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{dim =} \KeywordTok{c}\NormalTok{(Tt, nb_port, }\KeywordTok{ncol}\NormalTok{(returns) }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{))   }\CommentTok{# Initial portf. weights}
\NormalTok{portf_returns <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{nrow =}\NormalTok{ Tt, }\DataTypeTok{ncol =}\NormalTok{ nb_port)                }\CommentTok{# Initial portf. returns }
\end{Highlighting}
\end{Shaded}

\normalsize

Next, because it is the purpose of this section, we isolate the
computation of the weights of sparse-hedging portfolios. In the case of
minimum variance portfolios, when \(\boldsymbol{\mu}=\boldsymbol{1}\),
the weight in asset 1 will simply be the sum of all terms in Equation
\eqref{eq:sparsehedgeeq2} and the other weights have similar forms.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{weights_sparsehedge <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(returns, alpha, lambda)\{  }\CommentTok{# The parameters are defined here}
\NormalTok{    w <-}\StringTok{ }\DecValTok{0}                                                \CommentTok{# Initiate weights}
    \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(returns))\{                            }\CommentTok{# Loop on the assets}
\NormalTok{        y <-}\StringTok{ }\NormalTok{returns[,i]                                  }\CommentTok{# Dependent variable}
\NormalTok{        x <-}\StringTok{ }\NormalTok{returns[,}\OperatorTok{-}\NormalTok{i]                                 }\CommentTok{# Independent variable}
\NormalTok{        fit <-}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(x,y, }\DataTypeTok{family =} \StringTok{"gaussian"}\NormalTok{, }\DataTypeTok{alpha =}\NormalTok{ alpha, }\DataTypeTok{lambda =}\NormalTok{ lambda)}
\NormalTok{        err <-}\StringTok{ }\NormalTok{y}\OperatorTok{-}\KeywordTok{predict}\NormalTok{(fit, x)                          }\CommentTok{# Prediction errors}
\NormalTok{        w[i] <-}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{-}\KeywordTok{sum}\NormalTok{(fit}\OperatorTok{$}\NormalTok{beta))}\OperatorTok{/}\KeywordTok{var}\NormalTok{(err)                }\CommentTok{# Output: weight of asset i}
\NormalTok{    \}}
    \KeywordTok{return}\NormalTok{(w }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(w))                                    }\CommentTok{# Normalisation of weights}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\normalsize

In order to benchmark our strategy, we define a meta-weighting function
that embeds three strategies: the EW benchmarks (1), the classical GMV
(2) and the sparse-hedging minimum variance (3). For the GMV, since
there are much more assets than date, the covariance matrix is singular.
Thus, we as a small heuristic shrinkage term. For a more rigorous
treatment of this technique, ce refer to the original article
\citet{ledoit2004well} and to the recent improvements mentioned in
\citet{ledoit2017nonlinear}. In short, we use
\(\hat{\boldsymbol{\Sigma}}=\boldsymbol{\Sigma}_S+\delta \boldsymbol{I}\)
for some small constant \(\delta\) (equal to 0.001 in the code below).

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{weights_multi <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(returns,j, alpha, lambda)\{}
\NormalTok{    N <-}\StringTok{ }\KeywordTok{ncol}\NormalTok{(returns)}
    \ControlFlowTok{if}\NormalTok{(j }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{)\{                                    }\CommentTok{# j = 1 => EW}
        \KeywordTok{return}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\NormalTok{N,N))}
\NormalTok{    \}}
    \ControlFlowTok{if}\NormalTok{(j }\OperatorTok{==}\StringTok{ }\DecValTok{2}\NormalTok{)\{                                    }\CommentTok{# j = 2 => Minimum Variance}
\NormalTok{        sigma <-}\StringTok{ }\KeywordTok{cov}\NormalTok{(returns) }\OperatorTok{+}\StringTok{ }\FloatTok{0.01} \OperatorTok{*}\StringTok{ }\KeywordTok{diag}\NormalTok{(N)     }\CommentTok{# Covariance matrix + regularizing term}
\NormalTok{        w <-}\StringTok{ }\KeywordTok{solve}\NormalTok{(sigma) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,N)             }\CommentTok{# Inverse & multiply}
        \KeywordTok{return}\NormalTok{(w }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(w))                         }\CommentTok{# Normalize}
\NormalTok{    \}}
    \ControlFlowTok{if}\NormalTok{(j }\OperatorTok{==}\StringTok{ }\DecValTok{3}\NormalTok{)\{                                    }\CommentTok{# j = 3 => Penalised / elasticnet}
\NormalTok{        w <-}\StringTok{ }\KeywordTok{weights_sparsehedge}\NormalTok{(returns, alpha, lambda)}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\normalsize

Finally, we proceed to the backtesting loop. Given the number of assets,
the execution of the loop takes a few minutes. At the end of the loop,
we compute the standard deviation of portfolio returns (monthly
volatility). This is the key indicator as minimum variance seeks to
minimize this particular metric.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(t }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(t_oos))\{                                                 }\CommentTok{# Loop = rebal. dates}
\NormalTok{    temp_data <-}\StringTok{ }\NormalTok{returns }\OperatorTok{%>%}\StringTok{                                               }\CommentTok{# Data for weights}
\StringTok{        }\KeywordTok{filter}\NormalTok{(date }\OperatorTok{<}\StringTok{ }\NormalTok{t_oos[t]) }\OperatorTok{%>%}\StringTok{                                        }\CommentTok{# Expand. window}
\StringTok{        }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{date) }\OperatorTok{%>%}
\StringTok{        }\KeywordTok{as.matrix}\NormalTok{() }
\NormalTok{    realised_returns <-}\StringTok{ }\NormalTok{returns }\OperatorTok{%>%}\StringTok{                                        }\CommentTok{# OOS returns}
\StringTok{        }\KeywordTok{filter}\NormalTok{(date }\OperatorTok{==}\StringTok{  }\NormalTok{t_oos[t]) }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{date)}
    \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{nb_port)\{                                                   }\CommentTok{# Looping over strats}
\NormalTok{        portf_weights[t,j,] <-}\StringTok{ }\KeywordTok{weights_multi}\NormalTok{(temp_data, j, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.1}\NormalTok{)       }\CommentTok{# Hard-coded params!}
\NormalTok{        portf_returns[t,j] <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(portf_weights[t,j,] }\OperatorTok{*}\StringTok{ }\NormalTok{realised_returns)  }\CommentTok{# Portf. returns}
\NormalTok{    \}}
\NormalTok{\}}
\KeywordTok{apply}\NormalTok{(portf_returns, }\DecValTok{2}\NormalTok{, sd)  }\CommentTok{# Portfolio volatilities (monthly scale)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.04180422 0.03350424 0.02672169
\end{verbatim}

\normalsize

The aim of the sparse hedging restrictions is to provide a better
estimate of the covariance structure of assets so that the estimation of
minimum variance portfolio weights is more accurate. From the above
exercise, we see that the monthly volatility is indeed reduced when
building covariance matrices based on sparse hedging relationships. This
is not the case if we use the shrunk sample covariance matrix because
there is probably too much noise in the estimates of correlations
between assets. Working with daily returns would likely improve the
quality of the estimates. But the above backtest shows that the
penalized methodology performs well even when the number of observations
(dates) is small compared to the number of assets.

\hypertarget{predictive-regressions}{%
\section{Predictive regressions}\label{predictive-regressions}}

\hypertarget{literature-review-and-principle}{%
\subsection{Literature review and
principle}\label{literature-review-and-principle}}

The topic of predictive regressions sits on a collection of very
interesting articles. One influential contribution is
\citet{stambaugh1999predictive}, where the authors show the perils of
regressions in which the independent variables are autocorrelated. In
this case, the usual OLS estimate is biased and must therefore be
corrected.

A second important topic pertains to the time-dependence of the
coefficients in predictive regressions. One contribution in this
direction is \citet{dangl2012predictive}, where latent variables and
Bayesian selection. More recently \citet{kelly2019characteristics} also
use latent variables to model the cross-section of stock returns. The
time-varying nature of coefficients of predictive regressions is further
documented by \citet{henkel2011time} for short term returns. Lastly,
\citet{farmer2019pockets} introduce the concept of pockets of
predictability: assets or markets experiences different phases; in some
phases they are predictable and in some others, they aren't. Pockets are
measured both by the number of days that a \emph{t}-statistic is above a
particular threshold and by the magnitude of the \(R^2\) over the
considered period.

The introduction of penalization within predictive regressions goes back
at least to \citet{rapach2013international}, where they are used to
assess lead-lag relationships between US markets and other international
stock exchanges. More recently, \citet{chinco2019sparse} use LASSO
regressions to forecast high frequency returns based on past returns (in
the cross-section) at various horizons. They report statistically
significant gains. \citet{han2018firm} and \citet{rapach2019time} use
LASSO and elasticnet regressions (respectively) to improve forecast
combinations and single out the characteristics that matter when
explaining stock returns.

These contributions underline the relevant of the overlap between
predictive regressions and penalized regressions. In simple
machine-learning based asset pricing, we often seek to build models such
as that of Equation \eqref{eq:genML}. If we stick to a linear relationship
and add penalization terms, then the model becomes:
\[r_{t+1,n} = \alpha_n + \sum_{k=1}^K\beta_n^kf^k_{t,n}+\epsilon_{t+1,n}, \quad \text{s.t.} \quad (1-\alpha)\sum_{j=1}^J |\beta_j| +\alpha\sum_{j=1}^J \beta_j^2< \theta\]
where we use \(f^k_{t,n}\) or \(x_{t,n}^k\) interchangeably and
\(\theta\) is some penalization intensity. Again, one of the aim of the
regularization is to generate more robust estimates. If the patterns
extracted hold out of sample, then
\[\hat{r}_{t+1,n} = \hat{\alpha}_n + \sum_{k=1}^K\hat{\beta}_n^kf^k_{t,n},\]
will be a relatively reliable proxy of future performance.

\hypertarget{code-and-results-1}{%
\subsection{Code and results}\label{code-and-results-1}}

Given the form of our dataset, implementing penalized predictive
regressions is easy.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y_penalized_train <-}\StringTok{ }\NormalTok{training_sample}\OperatorTok{$}\NormalTok{R1M_Usd                              }\CommentTok{# Dependent variable}
\NormalTok{x_penalized_train <-}\StringTok{ }\NormalTok{training_sample }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(features) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{() }\CommentTok{# Predictors}
\NormalTok{fit_pen_pred <-}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(x_penalized_train, y_penalized_train, }\DataTypeTok{alpha =} \FloatTok{0.1}\NormalTok{, }\DataTypeTok{lambda =} \FloatTok{0.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\normalsize

We then report two key performance measures: the mean squared error and
the hit ratio. A detailed account of metrics is given later in the book
(Chapter \ref{performance-metrics}).

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x_penalized_test <-}\StringTok{ }\NormalTok{testing_sample }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(features) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{()    }\CommentTok{# Predictors}
\KeywordTok{mean}\NormalTok{((}\KeywordTok{predict}\NormalTok{(fit_pen_pred, x_penalized_test) }\OperatorTok{-}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd)}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\CommentTok{# MSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.03699696
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{predict}\NormalTok{(fit_pen_pred, x_penalized_test) }\OperatorTok{*}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{) }\CommentTok{# Hit ratio}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5460346
\end{verbatim}

\normalsize

From an investor's standpoint, the MSE (or even the mean absolute error)
are hard to interpret because it complicated to map them mentally into
some intuitive financial indicator. In this perspective, the hit ratio
is more natural. It tells the proportion of correct signs achieved by
the predictions. If the investor is long in positive signals and short
in negative ones, the hit ratio indicates the proportion of `correct'
bets (the positions that go in the expected direction). A natural
threshold is 50\% but because of transaction costs, 51\% of accurate
forecasts probably won't be profitable. The figure 0.5460346 can be
deemed a relatively good hit ratio, though not a very impressive one.

\hypertarget{coding-exercises-2}{%
\section{Coding exercises}\label{coding-exercises-2}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  On the test sample, evaluate the impact of the two elastic net
  parameters on out-of-sample accuracy.\\
\item
\end{enumerate}

\hypertarget{trees}{%
\chapter{Tree-based methods}\label{trees}}

\label{sec:trees}

Classification and regression trees are simple yet powerful clustering
algorithms popularised by the monograph
\citet{breiman1984classification}. Decision trees and their extensions
are known to be quite efficient forecasting tools when working on
tabular data. A large proportion of winning solutions in ML contests
(especially on the Kaggle webiste\footnote{See www.kaggle.com. }) resort
to improvements of trees.

Recently, the surge in Machine Learning applications in Finance has led
to multiple publications that use trees in portfolio allocation
problems. A long though not exhaustive list includes:
\citet{ballings2015evaluating}, \citet{patel2015predicting},
\citet{patel2015bpredicting}, \citet{moritz2016tree},
\citet{krauss2017deep}, \citet{gu2018empirical}, \citet{guida2019big},
\citet{coqueret2019training} and \citet{simonian2019machine}. One
notable contribution is \citet{bryzgalova2019forest} in which the
authors create factors from trees by sorting portfolios via simple
trees, which they call \emph{Asset Pricing Trees}.

In this chapter, we review the methodologies associated to trees and
their applications in portfolio choice.

\hypertarget{simple-trees}{%
\section{Simple trees}\label{simple-trees}}

\hypertarget{principle}{%
\subsection{Principle}\label{principle}}

Decision trees seek to partition datasets into homogeneous clusters.
Given an exogenous variable \(\mathbf{Y}\) and features \(\mathbf{X}\),
trees iteratively split the sample into groups (usually two at a time)
which are as homogeneous in \(\mathbf{Y}\) as possible. The splits are
made according to one variable within the set of features. A short word
on nomenclature: when \(\mathbf{Y}\) consists of real numbers, we talk
about \emph{regression trees} and when \(\mathbf{Y}\) is categorical, we
use the term \emph{classification trees}.

Before formalising this idea, we illustrate this process in Figure
\ref{fig:treescheme}. There are 12 stars with three features: color,
size and complexity (number of branches).

\begin{figure}[H]
\includegraphics[width=850px]{images/tree_scheme} \caption{Elementary tree scheme: visualization of the splitting process.}\label{fig:treescheme}
\end{figure}

The dependent variable is the color (let's consider the wavelength
associated to the color for simplicity). The first split is made
according to size or complexity. Clearly, complexity is the better
choice: complicated stars are blue and green while simple stars are
yellow, orange and red. Splitting according to size would have mixed
blue and yellow stars (small ones) and green and orange stars (large
ones).

The second step is to split the two clusters one level further. Since
only one variable (size) is relevant, the secondary splits are
straightforward. In the end, our stylised tree has four consistent
clusters. The analogy with factor investing is simple: the color
represents performance: red for high performance and blue for mediocre
performance. The features (size and complexity of stars) are replaced by
firm-specific attributes, such as capitalization, accounting ratios,
etc. Hence, the purpose of the exercise is to find the characteristics
that allow to split firms into the ones that will perform well versus
those likely to fare more poorly.

We now turn to the technical construction of regression trees. We follow
the standard literature as exposed in \citet{breiman1984classification}
or in Chapter 9 of \citet{friedman2009elements}. Given a sample of
(\(y_i\),\(\mathbf{x}_i\)) of size \(I\), a \emph{regression} tree seeks
the splitting points that minimize the total variation of the \(y_i\)
inside the two child clusters. In order to do that, it proceeds in two
steps. First, it finds, for each feature \(x_i^{(k)}\), the best
splitting point (so that the clusters are homogeneous in
\(\mathbf{Y}\)). Second, it selects the feature that achieves the
highest level of homogeneity.

Homogeneity in regression trees is closely linked to variance. Since we
want the \(y_i\) inside each cluster to be similar, we seek to minimize
their variability inside each cluster and then sum the two figures. We
cannot sum the variances because this would not take into account the
relative sizes of clusters. Hence, we work with \emph{total} variation,
which is the variance times the number of elements in the clusters.

Below, the notation is a bit heavy because we resort to superscripts
\(k\) (the index of the feature), but it is largely possible to ignore
these superscripts to ease understanding. The first step is to find the
best split for each feature, that is, solve
\(\underset{c^{(k)}}{\text{argmin}} \ V^{(k)}_I(c^{(k)})\) with
\begin{equation}
\label{eq:node}
V^{(k)}_I(c^{(k}))= \underbrace{\sum_{x_i^{(k)}<c^{(k)}}\left(y_i-m_I^{k,-}(c^{(k)}) \right)^2}_{\text{Total dispersion of first cluster}} + \underbrace{\sum_{x_i^{(k)}>c^{(k)}}\left(y_i-m_I^{k,+}(c^{(k)}) \right)^2}_{\text{Total dispersion of second cluster}},
\end{equation} where
\(m_I^{k,-}(c^{(k)})=\frac{1}{\#\{i,x_i^{(k)}<c^{(k)} \}}\sum_{\{x_i^{(k)}<c^{(k)} \}}y_i\)
and
\(m_I^{k,+}(c^{(k)})=\frac{1}{\#\{i,x_i^{(k)}>c^{(k)} \}}\sum_{\{x_i^{(k)}>c^{(k)} \}}y_i\)
are the average values of \(Y\), conditional on \(X^{(k)}\) being
smaller or larger than \(c\). The cardinal function \(\#\{\cdot\}\)
counts the number of instances of its argument. For feature \(k\), the
optimal split \(c^{k,*}\) is thus the one for which the total dispersion
over the two subgroups is the smallest.

The optimal splits satisfy
\(c^{k,*}= \underset{c^{(k)}}{\text{argmin}} \ V^{(k)}_I(c^{(k)})\). Of
all the possible splitting variables, the tree will choose the one that
minimizes the total dispersion:
\(k^*=\underset{k}{\text{argmin}} \ V^{(k)}_I(c^{k,*})\).

After one split is performed, the procedure continues on the two newly
formed clusters. There are several criteria that can determine when to
stop the splitting process (see Section \ref{pruning-criteria}). One
simple criterion is to fix a maximum number of levels (the depth) for
the tree. A usual condition is to impose a minimum gain that is expected
for each split. If the reduction in dispersion after the split is only
marginal and below a specified threshold, then the split is not
executed. For further technical discussions on decision trees, we refer
for instance to section 9.2.4 of \citet{friedman2009elements}.

When the tree is built (trained), a prediction for new instances is easy
to make. Given its feature values, the instance ends up in one leaf of
the tree. Each leaf has an average value for the label: this is the
predicted outcome. Of course, this only works when the label is
numerical. We discuss below the changes that occur when it is
categorical.

\hypertarget{further-details-on-classification}{%
\subsection{Further details on
classification}\label{further-details-on-classification}}

Classification exercises are somewhat more complex than regression
tasks. The most obvious difference is the loss function which must take
into account the fact that the final output is not a simple number, but
a vector. The output \(\tilde{\textbf{y}}_i\) has as many elements as
there are categories in the label and each element is the probability
that the instance belong to the corresponding category.

For instance, if there are 3 categories: \emph{buy}, \emph{hold} and
\emph{sell}, then each instance would have a label with as many columns
as there are classes. Following our example, one label would be (1,0,0)
for a \emph{buy} position for instance. We refer to Section
\ref{categorical-labels} for a introduction on this topic.

Inside a tree, labels are aggregated at each cluster level. A typical
output would look like (0.6,0.1,0.3): they are the proportions of each
class represented within the cluster. In this case, the cluster has 60\%
of \emph{buy}, 10\% of \emph{hold} and 30\% of \emph{sell}.

The loss function must take into account this multidimensionality of the
label. When building trees, since the aim is to favor homogeneity, the
loss penalizes outputs that are not concentrated towards one class.
Indeed, facing a diversified output of (0.3,0.4,0.3) is much harder to
handle than the concentrated case of (0.8,0.1,0.1).

The algorithm is thus seeking purity: it searches a splitting criterion
that will lead to clusters that are as pure as possible, i.e., with one
very dominant class, or a few dominant classes. There are several
metrics proposed by the literature and all are based on the proportions
coded in the output. If there are \(J\) classes, we denote these
proportions with \(p_j\). The usual loss functions are:

\begin{itemize}
\tightlist
\item
  the Gini impurity index: \(1-\sum_{j=1}^Jp_j^2;\)\\
\item
  the misclassification error: \(1-\underset{j}{\text{max}}\, p_j;\)\\
\item
  entropy: \(-\sum_{j=1}^J\log(p_j)p_j.\)
\end{itemize}

The Gini index is nothing but one minus the Herfindahl index which
measures the diversification of a portfolio. Trees seek partitions that
are the least diversified. The minimum value of the Gini index is zero
and reached when one \(p_j=1\) and all others are equal to zero. The
maximum value is equal to \(1-1/J\) and is reached when all \(p_j=1/J\).
Similar relationships hold for the other two losses. One drawback of the
misclassification error is its lack of differentiability which explains
why the other two options are often favored.

Once the tree is grown, new instances automatically belong to one final
leaf. This leaf is associated to the proportions of classes it nests.

\hypertarget{pruning-criteria}{%
\subsection{Pruning criteria}\label{pruning-criteria}}

When building a tree, the splitting process can be pursued until the
full tree is grown, that is, when:\\
- when all instances belong to separate leaves, and/or\\
- when all leaves comprise instances that cannot be further segregated
based on the current set of features.

At this stage, the splitting process cannot be pursued.

Obviously, fully grown trees often lead to almost perfect fits when the
predictors are relevant, numerous and numerical. Nonetheless, the fine
grained idiosyncrasies of the training sample are of little interest for
out-of-sample predictions. For instance, being able to perfectly match
the patterns of 2000 to 2006 will probably not be very interesting in
2007 to 2009. The most reliable sections of the trees are those closest
to the root because they embed large portions of the data: the average
values in the early clusters are trustworthy because the are computed on
a large number of observations. The first splits are those that matter
the most because they highlight the most general patterns. The deepest
splits only deal with the peculiarities of the sample.

Thus, it is imperative to limit the size of the tree. There are several
ways to prune the tree and all depend on some particular criteria. We
list a few of them below:

\begin{itemize}
\tightlist
\item
  Impose a minimum number of instances for each terminal node (leaf).
  This ensures that each final cluster is composed by a sufficient
  number of observations. Hence, the average value of the label will be
  reliable because calculated on a large amount of data.\\
\item
  Similarly, it can be imposed that a cluster has a minimal size before
  even considering any further split. This criterion is of course
  related to the one above.\\
\item
  Require a certain threshold of improvement in the fit. If a split does
  not sufficiently reduce the loss, then it can be deemed unnecessary.
  The user specifies a small number \(\epsilon>0\) and a split is only
  validated if the loss obtained post-split is smaller than
  \(1-\epsilon\) times the loss before the split.\\
\item
  Limit the depth of the tree. The depth is defined as the overal
  maximum number of splits between the root and any leaf of the tree.
\end{itemize}

In the example below, we implement all of these criteria at the same
time, but usually, two of them at most should suffice.

\hypertarget{code-and-interpretation}{%
\subsection{Code and interpretation}\label{code-and-interpretation}}

We start with a simple tree and its interpretation. The label is the
future 1 month return and the features are all predictors available in
the sample. The tree is trained on the full sample.

\begin{verbatim}
## Loading required package: rpart
\end{verbatim}

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rpart)              }\CommentTok{# Tree package }
\KeywordTok{library}\NormalTok{(rpart.plot)         }\CommentTok{# Tree plot package}
\NormalTok{formula <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\StringTok{"R1M_Usd ~"}\NormalTok{, }\KeywordTok{paste}\NormalTok{(features, }\DataTypeTok{collapse =} \StringTok{" + "}\NormalTok{)) }\CommentTok{# Defines the model }
\NormalTok{formula <-}\StringTok{ }\KeywordTok{as.formula}\NormalTok{(formula)                                   }\CommentTok{# Forcing formula object}
\NormalTok{fit_tree <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(formula,}
             \DataTypeTok{data =}\NormalTok{ data_ml,     }\CommentTok{# Data source: full sample}
             \DataTypeTok{minbucket =} \DecValTok{1500}\NormalTok{,   }\CommentTok{# Min nb of obs required in each terminal node (leaf)}
             \DataTypeTok{minsplit =} \DecValTok{4000}\NormalTok{,    }\CommentTok{# Min nb of obs required to continue splitting}
             \DataTypeTok{cp =} \FloatTok{0.0001}\NormalTok{,        }\CommentTok{# Precision: smaller = more leaves}
             \DataTypeTok{maxdepth =} \DecValTok{3}        \CommentTok{# Maximum depth (i.e. tree levels)}
\NormalTok{             ) }
\KeywordTok{rpart.plot}\NormalTok{(fit_tree)             }\CommentTok{# Plot the tree}
\end{Highlighting}
\end{Shaded}

\begin{figure}[b]

{\centering \includegraphics[width=400px]{ML_factor_files/figure-latex/rpart1-1} 

}

\caption{Simple tree. The dependent variable is the 1 month future return.}\label{fig:rpart1}
\end{figure}

\normalsize

There usually exists a convention in the representation of trees. At
each node, a condition describes the split with a boolean expression. If
the expression is true, then the instance goes to the left cluster, if
not, it goes to the right cluster. Given the whole sample, the initial
split in this tree (Figure \ref{fig:rpart1}) is performed according to
market capitalization. If the 6 month market capitalization score (or
value) of the instance is above 0.025, then the instance is placed in
the left bucket, otherwise, it goes in the right bucket.

At each node, there are two important metrics. The first one is the
average value of the label in the cluster and the second one is the
proportion of instances in the cluster. At the top of the tree, all
instances (100\%) are present and the average 1 month future return is
1.3\%. One level below, the left cluster is by far the most crowded,
with roughly 98\% of observations averaging a 1.2\% return. The right
cluster is much smaller (2\%) but concentrates instances with a much
higher average return (6.1\%).

The splitting process continues similarly at each node until some
condition is satisfied (typically here: the maximum depth is reached). A
color codes the average return: from white (low return) to blue (high
return). The cluster with the lowest average return consists of firms
that satisfy \emph{all} the following criteria:

\begin{itemize}
\tightlist
\item
  have a 6 month market capitalization score above 0.025;\\
\item
  have a 3 month market capitalization score above 0.19;\\
\item
  have a score of recurring earnings over total assets above 0.025.
\end{itemize}

Notice that one peculiarity of trees is their possible heterogeneity in
cluster sizes. Sometimes, a few clusters gather almost all of the
observations while a few small groups embed some outliers. This is not a
favorable property of trees as small groups are more likely to be flukes
and may fail to generalize out-of-sample.

This is why we imposed restrictions during the construction of the tree.
The first one (minbucket = 1500 in the code) imposes that each cluster
consists of at least 1500 instances. The second one (minsplit) further
imposes that a cluster comprises at least 4000 observation in order to
pursue the splitting process. The cp = 0.0001 parameter in the code
requires any split to reduce the loss below 0.9999 times its original
value before the split. Finally, the maximum depth of three essentially
means that there are at most three splits between the root of the tree
and any terminal leaf.

The complexity of the tree (measured by the number of terminal leaves)
is a decreasing function of minbucket, minsplit and cp and an increasing
function of maximum depth.

Once the model has be trained (i.e., the tree is grown), a prediction
for any instance is the average value of the label within the cluster
where the instance should land.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{predict}\NormalTok{(fit_tree, data_ml[}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{,]) }\CommentTok{# Test (prediction) on the first five instances of the sample}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           1           2           3           4           5 
## 0.009529823 0.009529823 0.009529823 0.009529823 0.009529823
\end{verbatim}

\normalsize

Given the figure, we immediately conclude that the first three instances
belong to the second cluster (starting from the left) and the last two
belong to the fourth.

As a verification of the first split, we plot the smoothed average of
future returns, conditionally on market capitalization, past return and
price-to-book scores.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data_ml }\OperatorTok{%>%}\StringTok{ }\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{stat_smooth}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Mkt_Cap_6M_Usd, }\DataTypeTok{y =}\NormalTok{ R1M_Usd, }\DataTypeTok{color =} \StringTok{"Market Cap"}\NormalTok{), }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{stat_smooth}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Pb, }\DataTypeTok{y =}\NormalTok{ R1M_Usd, }\DataTypeTok{color =} \StringTok{"Price-to-Book"}\NormalTok{), }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{stat_smooth}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Mom_Sharp_11M_Usd, }\DataTypeTok{y =}\NormalTok{ R1M_Usd, }\DataTypeTok{color =} \StringTok{"Momentum"}\NormalTok{), }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Predictor"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[b]

{\centering \includegraphics[width=400px]{ML_factor_files/figure-latex/rpart3mkt-1} 

}

\caption{Average of 1 month future returns, conditionally on market capitalization and price-to-book scores}\label{fig:rpart3mkt}
\end{figure}

\normalsize

Indeed, we acknowledge a strong impact of market capitalization for the
smallest firms: they appear to earn a very high return (close to +5\% on
a monthly basis). The pattern is much more pronounced compared to other
well documented anomalies namely the value premium and the
(cross-sectional) momentum effect.

Finally, we assess the predictive quality of a single tree on the
testing set (the tree is grown on the training set). We use a deeper
tree, with a maximum depth of five.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit_tree2 <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(formula, }
             \DataTypeTok{data =}\NormalTok{ training_sample,     }\CommentTok{# Data source: training sample}
             \DataTypeTok{minbucket =} \DecValTok{1500}\NormalTok{,           }\CommentTok{# Min nb of obs required in each terminal node (leaf)}
             \DataTypeTok{minsplit =} \DecValTok{4000}\NormalTok{,            }\CommentTok{# Min nb of obs required to continue splitting}
             \DataTypeTok{cp =} \FloatTok{0.0001}\NormalTok{,                }\CommentTok{# Precision: smaller = more leaves}
             \DataTypeTok{maxdepth =} \DecValTok{5}                \CommentTok{# Maximum depth (i.e. tree levels)}
\NormalTok{             ) }
\KeywordTok{mean}\NormalTok{((}\KeywordTok{predict}\NormalTok{(fit_tree2, testing_sample) }\OperatorTok{-}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd)}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\CommentTok{# MSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.03700039
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{predict}\NormalTok{(fit_tree2, testing_sample) }\OperatorTok{*}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{) }\CommentTok{# Hit ratio}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5416619
\end{verbatim}

\normalsize

The mean squared error is usually hard to interpret. It's not easy to
map an error on returns into the impact on investment decisions. The hit
ratio is a more intuitive indicator because it evaluates the proportion
of correct guesses (and hence profitable investments). Obviously, it is
not perfect: 55\% of small gains can be mitigated by 45\% of large
losses. Nonetheless, it is a popular metric and moreover it corresponds
to the usual accuracy measure often computed in binary classification
exercises. Here, an accuracy of 0.5416619 is satisfactory. Even if any
number above 50\% may seem valuable, it must not be forgottent that
transaction costs will curtail benefits. Hence, the benchmark threshold
is probably at least at 52\%.

\hypertarget{random-forests}{%
\section{Random forests}\label{random-forests}}

While trees give intuitive representations of relationships between
\(\mathbf{Y}\) and \(\mathbf{X}\), they can be improved via the simple
idea of ensembles (which is discussed both more generally and in more
details in Chapter \ref{ensemble}).

\hypertarget{principle-1}{%
\subsection{Principle}\label{principle-1}}

One intuitive extension of predicting tools is a \emph{combination} of
such tools. Naturally, it is not obvious upfront which individual model
is the best, hence a combination seems a reasonable path towards the
diversification of prediction errors (when they are not too correlated).
Some theoretical foundations of model combinations were laid out in
\citet{schapire1990strength}.

More practical considerations were proposed later in
\citet{ho1995random} and more importantly in \citet{breiman2001random}
which is the major reference for random forests. There are two ways to
create multiple predictors from simple trees and random forests combines
both:

\begin{itemize}
\tightlist
\item
  first, the model can be trained on similar yet different datasets. One
  way to achieve this is via bootstrap: the instances are resampled with
  or without replacement, yielding new training data each time a new
  tree is built.
\item
  second, the data can be altered by curtailing the number of
  predictors. Alternative models are built based on different sets of
  features. The user chooses how many features to retain and then the
  algorithm selects these features randomly at each try.
\end{itemize}

Hence, it becomes simple to grow many different trees and the ensemble
is simply a weigthed combination of all trees. Usually, equal weights
are used, which is an agnostic and robust choice. We illustrate the idea
of simple combinations (also referred to as bagging) in Figure
\ref{fig:RF} below. The terminal prediction is simply the mean of all
intermediate predictions.

\begin{figure}[b]
\includegraphics[width=22.96in]{images/tree_RF} \caption{Combining tree outputs via random forests.}\label{fig:RF}
\end{figure}

Random forests, because they are built on the idea of bootstrapping are
more efficient than simple trees. They are used by
\citet{ballings2015evaluating}, \citet{patel2015predicting},
\citet{krauss2017deep}, and \citet{huck2019large} and they are shown to
perform very well in these papers. The original theoretical properties
of random forests are demonstrated in \citet{breiman2001random} for
classification trees and are fairly straightforward (it's likely similar
formulations hold for other ensemble types). The inaccuracy of the
aggregation (as measured by the so-called generalization error) is
bounded by \(\bar{\rho}(1-s^2)/s^2\), where\\
- \(s\) is the strength (average quality) of the individual classifiers
and\\
- \(\bar{\rho}\) is the average correlation between the learners.

Notably, \citet{breiman2001random} also shows that as the number of
trees grows to infinity, the inaccuracy converges to some finite number
which explains why random forests are not prone to overfitting.

While the original paper of \citet{breiman2001random} is dedicated to
classification models, many articles have since then tackled the problem
of regression trees. We refer the interested reader to
\citet{biau2012analysis} and to \citet{scornet2015consistency}. Finally,
further results on classifying ensembles can be obtained in
\citet{biau2008consistency} and we mention the short survey paper
\citet{denil2014narrowing} which sums up recent results in this field.

\hypertarget{code-and-results-2}{%
\subsection{Code and results}\label{code-and-results-2}}

Several implementations of random forests exist. For simplicity, we
choose to work with the original R library, but another choice could be
the one developed by h2o, which is a highly efficient meta-environment
for machine learning (coded in Java).

The syntax of randomForest follows that of many ML libraries. The full
list of options for some random forest implementations is prohibitively
large.\footnote{See, e.g.,
  \url{http://docs.h2o.ai/h2o/latest-stable/h2o-r/docs/reference/h2o.randomForest.html}}
Below, we train a model and exhibit the predictions for the first 5
instances of the testing sample.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(randomForest) }
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)                                }\CommentTok{# Sets the random seed}
\NormalTok{fit_RF <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(formula,             }\CommentTok{# Same formula as for simple trees!}
                 \DataTypeTok{data =}\NormalTok{ training_sample,    }\CommentTok{# Data source: training sample}
                 \DataTypeTok{sampsize =} \DecValTok{10000}\NormalTok{,          }\CommentTok{# Size of (random) sample for each tree}
                 \DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{,           }\CommentTok{# Is the sampling done with replacement?}
                 \DataTypeTok{nodesize =} \DecValTok{250}\NormalTok{,            }\CommentTok{# Minimum size of terminal cluster}
                 \DataTypeTok{ntree =} \DecValTok{40}\NormalTok{,                }\CommentTok{# Nb of random trees}
                 \DataTypeTok{mtry =} \DecValTok{30}                  \CommentTok{# Nb of predictive variables for each tree}
\NormalTok{    )}
\KeywordTok{predict}\NormalTok{(fit_RF, testing_sample[}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{,])       }\CommentTok{# Prediction over the first 5 test instances }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            1            2            3            4            5 
##  0.009787728  0.012507087  0.008722386  0.009398814 -0.011511758
\end{verbatim}

\normalsize

One first comment is that each instance has its own prediction, which
contrasts with the outcome of simple tree-based outcomes. Combining many
trees leads to tailored forecasts. Note that the second line of the
chunk freezes the random number generation. Indeed, random forests are
by construction contingent on the arbitrary combinations of instances
and features that are chosen to build the individual learners.

In the above example, each individual learner (tree) is built on 10,000
randomly chosen instances (without replacement) and each terminal leaf
(cluster) must comprise at least 240 elements (observations). In total,
40 trees are aggregated and each tree is constructed based on 30
randomly chosen predictors (out of the whole set of features).

Unlike for simple trees, it is not possible to simply illustrate the
outcome of the learning process. It could be possible to extract all 40
trees but a synthetic visualization is out-of-reach. A simplified view
can be obtained via variable importance, as is discussed in Section
\ref{variable-importance}.

Finally, we can assess the accuracy of the model.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{((}\KeywordTok{predict}\NormalTok{(fit_RF, testing_sample) }\OperatorTok{-}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd)}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\CommentTok{# MSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.03698197
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{predict}\NormalTok{(fit_RF, testing_sample) }\OperatorTok{*}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{) }\CommentTok{# Hit ratio}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5370186
\end{verbatim}

\normalsize

The MSE is smaller than 4\% and the hit ratio is close to 54\%, which is
reasonably above both 50\% and 52\% thresholds.

Let's see if we can improve the hit ratio by resorting to a
classification exercise. We start by training the model on a new formula
(the label is R1M\_Usd\_C).

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula_C <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\StringTok{"R1M_Usd_C ~"}\NormalTok{, }\KeywordTok{paste}\NormalTok{(features, }\DataTypeTok{collapse =} \StringTok{" + "}\NormalTok{)) }\CommentTok{# Defines the model }
\NormalTok{formula_C <-}\StringTok{ }\KeywordTok{as.formula}\NormalTok{(formula_C)                                   }\CommentTok{# Forcing formula object}
\NormalTok{fit_RF_C <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(formula_C,         }\CommentTok{# New formula! }
                 \DataTypeTok{data =}\NormalTok{ training_sample,    }\CommentTok{# Data source: training sample}
                 \DataTypeTok{sampsize =} \DecValTok{20000}\NormalTok{,          }\CommentTok{# Size of (random) sample for each tree}
                 \DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{,           }\CommentTok{# Is the sampling done with replacement?}
                 \DataTypeTok{nodesize =} \DecValTok{250}\NormalTok{,            }\CommentTok{# Minimum size of terminal cluster}
                 \DataTypeTok{ntree =} \DecValTok{40}\NormalTok{,                }\CommentTok{# Number of random trees}
                 \DataTypeTok{mtry =} \DecValTok{30}                  \CommentTok{# Number of predictive variables for each tree }
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\normalsize

We can then assess the proportion of correct (binary) guesses.
\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{predict}\NormalTok{(fit_RF_C, testing_sample) }\OperatorTok{==}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd_C) }\CommentTok{# Hit ratio}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4977353
\end{verbatim}

\normalsize

The accuracy is disappointing. There are two potential explanations for
this (beyond the possibility of very different patterns in the training
and testing sets). The first one is the sample size, which may be too
small. The original training set has more than 200,000 observations,
hence we retain only one in 10 in the above training specification. We
are thus probably sideline relevant information and the cost can be
heavy. The second reason is the number of predictors, which is set to
30, i.e., one third of the total at our disposal. Unfortunately, this
leaves room for the algorithm to pick less pertinent predictors. The
default numbers of predictors chosen by the routines are \(\sqrt{p}\)
and \(p/3\) for classification and regression tasks, respectively. Here
\(p\) is the total number of features.

\hypertarget{adaboost}{%
\section{Boosted trees: Adaboost}\label{adaboost}}

The idea of boosting is slightly more advanced compared to agnostic
aggregation. In random forest, we hope that the diversification through
many trees will improve the overall quality of the model. In boosting,
it is sought to iteratively improve the model whenever a new tree is
added. There are many ways to boost learning and we present two that can
easily be implemented with trees. The first one (Adaboost, for adaptive
boosting) improves the learning process by progressively focusing on the
instances that yield the largest errors. The second one (xgboost) is a
flexible algorithm in which each new tree is only focused on the
minimization of the training sample loss.

\hypertarget{methodology}{%
\subsection{Methodology}\label{methodology}}

The origins of adaboost go back to \citet{freund1997decision},
\citet{freund1996experiments} and for the sake of completeness, we also
mention the book dedicated on boosting \citet{schapire2012boosting}.
Extensions of these ideas are proposed in \citet{friedman2000additive}
(the so-called real Adaboost algorithm) and in
\citet{drucker1997improving} (for regression analysis). Theoretical
treatments were derived by \citet{breiman2004population}.

We start by directly stating the general structure of the algorithm:

\begin{itemize}
\tightlist
\item
  set equal weights \(w_i=I^{-1}\);\\
\item
  For \(m=1,\dots,M\) do:
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find a learner \(l_m\) that minimizes the weighted loss
  \(\sum_{i=1}^Iw_iL(l_m(\textbf{x}_i),\textbf{y}_i)\);
\item
  Compute a learner weight \begin{equation}
  \label{eq:adaboostam}
  a_m=f_a(\textbf{w},l_m(\textbf{x}),\textbf{y});
  \end{equation}
\item
  Update the instance weights \begin{equation}
  \label{eq:adaboostw}
  w_i \leftarrow w_ie^{f_w(l_m(\textbf{x}_i), \textbf{y}_i)};
  \end{equation}
\item
  Normalize the \(w_i\) to sum to one;
\end{enumerate}

\begin{itemize}
\tightlist
\item
  The output for instance \(\textbf{x}_i\) is a simple function of
  \(\sum_{m=1}^M a_ml_m(\textbf{x}_i)\), \begin{equation}
  \label{eq:adaboosty}
  \tilde{y}_i=f_y\left(\sum_{m=1}^M a_ml_m(\textbf{x}_i) \right).
  \end{equation}
\end{itemize}

Let us comment on the steps of the algorithm. The formulation holds for
many variations of Adaboost and we will specify the functions \(f_a\)
and \(f_w\) below.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The first step seeks to find a learner (tree) \(l_m\) that minimizes a
  weighted loss. Here the base loss function \(L\) essentially depends
  on the task (regression versus classification).
\item
  The second and third steps are the most interesting because they are
  the heart of Adaboost: they define the way the algorithm adapts
  sequentially. Because the purpose is to aggregate models, a more
  sophisticated approach compared to uniform weights for learners is a
  tailored weight for each learner. A natural property (for \(f_a\))
  should be that a learner that yields a smaller error should have a
  larger weight because it is more accurate.
\item
  The third step is to change the weights of observations. In this case,
  because the model aims at improving the learning process, \(f_w\) is
  constructed to give more weight on observations for which the current
  model does not do a good job (i.e., generates the largest errors).
  Hence, the next learner will be incentivized to pay more attention on
  these pathological cases.
\item
  The third step is a simple scaling procedure.
\end{enumerate}

In Table \ref{tab:adaboost}, we detail two examples of weighting
functions used in the literature. For the original Adaboost
(\citet{freund1996experiments}, \citet{freund1997decision}), the label
is binary with values +1 and -1 only. The second example stems from
\citet{drucker1997improving} and is dedicated to regression analysis
(with real-valued label). The interested reader can have a look at other
possibilities in \citet{schapire2003boosting} and
\citet{ridgeway1999boosting}.

\begin{longtable}[]{@{}lll@{}}
\caption{\label{tab:adaboost} Examples of functions for Adaboost-like
algorithms.}\tabularnewline
\toprule
\begin{minipage}[b]{0.30\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
Bin. classif. (orig. Adaboost)\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
Regression (Drucker (1997))\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.30\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
Bin. classif. (orig. Adaboost)\strut
\end{minipage} & \begin{minipage}[b]{0.30\columnwidth}\raggedright
Regression (Drucker (1997))\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Individual error\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(\epsilon_i=\textbf{1}_{\left\{y_i\neq l_m(\textbf{x}_i) \right\}}\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(\epsilon_i=\frac{|y_i- l_m(\textbf{x}_i)|}{\underset{i}{\max}|y_i- l_m(\textbf{x}_i)|}\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Weight of learner via \(f_a\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(f_a=\log\left(\frac{1-\epsilon}{\epsilon} \right)\),with
\(\epsilon=I^{-1}\sum_{i=1}^Iw_i \epsilon_i\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(f_a=\log\left(\frac{1-\epsilon}{\epsilon} \right)\),with
\(\epsilon=I^{-1}\sum_{i=1}^Iw_i \epsilon_i\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Weight of instances via \(f_w(i)\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(f_w=f_a\epsilon_i\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(f_w=f_a\epsilon_i\)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.30\columnwidth}\raggedright
Output function via \(f_y\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
\(f_y(x) = \text{sign}(x)\)\strut
\end{minipage} & \begin{minipage}[t]{0.30\columnwidth}\raggedright
weighted median of predictions\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Let us comment on the original Adaboost specification. The basic error
term
\(\epsilon_i=\textbf{1}_{\left\{y_i\neq l_m(\textbf{x}_i) \right\}}\) is
a dummy number indicating if the prediction is correct (we recall only
two values are possible, +1 and -1). The average error
\(\epsilon\in [0,1]\) is simply a weighted average of individual errors
and the weight of the \(m^{th}\) learner defined in Equation
\eqref{eq:adaboostam} is given by
\(a_m=\log\left(\frac{1-\epsilon}{\epsilon} \right)\). The function
\(x\mapsto \log((1-x)x^{-1})\) decreases on \([0,1]\) and switches sign
(from positive to negative) at \(x=1/2\). Hence, when the average error
is small, the learner has a large positive weight but when the error
becomes large, the learner can even obtain a negative weight. Indeed,
the threshold \(\epsilon>1/2\) indicated that the learner is wrong more
than 50\% of the time. Obviously, this indicates a problem and the
learner should even be discarded.

The change in instance weights follows a similar logic. The new weight
is proportional to
\(w_i\left(\frac{1-\epsilon}{\epsilon} \right)^{\epsilon_i}\). If the
prediction is right and \(\epsilon_i=0\), the weight is unchanged. If
the prediction is wrong and \(\epsilon_i=1\), the weight is adjusted
depending on the aggregate error \(\epsilon\). If the error is small and
the learner efficient (\(\epsilon<1/2\)), then
\((1-\epsilon)/\epsilon>1\) and the weight of the instance increases.
This means that for the next round, the learner will have to focus more
on instance \(i\).

Lastly, the final prediction of the model corresponds to the sign of the
weighted sums of individual predictions: if the sum is positive, the
model will predict +1 and it will yield -1 otherwise.\footnote{The Real
  Adaboost of \citet{friedman2000additive} has a different output: the
  probability of belonging to a particular class.} The odds of a zero
sum are negligible. In the case of numerical labels, the process is
slightly more complicated and we refer to Section 3, step 8 of
\citet{drucker1997improving} for more details on how to proceed.

We end this presentation with one word on instance weighting. There are
two ways to deal with this topic. The first one works at the level of
the loss functions. For regression trees, Equation \eqref{eq:node} would
naturally generalize to
\[V^{(k)}_N(c^{(k}), \textbf{w})= \sum_{x_i^{(k)}<c^{(k)}}w_i\left(y_i-m_N^{k,-}(c^{(k)}) \right)^2 + \sum_{x_i^{(k)}>c^{(k)}}w_i\left(y_i-m_N^{k,+}(c^{(k)}) \right)^2,\]

and hence an instance with a large weight \(w_i\) would contribute more
to the dispersion of its cluster. For classification objectives, the
alteration is more complex and we refer to \citet{ting2002instance} for
one example of an instance-weighted tree-growing algorithm. The idea is
closely linked to the alteration of the misclassification risk via a
loss matrix (see Section 9.2.4 in \citet{friedman2009elements}).

The second way to enforce instance weighting is via random sampling. If
instances have weights \(w_i\), then the training of learners can be
performed over a sample that is randomly extracted with distribution
equal to \(w_i\). In this case, an instance with a larger weight will
have more chances to be represented in the training sample. The original
adaboost algorithm relies on this method.

\hypertarget{illustration}{%
\subsection{Illustration}\label{illustration}}

Below, we test an implementation of the original adaboost classifier. As
such, we work with the R1M\_Usd\_C variable and change the model
formula. The computational cost of adaboost is high on large datasets,
thus we work with a smaller sample and we only impose three iterations.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(fastAdaboost)                                                     }\CommentTok{# Adaboost package }
\NormalTok{subsample <-}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{52000}\NormalTok{)}\OperatorTok{*}\DecValTok{4}                                                  \CommentTok{# Target small sample}
\NormalTok{fit_adaboost_C <-}\StringTok{ }\KeywordTok{adaboost}\NormalTok{(formula_C,                                     }\CommentTok{# Model spec.}
                         \DataTypeTok{data =} \KeywordTok{data.frame}\NormalTok{(training_sample[subsample,]),  }\CommentTok{# Data source}
                         \DataTypeTok{nIter =} \DecValTok{3}\NormalTok{)                                       }\CommentTok{# Number of trees              }
\end{Highlighting}
\end{Shaded}

\normalsize

Finally, we evaluate the performance of the classifier.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd_C }\OperatorTok{==}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit_adaboost_C, testing_sample)}\OperatorTok{$}\NormalTok{class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5028202
\end{verbatim}

\normalsize

The accuracy (as evaluated by the hit ratio) is clearly not
satisfactory. One reason for this may be the restrictions we enforced
for the training (smaller sample and only three trees).

\hypertarget{boosted-trees-extreme-gradient-boosting}{%
\section{Boosted trees: extreme gradient
boosting}\label{boosted-trees-extreme-gradient-boosting}}

The ideas behind tree boosting were popularized, among others, by
\citet{mason2000boosting}, \citet{friedman2001greedy}, and
\citet{friedman2002stochastic}. In this case, the combination of
learners (prediction tools) is not agnostic as in random forest, but
adapted (or optimized) at the learner level. At each step \(s\), the sum
of models \(M_S=\sum_{s=1}^{S-1}m_s+m_S\) is such that the last learner
\(m_S\) was precisely designed to reduce the loss of \(M_S\) on the
training sample.

Below, we follow closely the original work of \citet{chen2016xgboost}
because their algorithm yields incredibly accurate predictions and also
because it is highly customizable. It is their implementation that we
use in our empirical section. The other popular alternative is lightgbm
(see \citet{ke2017lightgbm}). What XGBoost seeks to minimise is the
objective
\[O=\underbrace{\sum_{i=1}^I \text{loss}(y_i,\tilde{y}_i)}_{\text{error term}} \quad + \underbrace{\sum_{j=1}^J\Omega(T_j)}_{\text{regularisation term}}.\]
The first term (over all instances) measures the distance between the
true label and the output from the model. The second term (over all
trees) penalises models that are too complex.

For simplicity, we propose the full derivation with the simplest loss
function \(\text{loss}(y,\tilde{y})=(y-\tilde{y})^2\), so that:
\[O=\sum_{i=1}^I \left(y_i-m_{J-1}(\mathbf{x}_i)-T_J(\mathbf{x}_i)\right)^2+ \sum_{j=1}^J\Omega(T_j).\]

\hypertarget{managing-loss}{%
\subsection{Managing Loss}\label{managing-loss}}

Let us assume that we have already built all trees \(T_{j}\) up to
\(j=1,\dots,J-1\) (and hence model \(M_{J-1}\)): how to choose tree
\(T_J\) optimally? We rewrite \begin{align*}
O&=\sum_{i=1}^I \left(y_i-m_{J-1}(\mathbf{x}_i)-T_J(\mathbf{x}_i)\right)^2+ \sum_{j=1}^J\Omega(T_j) \\
&=\sum_{i=1}^I\left\{y_i^2+m_{J-1}(\mathbf{x}_i)^2+T_J(\mathbf{x}_i)^2 \right\} + \sum_{j=1}^{J-1}\Omega(T_j)+\Omega(T_J) \quad \text{(squared terms + penalisation)}\\
& \quad -2 \sum_{i=1}^I\left\{y_im_{J-1}(\mathbf{x}_i)+y_iT_J(\mathbf{x}_i)-m_{J-1}(\mathbf{x}_i) T_J(\mathbf{x}_i))\right\}\quad \text{(cross terms)} \\
&= \sum_{i=1}^I\left\{-2 y_iT_J(\mathbf{x}_i)+2m_{J-1}(\mathbf{x}_i) T_J(\mathbf{x}_i))+T_J(\mathbf{x}_i)^2 \right\} +\Omega(T_J) + c
\end{align*} All terms known at step \(J\) (i.e., indexed by \(J-1\))
vanish because they do not enter the optimisation scheme. The are
embedded in the constant \(c\).

Things are fairly simple with quadratic loss. For more complicated loss
functions, Taylor expansions are used (see the original paper).

\hypertarget{penalisation}{%
\subsection{Penalisation}\label{penalisation}}

In order to go any further, we need to specify the way the penalisation
works. For a given tree \(T\), we specify its structure by
\(T(x)=w_{q(x)}\), where \(w\) is the output value of some leaf and
\(q(\cdot)\) is the function that maps an input to its final leaf. This
encoding is illustrated in Figure \ref{fig:treeq}. The function \(q\)
indicates the path, while the vector \(\textbf{w}=w_i\) codes the
terminal leaf values.

\begin{figure}[H]

{\centering \includegraphics[width=400px]{images/tree_q} 

}

\caption{Coding a decision tree}\label{fig:treeq}
\end{figure}

We write \(l=1,\dots,L\) for the indices of the leafs of the tree. In
XGBoost, complexity is defined as:
\[\Omega(T)=\gamma L+\frac{\lambda}{2}\sum_{l=1}^Lw_l^2,\] where

\begin{itemize}
\tightlist
\item
  the first term penalises the \textbf{total number of leaves};\\
\item
  the second term penalises the \textbf{magnitude of output values}
  (this helps reduce variance).
\end{itemize}

The first penalization reduces the depth of the tree while the second
shrinks the size of the adjustments that will come from the latest tree.

\hypertarget{aggregation}{%
\subsection{Aggregation}\label{aggregation}}

We aggregate both sections of the objective (loss and penalization). We
write \(I_l\) for the set of the indices of the instances belonging to
leaf \(l\). Then,\\
\begin{align*}
O&= 2\sum_{i=1}^I\left\{ -y_iT_J(\mathbf{x}_i)+m_{J-1}(\mathbf{x}_i) T_J(\mathbf{x}_i))+\frac{T_J(\mathbf{x}_i)^2}{2} \right\} + \gamma L+\frac{\lambda}{2}\sum_{l=1}^Lw_l^2 \\
&=2\sum_{i=1}^I\left\{- y_iw_{q(\mathbf{x}_i)}+m_{J-1}(\mathbf{x}_i)w_{q(\mathbf{x}_i)})+\frac{w_{q(\mathbf{x}_i)}^2}{2} \right\} + \gamma L+\frac{\lambda}{2}\sum_{l=1}^Lw_l^2 \\
&=2 \sum_{l=1}^L \left(w_l\sum_{i\in I_l}(-y_i +m_{J-1}(\mathbf{x}_i))+ \frac{w_l^2}{2}\sum_{i\in I_l}\left(1+\frac{\lambda}{2}\right)\right)+ \gamma L
\end{align*}\\
The function is of the form \(aw_l+\frac{b}{2}w_l^2\), which has minimum
values \(-\frac{a^2}{2b}\) at point \(w_l=-a/b\). Thus, writing \#(.)
for the cardinal function that counts the number of items in a set,
\begin{align}
\label{eq:xgbweight}
\mathbf{\rightarrow} \quad w^*_l&=\frac{\sum_{i\in I_l}(y_i -m_{J-1}(\mathbf{x}_i))}{\left(1+\frac{\lambda}{2}\right)\#\{i\in I_l\}}, \text{ so that} \\
O_L(q)&=-\frac{1}{2}\sum_{l=1}^L \frac{\left(\sum_{i\in I_l}(y_i -m_{J-1}(\mathbf{x}_i))\right)^2}{\left(1+\frac{\lambda}{2}\right)\#\{i\in I_l\}}+\gamma L, \nonumber
\end{align} where we added the dependence of the objective both in \(q\)
(structure of tree) and \(L\) (number of leaves). Indeed, the meta-shape
of the tree remains to be determined.

\hypertarget{tree-structure}{%
\subsection{Tree structure}\label{tree-structure}}

Final problem: the \textbf{tree structure}! Let us take a step back. In
the construction of a simple regression tree, the output value at each
node is equal to the average value of the label within the node (or
cluster). When adding a new tree in order to reduce the loss, the nodes
values must be computed completely differently, which is the purpose of
Equation \eqref{eq:xgbweight}.

Nonetheless, the growing of the iterative trees follows similar lines as
simple trees. Features must be tested in order to pick the one that
minimizes the objective for each given split. The final question is
then: what's the best depth and when to stop growing the tree? The
method is to

\begin{itemize}
\tightlist
\item
  proceed node by node;\\
\item
  for each node, look at whether a split is useful (in terms of
  objective) or not:
  \[\text{Gain}=\frac{1}{2}\left(\text{Gain}_L+\text{Gain}_R+\text{Gain}_O \right)-\gamma\]\\
\item
  each gain is computed with respect to the instances in each bucket
  (cluster):
  \[\text{Gain}_\mathcal{X}= \frac{\left(\sum_{i\in I_\mathcal{X}}(y_i -m_{J-1}(\mathbf{x}_i))\right)^2}{\left(1+\frac{\lambda}{2}\right)\#\{i\in I_\mathcal{X}\}},\]
  where \(I_\mathcal{X}\) is the set of instances within cluster
  \(\mathcal{X}\).
\end{itemize}

\(\text{Gain}_O\) is the original gain (no split) and \(\text{Gain}_L\)
and \(\text{Gain}_R\) are the gains of the left and right cluster,
respectively. One work about the \(-\gamma\) adjustment in the above
formula: there is one unit of new leaves (two new minus one old)! This
makes a one leaf difference, hence \(\Delta L =1\) and the penalization
intensity for each new leaf is equal to \(\gamma\).

Lastly, we underline the fact that XGBoost also applies a learning rate:
each new tree is scaled by a factor \(\eta\), with \(\eta \in (0,1]\).
This is very useful because a pure aggregation of 100 optimized trees is
the best way to overfit the training sample.

\hypertarget{boostext}{%
\subsection{Extensions}\label{boostext}}

Several additional features are available to further prevent boosted
trees to overfit. Indeed, given a sufficiently large number of trees,
the aggregation is able to match the training sample very well, but may
fail to generalize well out-of-sample.

Following the pioneering work of \citet{srivastava2014dropout}, the DART
(Dropout for Additive Regression Trees) model was proposed by
\citet{rashmi2015dart}. The idea is to omit a specified number of trees
during training. The trees that are removed from the model are chosen
randomly. The full specifications can be found at
\url{https://xgboost.readthedocs.io/en/latest/tutorials/dart.html}.

Monotonicity constraints are another element that is featured both in
xgboost and lightgbm. Sometimes, it is expected that one particular
feature has a monotonous impact on the label. For instance, if one
deeply believes in momentum, then past returns should have an increasing
impact on future returns (in the cross-section of stocks).

Given the recursive nature of the splitting algorithm, it is possible to
choose when to perform a split (according to a particular variable) and
when not to. In Figure \ref{fig:monotonic}, we show how the algorithm
proceeds. All splits are performed according to the same feature. For
the first split, things are easy because it suffices to verify that the
averages of each cluster are ranked in the right direction. Things are
more complicated for the splits that occur below. Indeed, the average
values set by all above splits matter as they give bounds for acceptable
values for the future average values in lower splits. If a split
violates these bounds, then it is overlooked and another variable will
be chosen instead.

\begin{figure}[b]
\includegraphics[width=16.4in]{images/tree_monotonic} \caption{Imposing monotonic constraints. The constraints are shown in bold blue in the bottom leaves.}\label{fig:monotonic}
\end{figure}

\hypertarget{boostcode}{%
\subsection{Code and results}\label{boostcode}}

In this section, we train a model using the \emph{XGBoost} library.
Other options include \emph{catboost}, \emph{gbm}, \emph{lightgbm}, and
\emph{h2o}'s own version of boosted machines. Unlike many other
packages, the XGBoost function requires a particular syntax and
dedicated formats. The first step is thus to encapsulate the data
accordingly.

Moreover, because training times can be long, we shorten the training
sample as advocated in \citet{coqueret2019training}. We retain only the
40\% most extreme observations (in terms of label values: top 20\% and
bottom 20\%) and work with the small subset of features. In all coding
sections dedicated to boosted trees in this book, the models will be
trained with only 7 features.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(xgboost)                                                }\CommentTok{# The package for boosted trees}
\NormalTok{train_features_xgb <-}\StringTok{ }\NormalTok{training_sample }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{filter}\NormalTok{(R1M_Usd }\OperatorTok{<}\StringTok{ }\KeywordTok{quantile}\NormalTok{(R1M_Usd, }\FloatTok{0.2}\NormalTok{) }\OperatorTok{|}\StringTok{ }
\StringTok{               }\NormalTok{R1M_Usd }\OperatorTok{>}\StringTok{ }\KeywordTok{quantile}\NormalTok{(R1M_Usd, }\FloatTok{0.8}\NormalTok{)) }\OperatorTok{%>%}\StringTok{            }\CommentTok{# Extreme values only!}
\StringTok{    }\KeywordTok{select}\NormalTok{(features_short) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{()                      }\CommentTok{# Independent variable}
\NormalTok{train_label_xgb <-}\StringTok{ }\NormalTok{training_sample }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{filter}\NormalTok{(R1M_Usd }\OperatorTok{<}\StringTok{ }\KeywordTok{quantile}\NormalTok{(R1M_Usd, }\FloatTok{0.2}\NormalTok{) }\OperatorTok{|}\StringTok{ }
\StringTok{               }\NormalTok{R1M_Usd }\OperatorTok{>}\StringTok{ }\KeywordTok{quantile}\NormalTok{(R1M_Usd, }\FloatTok{0.8}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{select}\NormalTok{(R1M_Usd) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{()                             }\CommentTok{# Dependent variable}
\NormalTok{train_matrix_xgb <-}\StringTok{ }\KeywordTok{xgb.DMatrix}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ train_features_xgb, }
                                \DataTypeTok{label =}\NormalTok{ train_label_xgb)        }\CommentTok{# XGB format!}
\end{Highlighting}
\end{Shaded}

\normalsize

The second (optional) step is to determine the monotonicity constraints
that we want to impose. For simplicity, we will only enforce three
constraints on

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  market capitalization (negative, because large firms have smaller
  returns under the size anomaly);\\
\item
  price-to-book ratio (negative, because overvalued forms also have
  smaller returns under the value anomaly);\\
\item
  past annual returns (positive, because winner outperform losers under
  the momentum anomaly).
\end{enumerate}

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mono_const <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{length}\NormalTok{(features))                   }\CommentTok{# Initialize the vector}
\NormalTok{mono_const[}\KeywordTok{which}\NormalTok{(features }\OperatorTok{==}\StringTok{ "Mkt_Cap_12M_Usd"}\NormalTok{)] <-}\StringTok{ }\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{) }\CommentTok{# Decreasing in market cap}
\NormalTok{mono_const[}\KeywordTok{which}\NormalTok{(features }\OperatorTok{==}\StringTok{ "Pb"}\NormalTok{)] <-}\StringTok{ }\NormalTok{(}\OperatorTok{-}\DecValTok{1}\NormalTok{)              }\CommentTok{# Decreasing in price-to-book}
\NormalTok{mono_const[}\KeywordTok{which}\NormalTok{(features }\OperatorTok{==}\StringTok{ "Mom_11M_Usd"}\NormalTok{)] <-}\StringTok{ }\DecValTok{1}        \CommentTok{# Increasing in past return}
\end{Highlighting}
\end{Shaded}

\normalsize

The third step is to train the model on the formatted training data.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit_xgb <-}\StringTok{ }\KeywordTok{xgb.train}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ train_matrix_xgb,     }\CommentTok{# Data source }
              \DataTypeTok{eta =} \FloatTok{0.3}\NormalTok{,                          }\CommentTok{# Learning rate}
              \DataTypeTok{objective =} \StringTok{"reg:linear"}\NormalTok{,           }\CommentTok{# Objective function}
              \DataTypeTok{max_depth =} \DecValTok{4}\NormalTok{,                      }\CommentTok{# Maximum depth of trees}
              \DataTypeTok{lambda =} \DecValTok{1}\NormalTok{,                         }\CommentTok{# Penalisation of leaf values}
              \DataTypeTok{gamma =} \FloatTok{0.1}\NormalTok{,                        }\CommentTok{# Penalisation of number of leaves}
              \DataTypeTok{nrounds =} \DecValTok{30}\NormalTok{,                       }\CommentTok{# Number of trees used (rather low here)}
              \DataTypeTok{monotone_constraints =}\NormalTok{ mono_const,  }\CommentTok{# Monotonicity constraints}
              \DataTypeTok{verbose =} \DecValTok{0}                         \CommentTok{# No comment from the algo }
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\normalsize

Finally, we evaluate the performance of the model. Note that before
that, a proper formatting of the testing sample is required.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{xgb_test <-}\StringTok{ }\NormalTok{testing_sample }\OperatorTok{%>%}\StringTok{                                }\CommentTok{# Test sample => XGB format}
\StringTok{    }\KeywordTok{select}\NormalTok{(features_short) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{as.matrix}\NormalTok{() }
\KeywordTok{mean}\NormalTok{((}\KeywordTok{predict}\NormalTok{(fit_xgb, xgb_test) }\OperatorTok{-}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd)}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\CommentTok{# MSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.03804396
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{predict}\NormalTok{(fit_xgb, xgb_test) }\OperatorTok{*}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{) }\CommentTok{# Hit ratio}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5047146
\end{verbatim}

\normalsize

The performance is comparable to those observed for other predictive
tools. As a final exercise, we show one implementation of a
classification task under XGBoost. Only the label changes. In XGBoost,
labels must be coded with integer number, starting at zero exactly. In
R, factors are numerically coded as integers numbers starting from one,
hence the mapping is simple.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train_label_C <-}\StringTok{ }\NormalTok{training_sample }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{filter}\NormalTok{(R1M_Usd }\OperatorTok{<}\StringTok{ }\KeywordTok{quantile}\NormalTok{(R1M_Usd, }\FloatTok{0.2}\NormalTok{) }\OperatorTok{|}\StringTok{          }\CommentTok{# Either low 20% returns }
\StringTok{               }\NormalTok{R1M_Usd }\OperatorTok{>}\StringTok{ }\KeywordTok{quantile}\NormalTok{(R1M_Usd, }\FloatTok{0.8}\NormalTok{)) }\OperatorTok{%>%}\StringTok{   }\CommentTok{# Or top 20% returns}
\StringTok{    }\KeywordTok{select}\NormalTok{(R1M_Usd_C)}
\NormalTok{train_matrix_C <-}\StringTok{ }\KeywordTok{xgb.DMatrix}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ train_features_xgb, }
                              \DataTypeTok{label =} \KeywordTok{as.numeric}\NormalTok{(train_label_C }\OperatorTok{==}\StringTok{ "TRUE"}\NormalTok{)) }\CommentTok{# XGB format!}
\end{Highlighting}
\end{Shaded}

\normalsize

When working with categories, the loss function is usually the softmax
function (see Section \ref{notations}).

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit_xgb_C <-}\StringTok{  }\KeywordTok{xgb.train}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ train_matrix_C,  }\CommentTok{# Data source (pipe input)}
              \DataTypeTok{eta =} \FloatTok{0.8}\NormalTok{,                        }\CommentTok{# Learning rate}
              \DataTypeTok{objective =} \StringTok{"multi:softmax"}\NormalTok{,      }\CommentTok{# Objective function}
              \DataTypeTok{num_class =} \DecValTok{2}\NormalTok{,                    }\CommentTok{# Number of classes}
              \DataTypeTok{max_depth =} \DecValTok{4}\NormalTok{,                    }\CommentTok{# Maximum depth of trees}
              \DataTypeTok{nrounds =} \DecValTok{10}\NormalTok{,                     }\CommentTok{# Number of trees used}
              \DataTypeTok{verbose =} \DecValTok{0}                       \CommentTok{# No warning message }
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\normalsize

We can then proceed to the assessment of the quality of the model. We
adjust the prediction to the value of the true label and count the
proportion of accurate forecasts.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{predict}\NormalTok{(fit_xgb_C, xgb_test) }\OperatorTok{+}\StringTok{ }\DecValTok{1} \OperatorTok{==}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd_C)) }\CommentTok{# Hit ratio}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.495613
\end{verbatim}

\normalsize

Consistently with the previous classification attempts, the results are
underwhelming, as if switching to binary labels incurred a loss of
information.

\hypertarget{instweight}{%
\subsection{Instance weighting}\label{instweight}}

In the computation of the aggregate loss, it is possible to introduce
some flexibility and assign weight to instances:
\[O=\underbrace{\sum_{i=1}^I\mathcal{W}_i \times \text{loss}(y_i,\tilde{y}_i)}_{\text{weighted error term}} \quad + \underbrace{\sum_{j=1}^J\Omega(T_j)}_{\text{regularisation term (unchanged)}}.\]

In factor investing these weights can very well depend on the feature
values (\(\mathcal{W}_i=\mathcal{W}_i(\textbf{x}_i)\)). For instance,
for one particular characteristic \(\textbf{x}^k\), weights can be
increasing thereby giving more importance to assets with high values of
this characteristic (e.g., value stocks are favored compared to growth
stocks). One other option is to increase weights when the values of the
characteristic become more extreme (deep value and deep growth stocks
have larger weight). If the features are uniform, the weights can simply
be \(\mathcal{W}_i(x_i^k)\propto|x_i^k-0.5|\): firms with median value
0.5 have zero weight and as the feature value shifts towards 0 or 1, the
weight increases. Specifying weights on instances biases the learning
process just like views introduced à la \citet{black1992global}
influence the asset allocation process. The difference is that the nudge
is performed well ahead of the portfolio choice problem.

In xgboost, the implementation instance weighting is done very early, in
the definition of the xgb.DMatrix:

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{inst_weights <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\KeywordTok{nrow}\NormalTok{(train_features_xgb))               }\CommentTok{# Random weights}
\NormalTok{inst_weights <-}\StringTok{ }\NormalTok{inst_weights }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(inst_weights)              }\CommentTok{# Normalization}
\NormalTok{train_matrix_xgb <-}\StringTok{ }\KeywordTok{xgb.DMatrix}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ train_features_xgb, }
                                \DataTypeTok{label =}\NormalTok{ train_label_xgb,}
                                \DataTypeTok{weight =}\NormalTok{ inst_weights)        }\CommentTok{# Weights!}
\end{Highlighting}
\end{Shaded}

\normalsize

Then, in the subsequent stages, the optimization will be performed with
these hard-coded weights. The splitting points can be altered (via the
total weighted loss in clusters) and the terminal weight values
\eqref{eq:xgbweight} are also impacted.

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

We end this chapter by a discussion on the choice of predictive engine
with a view towards portfolio construction. As recalled in Chapter
\ref{intro}, the ML signal is just one building stage of construction of
the investment strategy. At some point, this signal must be translated
into portfolio weights.

From this perspective, simple trees appear suboptimal. Tree depth are
usually set between 3 and 6. This implies between 8 and 64 terminal
leaves at most, with possibly very unbalanced clusters. The likelihood
of having one cluster with 20\% to 30\% of the sample is high. This
means that when it comes to predictions, roughly 20\% to 30\% of the
instances will be given the same value.

On the other side of the process, portfolio policies commonly have a
fixed number of assets. Thus, having assets with equal signal does not
permit to discriminate and select a subset to be included in the
portfolio. For instance, if the policy requires exactly 100 stocks and
105 stocks have the same signal, the signal cannot be used for selection
purposes. It would have to be combined with exogenous information such
as the covariance matrix in a mean-variance type allocation.

Overall, this is one reason to prefer aggregate models. When the number
of learners is sufficiently large (5 is almost enough), the predictions
for assets will be unique and tailored to these assets. It then becomes
possible to discriminate via the signal and select only those assets
that have the most favorable signal. In practice, random forests and
boosted trees are probably the best choices.

\hypertarget{coding-exercises-3}{%
\section{Coding exercises}\label{coding-exercises-3}}

\hypertarget{NN}{%
\chapter{Neural networks}\label{NN}}

Neural networks (NN) are an immensely rich and complicated topic. In
this chapter, we introduce the simple ideas and concept behind the most
simple architectures of NN. For more exhaustive treatments on NN
idiosyncracies, we refer to the monographs \citet{haykin2009neural},
\citet{du2013neural} and \citet{goodfellow2016deep}. The latter is
available freely online: www.deeplearningbook.org. For a practical
introduction, we recommend the great book of \citet{chollet2017deep}.

For starters, we briefly comment on the qualification ``neural
network''. Most experts agree that the term is not very well chosen, as
NN have little to do with how the human brain works (of which we know
not that much). This explains why they are often referred to as
``artificial neural networks'' - we do not use the adjective for
notational simplicity. Because we consider it more appropriate, we
recall the definition of NN given by François Chollet: ``\emph{chains of
differentiable, parameterised geometric functions, trained with gradient
descent (with gradients obtained via the chain rule)}''.

Early references of neural networks in finance are \citet{bansal1993no}
and \citet{eakins1998analyzing}. Both have very different goals. In the
first one, the authors aims to estimate a nonlinear form for the pricing
kernel. In the second one, the purpose is to identify and quantify
relationships between institutional investments in stocks and the
attributes of the firms (an early contribution towards factor
investing). An early review (\citet{burrell1997impact}) lists financial
applications of NN during the 1990s decade.

The pure predictive ability of NN in financial markets is a popular
subject and we cite for example \citet{kimoto1990stock},
\citet{enke2005use}, \citet{zhang2009stock} and
\citet{guresen2011using}.\footnote{Neural networks have also been
  recently applied to derivatives pricing and hedging, see for instance
  the work of \citet{buehler2019deep} and the survey by
  \citet{ruf2019neural}.} This list is very far from exhaustive. More
recent studies on neural networks include:

\begin{itemize}
\item
  \citet{feng2019deep} use neural network to find factors that are the
  best at explaining the cross-section of stock returns.\\
\item
  \citet{gu2018empirical} map firm attributes and macro-economic
  variables into future returns. This creates a strong predictive tool
  that is able to forecast future returns very accurately.
\item
  \citet{chen2019deep} estimate the pricing kernel with a complex neural
  network structure including a generative adversarial network. This
  again gives crucial information on the structure of expected stock
  returns and can be used for portfolio construction (by building an
  accurate maximum Sharpe ratio policy).
\end{itemize}

\hypertarget{the-original-perceptron}{%
\section{The original perceptron}\label{the-original-perceptron}}

The origins of NN go back at least to \citet{rosenblatt1958perceptron}.
Its aim is binary classification. For simplicity, let us assume that the
output is \(\{0\) = do not invest\(\}\) versus \(\{1\) = invest\(\}\)
(e.g., derived from return, negative versus positive). Given the current
nomenclature, a perceptron can be defined as an activated linear
mapping. The model is the following:

\[f(\mathbf{x})=\left\{ \begin{array}{lll}
1 & \text{if } \mathbf{x}'\mathbf{w}+b >0\\
0  &\text{otherwise}
\end{array}\right.\] The vector of weights \(\mathbf{w}\) scales the
variables and the bias \(b\) shifts the decision barrier. Given values
for \(b\) and \(w_i\), the error is
\(\epsilon_i=y_i-1_{\left\{\sum_{j=1}^Jx_{i,j}w_j+w_0>0\right\}}\). As
is customary, we set \(b=w_0\) and add an initial constant column to
\(x\): \(x_{i,0}=1\), so that
\(\epsilon_i=y_i-1_{\left\{\sum_{j=0}^Jx_{i,j}w_j\right\}}\). In
contrast to regressions, perceptrons do not have closed-form solutions.
The optimal weights can only be approximated. Just like for regression,
one way to derive good weights is to minimize the sum of squared errors.
To this purpose, the simplest way to proceed is to

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  compute the current model value at point \(\textbf{x}_i\):
  \(\tilde{y}_i=1_{\left\{\sum_{j=0}^Jw_jx_{i,j}>0\right\}}\),
\item
  adjust the weight vector:
  \(w_j \leftarrow w_j + \eta (y_i-\tilde{y}_i)x_{i,j}\),
\end{enumerate}

which amounts to shifting the weights in the \textit{right} direction.
Just like for tree methods, the scaling factor \(\eta\) is the learning
rate. A large \(\eta\) will imply large shifts: learning will be rapid
but convergence may be slow or may even not occur. A small \(\eta\) is
usually preferable as it helps reduce the risk of overfitting.

In Figure \ref{fig:perceptron}, we illustrate this mechanism. The
initial model (dashed grey line) was trained on 7 points (3 red and 4
blue). A new black point comes in.

\begin{figure}[H]

{\centering \includegraphics[width=450px]{images/NN_percep_scheme} 

}

\caption{Scheme of a perceptron.}\label{fig:perceptron}
\end{figure}

\begin{itemize}
\tightlist
\item
  if the point is red, there is no need for adjustment: it is labelled
  correctly as it lies on the right side of the border.\\
\item
  if the point is blue, then the model needs to be updated
  appropriately. Given the rule mentioned above, this means adjusting
  the slope of the line downwards. Depending on \(\eta\), the shift will
  be sufficient to change the classification of the new point - or not.
\end{itemize}

At the time of its inception, the perceptron was an immense breakthrough
which received an intense media coverage (see
\citet{olazaran1996sociological} and \citet{anderson2000talking}). Its
rather simple structure was progressively generalized to networks
(combinations) of perceptrons. Each one of them is a simple unit and
units are gathered into layers. The next section describes the
organization of simple multilayer perceptrons.

\hypertarget{multilayer-perceptron}{%
\section{Multilayer perceptron}\label{multilayer-perceptron}}

\hypertarget{introduction-and-notations}{%
\subsection{Introduction and
notations}\label{introduction-and-notations}}

A perceptron can be viewed as a linear model to which is applied a
particular function: the Heaviside (step) function. Other choices of
functions are naturally possible. In the NN jargon, they are called
activation functions. Their purpose is to introduce nonlinearity in
otherwise very linear models.

Just like for random forest, the idea behind neural networks is to
combine perceptron-like building blocks. A popular representation of
neural networks is shown in Figure \ref{fig:NNnaive}. This scheme is
overly simplistic. It hides what is really going on: there is a
perceptron in each green circle and each output is activated by some
function before it is sent to the final output aggregation.

\begin{figure}[H]

{\centering \includegraphics[width=480px]{images/nn} 

}

\caption{Simplified scheme of a multi-layer perceptron.}\label{fig:NNnaive}
\end{figure}

A more faithful account of what is going on is laid out in Figure
\ref{fig:MLperceptron}.

\begin{figure}[H]

{\centering \includegraphics[width=16.53in]{images/NN_scheme} 

}

\caption{Detailed scheme of a perceptron with 2 intermediate layers.}\label{fig:MLperceptron}
\end{figure}

Before we proceed with comments, we introduce some notation that will be
used thoughout the chapter.

\begin{itemize}
\tightlist
\item
  The data is separated into a matrix \(\textbf{X}=x_{i,j}\) of features
  and a vector of output values \(\textbf{y}=y_i\). \(\textbf{x}\) or
  \(\textbf{x}_i\) denotes one line of \(\textbf{X}\).
\item
  A neural network will have \(L\ge1\) layers and for each layer \(l\),
  the number of units is \(U_l\ge1\).
\item
  The weights for unit \(k\) located in layer \(l\) are denoted with
  \(\textbf{w}_{k}^{(l)}=w_{k,j}^{(l)}\) and the corresponding biases
  \(b_{k}^{(l)}\). The length of \(\textbf{w}_{k}^{(l)}\) is equal to
  \(U_{l-1}\). \(k\) refers to the location of the unit in layer \(l\)
  while \(j\) to the unit in layer \(l-1\).
\item
  Outputs (post activation) are denoted \(o_{i,k}^{(l)}\) for instance
  \(i\), layer \(l\) and unit \(k\).
\end{itemize}

The process is the following. When entering the network, the data goes
though the initial linear mapping:\\
\[v_{i,k}^{(1)}=\textbf{x}_i'\textbf{w}^{(1)}_k+b_k^{(1)},  \text{for } l=1, \quad k \in [1,U_1],  \]\\
and this linear transformation will be repeated (with different weights)
for each layer of the network:
\[v_{i,k}^{(l)}=(\textbf{o}^{(l-1)}_i)'\textbf{w}^{(l)}_k+b_k^{(l)}, \text{for } l \ge 2,  \quad k \in [1,U_l]. \]\\
The connexions between the layers are the so-called outputs, which are
basically the linear mappings to which the activation functions have
been applied. The output of layer \(l\) is the input of layer \(l+1\).
\[o_{i,k}^{(l)}=f^{(l)}\left(v_{i,k}^{(l)}\right).\]\\
Finally, the terminal stage aggregates the outputs from the last
layer:\\
\[\tilde{y}_i =f^{(L+1)} \left((\textbf{o}^{(L)}_i)'\textbf{w}^{(L+1)}+b^{(L+1)}\right).\]

In the forward propagation of the input, the activation function
naturally play an important role. In Figure \ref{fig:activationf}, we
plot the most usual activation function used by neural network
libraries.

\begin{figure}[H]

{\centering \includegraphics[width=11.81in]{images/activation} 

}

\caption{Plot of most common activation functions.}\label{fig:activationf}
\end{figure}

Let us rephrase the process through the lens of factor investing. The
input \(\textbf{x}\) are the characteristics of the firms. The first
step is to multiply their value by weights and add a bias. This is
performed for all the units of the first layer. The output, which is a
linear combination of the input is then transformed by the activation
function. Each unit provides one value and all of these values are fed
to the second layer following the same process. This is iterated until
the end of the network. The purpose of the last layer is to yield an
output shape that corresponds to the label: if the label is numerical,
the output is a single number, if it is categorical, then usually it is
a vector with length equal to the number of categories.

It is possible to use a final activation function after the output. This
can have a huge importance on the result. Indeed, if the labels are
returns, applying a sigmoid function at the very end will be disastrous
because the sigmoid is always positive.

\hypertarget{universal-approximation}{%
\subsection{Universal approximation}\label{universal-approximation}}

One reason neural networks work well is that they are
\textit{universal approximators}. Given any bounded continuous function,
there exists a one-layer network that can approximate this function up
to arbitrary precision (see \citet{cybenko1989approximation} and for
early references, and section 4.2 in \citet{du2013neural} and section
6.4.1 in \citet{goodfellow2016deep} for more exhaustive lists of papers
and \citet{guliyev2018approximation} for recent results).

Formally, a one layer perceptron is defined by
\[f_n(\textbf{x})=\sum_{l=1}^nc_l\phi(\textbf{x}\textbf{w}_l+\textbf{b}_l)+c_0,\]
where \(\phi\) is a (non constant) bounded continuous function. Then,
for any \(\epsilon>0\), it is possible to find one \(n\) such that for
any continuous function \(f\) on the unit hypercube \([0,1]^d\),
\[|f(\textbf{x})-f_n(\textbf{x})|< \epsilon, \quad \forall \textbf{x} \in [0,1]^d.\]

This result is rather intuitive: it suffices to add units to the layer
to improve the fit. The process is more or less analogous to polynomial
approximation, though some subtleties arise depending on the properties
of the activations functions (boundedness, smoothness, convexity, etc.).
We refer to \citet{costarelli2016survey} for a survey on this topic.

The raw results on universal approximation imply that any well behaved
function \(f\) can be approached sufficiently closely by a simple neural
network, as long as the number of units can be arbitrarily large. Now,
they do not directly relate to the learning phase, i.e., when the model
is optimized with respect to a particular dataset. In a series of papers
(\citet{barron1993universal} and \citet{barron1994approximation}
notably), Barron gives a much more precise characterization of what
neural networks can achieve. In \citet{barron1993universal} it is for
instance proved a more precise verion of universal approximation: for
particular neural networks (with sigmoid activation),
\(\mathbb{E}[(f(\textbf{x})-f_n(\textbf{x}))^2]\le c_f/n\), which gives
a speed of convergence related to the size of the network. In the
expectation, the random term is \(\textbf{x}\): this corresponds to the
case where the data is considered to be a sample of i.i.d. observations
of a fixed distribution (this is the most common assumption in machine
learning).

Below, we state one important result that is easy to interpret; it is
taken from \citet{barron1994approximation}.

In the sequel, \(f_n\) corresponds to a possibly penalized neural
network with only one intermediate layer with \(n\) units and sigmoid
activation function. Moreover, both the supports of the predictors and
the label are assumed to be bounded (which is not a major constraint).
The most important metric in a regression exercise is the mean squared
error (MSE) and the main result is a bound (in order of magnitude) on
this quantity. For \(N\) randomly sampled i.i.d. points
\(y_i=f(x_i)+\epsilon_i\) on which \(f_n\) is trained, the best possible
empirical MSE behaves like

\begin{equation}
\label{eq:univapprox}
\mathbb{E}\left[(f(x)-f_n(x))^2 \right]=\underbrace{O\left(\frac{c_f}{n} \right)}_{\text{size of network}}+\ \underbrace{O\left(\frac{nK \log(N)}{N} \right)}_{\text{size of sample}},
\end{equation} where \(K\) is the dimension of the input (number of
columns) and \(c_f\) is a constant that depends on the generator
function \(f\). The above quantity provides a bound on the error that
can be achieved by the best possible neural network given a dataset of
size \(N\).

There are clearly two components in the decomposition of this bound. The
first one pertains to the complexity of the network. Just as in the
original unviversal approximation theorem, the error decreases with the
number of units in the network. But this is not enough! Indeed, the
sample size is of course a key driver in the quality of learning (of
i.i.d. observations). The second component of the bound indicates that
the error decreases at a slightly slower pace with respect to the number
of observations (\(\log(N)/N\)). and is linear in the number of units
and the size of the input. This clearly underlines the link (trade-off?)
between sample size and model complexity: having a very complex model is
useless if the sample is small just like a simple model will not catch
the fine relationships in a large dataset.

Overall, a neural network is a possibly very complicated function with a
lot of parameters. In linear regressions, it is possible to increase the
fit by spuriously adding exogenous variables. In neural networks, it
suffices to increase the number of parameters by arbitrarily adding
units to the layer(s). This is of course a very bad idea because
high-dimensional networks will mostly capture the particularities of the
sample they are trained on.

\hypertarget{backprop}{%
\subsection{Learning via back-propagation}\label{backprop}}

Just like for tree methods, neural networks are trained by minimizing
some loss function subject to some penalization:
\[O=\sum_{i=1}^I \text{loss}(y_i,\tilde{y}_i)+ \text{penalization},\]
where \(\tilde{y}_i\) are the values obtained by the model and \(y_i\)
are the \emph{true} values of the instances. A simple requirement that
eases computation is that the loss function be differentiable. The most
common choices are the squared error for regression tasks and
cross-entropy for classification tasks. We discuss the technicalities of
classification in the next subsection.

The training of a neural network amounts to alter the weights (and
biases) of all units in all layers so that \(O\) defined above is the
smallest possible. To ease the notation and given that the \(y_i\) are
fixed, let us write
\(D(\tilde{y}_i(\textbf{W}))=\text{loss}(y_i,\tilde{y}_i)\), where
\(\textbf{W}\) denotes the entirety of weights and biases in the
network. The updating of the weights will be performed via gradient
descent, i.e., via

\begin{equation}
\label{eq:graddesc}
\textbf{W} \leftarrow \textbf{W}-\eta  \frac{\partial D(\tilde{y}_i) }{\partial \textbf{W}}.
\end{equation}

This mechanism is the most classical in the optimization literature and
we illustrate it in Figure \ref{fig:newton}. We highlight the possible
suboptimality of large learning rates. In the diagram, the descent
associated with the high \(\eta\) will oscillate around the optimal
point whereas the one related to the small eta will converge more
directly.

The complicated task in the above equation is to compute the gradient
(derivative) which tell in which direction the adjustment should be
done. The problem is that the successive nested layers and associated
activations require many iterations of the chain rule for
differentiation.

\begin{figure}[H]

{\centering \includegraphics[width=480px]{images/Newton} 

}

\caption{Outline of gradient descent.}\label{fig:newton}
\end{figure}

The most common way to approximate a derivative is probably the finite
difference method. Under the usual assumptions (the loss is twice
differentiable), the centered difference satisfies:

\[\frac{\partial D(\tilde{y}_i(w_k))}{\partial w_k} = \frac{D(\tilde{y}_i(w_k+h))-D(\tilde{y}_i(w_k-h))}{2h}+O(h^2),\]
where \(h>0\) is some arbitrarily small number. Inspite of its apparent
simplicity, this method is costly computationally because it requires a
number of operations of the magnitude of the number of weights.

Luckily, there is a small trick that can considerably ease and speed up
the computation. The idea is to simply follow the chain rule and recycle
terms along the way. Let us start by recalling
\[\tilde{y}_i =f^{(L+1)} \left((\textbf{o}^{(L)}_i)'\textbf{w}^{(L+1)}+b^{(L+1)}\right)=f^{(L+1)}\left(b^{(L+1)}+\sum_{k=1}^{U_L} w^{(L+1)}_ko^{(L)}_{i,k} \right),\]
so that if we differentiate with the most immediate weights and biases,
we get: \begin{align}
\frac{\partial D(\tilde{y}_i)}{\partial w_k^{(L+1)}}&=D'(\tilde{y}_i) \left(f^{(L+1)} \right)'\left( b^{(L+1)}+\sum_{k=1}^{U_L} w^{(L+1)}_ko^{(L)}_{i,k}  \right)o^{(L)}_{i,k} \\   \label{eq:backprop1}
&= D'(\tilde{y}_i) \left(f^{(L+1)} \right)'\left( v^{(L+1)}_{i,k}  \right)o^{(L)}_{i,k} \\
\frac{\partial D(\tilde{y}_i)}{\partial b^{(L+1)}}&=D'(\tilde{y}_i) \left(f^{(L+1)} \right)'\left( b^{(L+1)}+\sum_{k=1}^{U_L} w^{(L+1)}_ko^{(L)}_{i,k}  \right). 
\end{align}

This is the easiest part. We must now go back one layer and this can
only be done via the chain rule. To access layer \(L\), we recall
identity
\(v_{i,k}^{(L)}=(\textbf{o}^{(L-1)}_i)'\textbf{w}^{(L)}_k+b_k^{(L)}=b_k^{(L)}+\sum_{j=1}^{U_L}o^{(L-1)}_{i,j}w^{(L)}_{k,j}\).
We can then proceed:

\begin{align}
\frac{\partial D(\tilde{y}_i)}{\partial w_{k,j}^{(L)}}&=\frac{\partial D(\tilde{y}_i)}{\partial v^{(L)}_{i,k}}\frac{\partial v^{(L)}_{i,k}}{\partial w_{k,j}^{(L)}} = \frac{\partial D(\tilde{y}_i)}{\partial v^{(L)}_{i,k}}o^{(L-1)}_{i,j}\\
&=\frac{\partial D(\tilde{y}_i)}{\partial o^{(L)}_{i,k}} \frac{\partial o^{(L)}_{i,k} }{\partial v^{(L)}_{i,k}}  o^{(L-1)}_{i,j} = \frac{\partial D(\tilde{y}_i)}{\partial o^{(L)}_{i,k}}  (f^{(L)})'(v_{i,k}^{(L)})  o^{(lL1)}_{i,j} \\
&=\underbrace{D'(\tilde{y}_i) \left(f^{(L+1)} \right)'\left(v^{(L+1)}_{i,k}  \right)}_{\text{computed above!}} w^{(L+1)}_k (f^{(L)})'(v_{i,k}^{(L)})  o^{(L-1)}_{i,j},
\end{align}

where as we show in the last line, one part of the derivative was
already computed in the previous step (Equation \eqref{eq:backprop1}).
Hence, we can recycle this number and only focus on the right part of
the expression.

The magic of the so-called backpropagation is that this will hold true
for each step of the differentiation. When computing the gradient for
weights and biases in layer \(l\), there will be two parts: one that can
be recycled from previous layers and another, local part, which depends
only on the values and activation function of the current layer. A nice
illustration of this process is given by the Google developer team:
\url{https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/}

When the data is formatted using tensors, it is possible to resort to
vectorization so that the number of calls is limited to an order of the
magnitude of the number of nodes (units) in the network.

The backpropagation algorithm can be summarized as follows. Given a
sample of points (possibly just one):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the data flows from left as is described in Figure
  \ref{fig:MLperceptron};\\
\item
  this allows the computation of the error or loss function;\\
\item
  all derivatives of this function (w.r.t. weights and biases) are
  computed, starting from the last layer and diffusing to the left
  (hence the term backpropagation);\\
\item
  all weights and biases can be updated to take the sample points into
  account (the model is adjusted to reduce the loss/error stemming from
  these points).
\end{enumerate}

This operation can be performed any number of times with different
sample sizes. We discuss this issue in Section \ref{howdeep}.

The learning rate \(\eta\) can be refined. One option to reduce
overfitting is to impose that after each epoch, the intensity of the
update decreases. One possible parametric form is
\(\eta=\alpha e^{- \beta t}\), where \(t\) is the epoch and
\(\alpha,\beta>0\). One further sophistication is to resort to so-called
\emph{momentum} (which originates from \citet{polyak1964some}):
\begin{align}
\label{eq:gradmom}
\textbf{W}_{t+1} & \leftarrow  \textbf{W}_{t} - \textbf{m}_t \quad \text{with} \nonumber \\
 \textbf{m}_t & \leftarrow \eta  \frac{\partial D(\tilde{y}_i)}{\partial \textbf{W}_{t}}+\gamma \textbf{m}_{t-1},
\end{align} where \(t\) is the index of the weight update. The idea of
momentum is to speed up the convergence by including a memory term of
the last adjustment (\(\textbf{m}_{t-1}\)) and going in the same
direction in the current update. The parameter \(\gamma\) is often taken
to be 0.9.

More complex and enhanced method have progressively developed:\\
- \citet{nesterov1983method} improves the momentum term by forecasting
the future shift in parameters;\\
- Adagrad (\citet{duchi2011adaptive}) uses a different learning rate for
each parameter;\\
- Adadelta (\citet{zeiler2012adadelta}) and Adam
(\citet{kingma2014adam}) combine the ideas of Adagrad and momentum.

Lastly, in some degenerate case, some gradients may explode and push
weights far from their optimal values. In order to avoid this
phenomenon, learning libraries implement gradient clipping. The user
specifies a maximum magnitude for gradients, usually expressed as a
norm. Whenever the gradient surpasses this magnitude, it is rescaled to
reach the authorized threshold. Thus, the direction remains the same,
but the adjustment is smaller.

\hypertarget{further-details-on-classification-1}{%
\subsection{Further details on
classification}\label{further-details-on-classification-1}}

In decision trees, the ultimate goal is to create homogeneous clusters,
and the process to reach this goal was outlined in the previous chapter.
For neural networks, things work differently because the objective is
explicitly to minimize the error between the prediction
\(\tilde{\textbf{y}}_i\) and a target label \(\textbf{y}_i\). Again,
here \(\textbf{y}_i\) is a vector full of zeros with only one \emph{one}
denoting the class of the instance.

Facing a classification problem, the trick is to use an appropriate
activation function at the very end of the network. The dimension of the
terminal output of the network should be equal to \(J\) (number of
classes to predict), and if, for simplicity, we write \(\textbf{x}_i\)
for the values of this output, the most commonly used activation is the
so-called \emph{softmax} function:

\[\tilde{\textbf{y}}_i=s(\textbf{x})_i=\frac{e^{x_i}}{\sum_{j=1}^Je^{x_j}}.\]

The justification of this choice is straightforward: it can take any
value as input (over the real line) and it sums to one over any
(finite-valued) output. Similarly as for trees, this yields a
`probability' vector over the classes. Often, the chosen loss is a
generalization of the entropy used for trees. Given the target label
\(\textbf{y}_i=(y_{i,1},\dots,y_{i,L})=(0,0,\dots,0,1,0,\dots,0)\) and
the predicted output
\(\tilde{\textbf{y}}_i=(\tilde{y}_{i,1},\dots,\tilde{y}_{i,L})\), the
cross-entropy is defined as

\begin{equation}
\label{eq:crossentropy}
\text{CE}(\textbf{y}_i,\tilde{\textbf{y}}_i)=-\sum_{j=1}^J\log(\tilde{y}_{i,j})y_{i,j}.
\end{equation}

Basically, it is a proxy of the dissimilarity between its two arguments.
One simple interpretation is the following. For the nonzero label value,
the loss is \(-\log(\tilde{y}_{i,l})\), while for all others, it is
zero. In the log, the loss will be minimal if \(\tilde{y}_{i,l}=1\),
which is exactly what we seek (i.e., \(y_{i,l}=\tilde{y}_{i,l}\)). In
applications, this best case scenario will not happen, and the loss will
simply increases when \(\tilde{y}_{i,l}\) drifts away downwards from
one.

\hypertarget{howdeep}{%
\section{How deep should we go? And other practical
issues}\label{howdeep}}

Beyond the ones presented in the previous sections, the user faces many
degrees of freedom when building a neural network. We present a few
classical choices that are available when constructing and training
neural networks.

\hypertarget{architectural-choices}{%
\subsection{Architectural choices}\label{architectural-choices}}

Arguably, the first choice pertains to the structure of the network.
Beyond the dichotomy feed-forward versus recurrent (see Section
\ref{recurrent-networks}), the immediate question is: how big (or how
deep) the networks should be. First of all, let us calculate the number
of parameters (i.e., weights plus biases) are estimated (optimized) in a
network.

\begin{itemize}
\tightlist
\item
  For the first layer, this gives \((U_0+1)U_1\) parameters, where
  \(U_0\) is the number of columns in \(\mathbb{X}\) (i.e., number of
  explanatory variables) and \(U_1\) is the number of units in the
  layer.\\
\item
  For layer \(l\in[2,L]\), the number of parameters is
  \((U_{l-1}+1)U_l\).\\
\item
  For the final output, there are simply \(U_L+1\) parameters.\\
\item
  In total, this means the total number of values to optimise is
  \[\mathcal{N}=\left(\sum_{l=1}^L(U_{l-1}+1)U_l\right)+U_L+1\]
\end{itemize}

As in any model, the number of parameters should be much smaller than
the number of instances. There is no fixed ratio, but it is preferable
if the sample size is \emph{at least} ten times larger than the number
of parameters. Below a ratio of 5, the risk of overfitting is high.
Given the amount of data readily available, this constraint is seldom an
issue, unless one wishes to work with a very large network.

The number of layers in current financial applications rarely exceed
three or four. The number of units per layer is often chosen to follow
the geometric pyramid rule (see, e.g., \citet{masters1993practical}). If
there are \(L\) hidden layers, with \(I\) features in the input and
\(O\) dimensions in the output (for regression tasks, \(O=1\)), then,
for the \(k^{th}\) layer, a rule of thumb for the number of units is
\[U_k\approx \left\lfloor O\left( \frac{I}{O}\right)^{\frac{L+1-k}{L+1}}\right\rfloor.\]
If there is only one intermediate layer, the recommended proxy is the
integer part of \(\sqrt{IO}\). If not, the network starts with many
units and the number of unit decreases exponentially towards the output
size.

Several studies have shown that very large architectures do not always
perform better than more shallow ones (e.g., \citet{gu2018empirical} and
\citet{orimoloye2019comparing} for high frequency data, i.e., not
factor-based). As a rule of thumb, a maximum of three hidden layers seem
to be sufficient for prediction purposes.

\hypertarget{frequency-of-weight-updates-and-learning-duration}{%
\subsection{Frequency of weight updates and learning
duration}\label{frequency-of-weight-updates-and-learning-duration}}

In the expression \eqref{eq:graddesc}, it is implicit that the computation
is performed for one given instance. If the sample size is very large
(hundreds of thousands or millions of instances), updating the weights
according to each point is computationally too costly. The updating is
then performed on groups of instances which are called batches. The
sample is (randomly) split into batches of fixed sizes and each update
is performed following the rule:

\begin{equation}
\label{eq:gradbatch}
\textbf{W} \leftarrow \textbf{W}-\eta  \frac{\partial \sum_{i \in \text{batch}} D(\tilde{y}_i)/\text{card}(\text{batch}) }{\partial \textbf{W}}.
\end{equation}

The change in weights is computed over the average loss computed over
all instances in the batch. The terminology for training includes:

\begin{itemize}
\tightlist
\item
  \textbf{epoch}: one epoch is reached when each instance of the sample
  has contributed to the update of the weights (i.e., the training).
  Often, training a NN requires several epochs and up to a few dozen.
\item
  \textbf{batch size}: the batch size is the number of samples used for
  one single update of weights.
\item
  \textbf{iterations}: the number of iterations can mean alternatively
  the ratio of sample size divided by batch size or this ratio
  multiplied by the number of epochs. It's either the number of weight
  updates required to reach one epoch or the total number of updates
  during the whole training.
\end{itemize}

When the batch is equal to only one instance, the method is referred to
as `stochastic gradient descent' (SGD): the instance can be chosen
randomly. When the batch size is strictly above one and below the total
number of instances, the learning is performed via `mini' batches.

\hypertarget{penalizations-and-dropout}{%
\subsection{Penalizations and dropout}\label{penalizations-and-dropout}}

At each level (layer), it is possible to enforce constraints or
penalizations on the weights (and biases). Just as for tree methods,
this helps slow down the learning to prevent overfitting on the training
sample. Penalizations are enforced directly on the loss function and the
objective function takes the form

\[O=\sum_{i=1}^I \text{loss}(y_i,\tilde{y}_i)+ \sum_{k} \lambda_k||\textbf{W}_k||_1+ \sum_j\delta_k||\textbf{W}_j||_2^2,\]
where the subscripts \(k\) and \(j\) pertain to the weights to which the
\(L^1\) and (or) \(L^2\) penalization is applied.

In addition, specific constraints can be enforced on the weights
directly during the training. Typically, two types of constraints are
used:

\begin{itemize}
\tightlist
\item
  norm constraints: a maximum norm is fixed for the weight vectors or
  matrices;\\
\item
  non-negativity constraint: all weights must be positive or zero.
\end{itemize}

Lastly, another (somewhat exotic) way to reduce the risk of overfitting
is simply to reduce the size (number of parameters) of the model.
\citet{srivastava2014dropout} propose to omit units during training
(hence the term `\textbf{dropout}'). The weights of randomly chosen
units are set to zero during training. All links from and to the unit
are ignored, which mechanically shrinks the network. In the testing
phase, all units are back, but the values (weights) must be scaled to
account for the missing activations during the training phase.

\hypertarget{code-samples-and-comments-for-vanilla-mlp}{%
\section{Code samples and comments for vanilla
MLP}\label{code-samples-and-comments-for-vanilla-mlp}}

There are several frameworks and libraries that allow robust and
flexible constructions of neural networks. Among them, Keras and
Tensorflow (developed by Google) are probably the most used at the time
we write this book (PyTorch, from Facebook, is one alternative). For
simplicity and because we believe it is the best choice, we implement
the NN with Keras (which is the high level API of Tensorflow, see
\url{https://www.tensorflow.org}). The original Python implementation is
reference on \url{https://keras.io}, and the details for the R version
can be found here: \url{https://keras.rstudio.com}. We recommend a
thorough installation before proceeeding. Because the native versions of
Tensorflow and Keras are written in Python (and accessed by R via the
\emph{reticulate} package), a running version of Python is required
below. To install Keras, please follow the instructions provided at
\url{https://keras.rstudio.com}.

In this section, we provide an detailed (though far from exhaustive)
account of how to train neural networks with Keras. For the sake of
completeness, we proceed in two steps. The first one relates to a very
simple regression exercise. Its purpose is to get the reader familiar
with the syntax of Keras. In the second step, we lay out many of the
options proposed by keras to perform a classification exercise. With
these two examples, we thus cover most of the mainstream topics falling
under the umbrella of feed-forward multilayered perceptrons.

\hypertarget{regression-example}{%
\subsection{Regression example}\label{regression-example}}

Before we head to the core of the NN, a short stage of data preparation
is required. Just as for penalized regressions (glmnet package) and
boosted trees (xgboost package), the data must be sorted into four parts
which are the combination of two dichotomies: training versus testing
and labels versus features. We define the corresponding variables below.
For simplicity, the first example is a regression exercise. A
classification task will be detailed below.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NN_train_features <-}\StringTok{ }\KeywordTok{select}\NormalTok{(training_sample, features) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{() }\CommentTok{# Matrix = important}
\NormalTok{NN_train_labels <-}\StringTok{ }\NormalTok{training_sample}\OperatorTok{$}\NormalTok{R1M_Usd}
\NormalTok{NN_test_features <-}\StringTok{ }\KeywordTok{select}\NormalTok{(testing_sample, features) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{()   }\CommentTok{# Matrix = important}
\NormalTok{NN_test_labels <-}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd}
\end{Highlighting}
\end{Shaded}

\normalsize

In Keras, the training of neural networks is performed through three
steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Defining the structure/architecture of the network;\\
\item
  Setting the loss function and learning process (options on the
  updating of weights)
\item
  Train by specifying the batch sizes and number of rounds (epochs)
\end{enumerate}

We start with a very simple architecture with two hidden layers.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(keras)}
\CommentTok{# install_keras() # To complete installation}
\NormalTok{model <-}\StringTok{ }\KeywordTok{keras_model_sequential}\NormalTok{()}
\NormalTok{model }\OperatorTok{%>%}\StringTok{   }\CommentTok{# This defines the structure of the network, i.e. how layers are organized}
\StringTok{    }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{16}\NormalTok{, }\DataTypeTok{activation =} \StringTok{'relu'}\NormalTok{, }\DataTypeTok{input_shape =} \KeywordTok{ncol}\NormalTok{(NN_train_features)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{8}\NormalTok{, }\DataTypeTok{activation =} \StringTok{'sigmoid'}\NormalTok{) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{1}\NormalTok{) }\CommentTok{# No activation means linear activation: f(x) = x.}
\end{Highlighting}
\end{Shaded}

\normalsize

The definition of the structure is very intuitive and uses the
\emph{sequential} syntax in which one input is iteratively transformed
by a layer until the last iteration which gives the output. Each layer
depends on two parameters: the number of layers and the activation
function that is applied to the output of the layer. One important point
is the input\_shape parameter for the first layer. It is required for
the first layer and is equal to the number of features. For the
subsequent layers, the input\_shape is dictated by the number of units
of the previous layer; hence it is not required. The activations that
are currently available are listed on
\url{https://keras.io/activations/}.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OperatorTok{%>%}\StringTok{ }\KeywordTok{compile}\NormalTok{(                             }\CommentTok{# Model specification}
    \DataTypeTok{loss =} \StringTok{'mean_squared_error'}\NormalTok{,               }\CommentTok{# Loss function}
    \DataTypeTok{optimizer =} \KeywordTok{optimizer_rmsprop}\NormalTok{(),           }\CommentTok{# Optimisation method (weight updating)}
    \DataTypeTok{metrics =} \KeywordTok{c}\NormalTok{(}\StringTok{'mean_absolute_error'}\NormalTok{)         }\CommentTok{# Output metric}
\NormalTok{)}
\KeywordTok{summary}\NormalTok{(model)                                 }\CommentTok{# Model architecture}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## dense (Dense)                    (None, 16)                    1504        
## ___________________________________________________________________________
## dense_1 (Dense)                  (None, 8)                     136         
## ___________________________________________________________________________
## dense_2 (Dense)                  (None, 1)                     9           
## ===========================================================================
## Total params: 1,649
## Trainable params: 1,649
## Non-trainable params: 0
## ___________________________________________________________________________
\end{verbatim}

\normalsize

The summary of the model lists the layers in their order from input to
output (forward pass). Because we are working with 93 features, the
number of parameters for the first layer (16 units) is 93 plus one (for
the bias) multiplied by 16, which makes 1504. For the second layer, the
number of inputs is equal to the size of the output from the previous
layer (16). Hence given the the second layer has 8 units, the total
number of parameters is (16+1)*8 = 136.

We set the loss function to the standard mean squared error. Other
losses are listed on \url{https://keras.io/losses/}: some of them work
only for regressions (MSE, MAE) and others only for classification
(categorical cross entropy, see Equation \eqref{eq:crossentropy}). The RMS
propragation optimizer is the classical mini-batch backpropagation
implementation. For other weight updating algorithms, we refer to
\url{https://keras.io/optimizers/}. The metric is the function used to
assess the quality of the model. It can be different from the loss: for
instance, using entropy for training and accuracy as the performance
metric.

The final stage fits the model to the data and requires some additional
training parameters:

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit_NN <-}\StringTok{ }\NormalTok{model }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{fit}\NormalTok{(NN_train_features,                                       }\CommentTok{# Training features}
\NormalTok{        NN_train_labels,                                         }\CommentTok{# Training labels}
        \DataTypeTok{epochs =} \DecValTok{10}\NormalTok{, }\DataTypeTok{batch_size =} \DecValTok{512}\NormalTok{,                           }\CommentTok{# Training parameters}
        \DataTypeTok{validation_data =} \KeywordTok{list}\NormalTok{(NN_test_features, NN_test_labels) }\CommentTok{# Test data}
\NormalTok{)}
\KeywordTok{plot}\NormalTok{(fit_NN)                                                                  }\CommentTok{# Plot, evidently!}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=400px]{ML_factor_files/figure-latex/NN3-1} 

}

\caption{Output from a trained neural network (regression task)}\label{fig:NN3}
\end{figure}

\normalsize

In keras, the plot of the trained model shows four different curves
(shown here in Figure \ref{fig:NN3}). The top graph displays the
improvement (or lack thereof) in loss as the number of epochs increases.
Usually, the algorithm starts by learning rapidly and then converges to
a point where any additional epoch does not improve. In the example
above, this point arrives rather quickly because it is hard to notice
any gain beyond the fourth epoch. The two colors show the performance on
the two samples: the training sample and the testing sample. By
construction, the loss will always improve (even marginally) on the
training sample. When the impact is negligible on the testing sample
(the curve is flat, as is the case here), the model fails to generalize
out-of-sample: the gains obtained by training on the original sample do
not translate to gains on previously unseen data, thus, the model seems
to be learning noise.

The second graph shows the same behavior but computed using the metric
function. The correlation (in absolute terms) between the two curves
(loss and metric) is usually high. If one of them is flat, the other
should be as well.

In order to obtain the parameters of the model, the user can call
get\_weights(model)\footnote{In case of package conflicts, use
  keras::get\_weights(model). Indeed, another package in the machine
  learning landscape, \emph{yardstick}, uses the function name
  ``get\_weights''.} We do not execute the code here because the size of
the output is much too large as their are thousands of weights.

Finally, from a practical point of view, the prediction is obtained via
the usual predict() function. We use this function below on the testing
sample to calculate the hit ratio.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{predict}\NormalTok{(model, NN_test_features) }\OperatorTok{*}\StringTok{ }\NormalTok{NN_test_labels }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{) }\CommentTok{# Hit ratio}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5429438
\end{verbatim}

\normalsize

Again, the hit ratio lies between 50\% and 55\%, which \emph{seems}
reasonably good. Most of the time, neural networks have their weights
initialized randomly. Hence, two independently trained networks with the
same architecture and same training data may well lead to very different
predictions and performance! One way to bypass this issue is to freeze
the random number generator. Models can also be easily exchanged by
loading weights via the set\_weights() function.

\hypertarget{classification-example}{%
\subsection{Classification example}\label{classification-example}}

We pursue our exploration of neural networks with a much more detailed
example. The aim is to carry out a classification task on the binary
label R1M\_Usd\_C. Before we proceed, we need to format the label
properly. To this purpose, we resort to one-hot encoding (see Section
\ref{categorical-labels}).

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dummies)                                            }\CommentTok{# Package for one-hot encoding}
\NormalTok{NN_train_labels_C <-}\StringTok{ }\NormalTok{training_sample}\OperatorTok{$}\NormalTok{R1M_Usd_C }\OperatorTok{%>%}\StringTok{ }\KeywordTok{dummy}\NormalTok{()  }\CommentTok{# One-hot encoding of the label}
\NormalTok{NN_test_labels_C <-}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd_C }\OperatorTok{%>%}\StringTok{ }\KeywordTok{dummy}\NormalTok{()    }\CommentTok{# One-hot encoding of the label}
\end{Highlighting}
\end{Shaded}

\normalsize

The labels NN\_train\_labels\_C and NN\_test\_labels\_C have two
columns: the first flags the instances with above median return and the
second flags those with below median returns. Note that we do not alter
the feature variables: they remain unchanged. Below, we set the
structure of the networks with many additional features compared to the
first one.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model_C <-}\StringTok{ }\KeywordTok{keras_model_sequential}\NormalTok{()}
\NormalTok{model_C }\OperatorTok{%>%}\StringTok{   }\CommentTok{# This defines the structure of the network, i.e. how layers are organized}
\StringTok{    }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{16}\NormalTok{, }\DataTypeTok{activation =} \StringTok{'tanh'}\NormalTok{,               }\CommentTok{# Nb units & activation}
                \DataTypeTok{input_shape =} \KeywordTok{ncol}\NormalTok{(NN_train_features),         }\CommentTok{# Size of input}
                \DataTypeTok{kernel_initializer =} \StringTok{"random_normal"}\NormalTok{,          }\CommentTok{# Initialization of weights}
                \DataTypeTok{kernel_constraint =} \KeywordTok{constraint_nonneg}\NormalTok{()) }\OperatorTok{%>%}\StringTok{   }\CommentTok{# Weights should be nonneg}
\StringTok{    }\KeywordTok{layer_dropout}\NormalTok{(}\DataTypeTok{rate =} \FloatTok{0.25}\NormalTok{) }\OperatorTok{%>%}\StringTok{                             }\CommentTok{# Dropping out 25% units}
\StringTok{    }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{8}\NormalTok{, }\DataTypeTok{activation =} \StringTok{'elu'}\NormalTok{,                 }\CommentTok{# Nb units & activation}
                \DataTypeTok{bias_initializer =} \KeywordTok{initializer_constant}\NormalTok{(}\FloatTok{0.2}\NormalTok{),  }\CommentTok{# Initialization of biases}
                \DataTypeTok{kernel_regularizer =} \KeywordTok{regularizer_l2}\NormalTok{(}\FloatTok{0.01}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# Penalization of weights }
\StringTok{    }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{2}\NormalTok{, }\DataTypeTok{activation =} \StringTok{'softmax'}\NormalTok{)             }\CommentTok{# Softmax for categorical output}
\end{Highlighting}
\end{Shaded}

\normalsize

Before we start commenting on the many options used above, we highlight
that keras models, unlike many R variables, are mutable objects. This
means that any piping \%\textgreater{}\% after calling a model will
alter it. Hence, successive trainings do not start from scratch but from
the result of the previous training.

First, the options used above and below were chosen as illustrative
examples and do not serve to particularly improve the quality of the
model. The first change compared to Section \ref{regression-example} are
the activation functions. The first two are simply new cases while the
third one (for the output layer) is imperative. Indeed, since the goal
is classification, the dimension of the output must be equal to the
number of categories of the labels. The activation that yields a
multivariate is the softmax function. Note that we must also specify the
number of classes (categories) in the terminal layer.

The second major innovation are options pertaining to parameters. One
family of options deals with the initialization of weights and biases.
In keras, weights are referred to as the `kernel'. The list of
initializers is quite long and we suggest the interest reader has a look
at the keras reference (\url{https://keras.io/initializers/}). Most of
them are random, but some of them are constant.

Another family of options are the constraints and norm penalization that
are applied on the weights and biases during training. In the above
example, the weights of the first layer are coerced to be nonnegative
while the weights of the second layer see their magnitude penalized by a
factor (0.01) times their \(L^2\) norm.

Lastly, the final novelty is the dropout layer (see Section
\ref{penalizations-and-dropout}) between the first and second layers.
According to this layer, one fourth of the units in the first layer will
be (randomly) omitted during training.

The specification of the training is outlined below.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model_C }\OperatorTok{%>%}\StringTok{ }\KeywordTok{compile}\NormalTok{(                               }\CommentTok{# Model specification}
    \DataTypeTok{loss =} \StringTok{'binary_crossentropy'}\NormalTok{,                  }\CommentTok{# Loss function}
    \DataTypeTok{optimizer =} \KeywordTok{optimizer_adam}\NormalTok{(}\DataTypeTok{lr =} \FloatTok{0.005}\NormalTok{,         }\CommentTok{# Optimisation method (weight updating)}
                               \DataTypeTok{beta_1 =} \FloatTok{0.9}\NormalTok{, }
                               \DataTypeTok{beta_2 =} \FloatTok{0.95}\NormalTok{),        }
    \DataTypeTok{metrics =} \KeywordTok{c}\NormalTok{(}\StringTok{'categorical_accuracy'}\NormalTok{)            }\CommentTok{# Output metric}
\NormalTok{)}
\KeywordTok{summary}\NormalTok{(model_C)                                   }\CommentTok{# Model structure}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## dense_3 (Dense)                  (None, 16)                    1504        
## ___________________________________________________________________________
## dropout (Dropout)                (None, 16)                    0           
## ___________________________________________________________________________
## dense_4 (Dense)                  (None, 8)                     136         
## ___________________________________________________________________________
## dense_5 (Dense)                  (None, 2)                     18          
## ===========================================================================
## Total params: 1,658
## Trainable params: 1,658
## Non-trainable params: 0
## ___________________________________________________________________________
\end{verbatim}

\normalsize

Here again, many changes have been made: all levels have been revised.
The loss is now the crossentropy. Since we work with two categories, we
resort to a specific choice (binary crossentropy) but the more general
form is categorical\_crossentropy and works for any number of classes
(strictly above 1). The optimizer is also different and allows for
several parameters and we refer to \citet{kingma2014adam}. Simply put,
the two beta parameters control decay rates for exponentially-weighted
moving averages used in the update of weights. The two averages are
estimates for the first and second moment of the gradient and can be
exploited to increase the speed of learning. The performance metric in
the above chunk is the categorical accuracy. In multiclass
classification, the accuracy is defined as the average accuracy over all
classes and all predictions. Since a prediction for one instance is a
vector of weights, the `terminal' prediction is the class that is
associated with the largest weight. The accuracy then measures the
proportion of times when the prediction is equal to the realized value.

Finally, we proceed with the training of the model.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit_NN_C <-}\StringTok{ }\NormalTok{model_C }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{fit}\NormalTok{(NN_train_features,                                   }\CommentTok{# Training features}
\NormalTok{        NN_train_labels_C,                                   }\CommentTok{# Training labels}
        \DataTypeTok{epochs =} \DecValTok{20}\NormalTok{, }\DataTypeTok{batch_size =} \DecValTok{512}\NormalTok{,                       }\CommentTok{# Training parameters}
        \DataTypeTok{validation_data =} \KeywordTok{list}\NormalTok{(NN_test_features, }
\NormalTok{                               NN_test_labels_C),            }\CommentTok{# Test data}
        \DataTypeTok{verbose =} \DecValTok{0}\NormalTok{,                                         }\CommentTok{# No comments from algo}
        \DataTypeTok{callbacks =} \KeywordTok{list}\NormalTok{(}
            \KeywordTok{callback_early_stopping}\NormalTok{(}\DataTypeTok{monitor =} \StringTok{"val_loss"}\NormalTok{,    }\CommentTok{# Early stopping:}
                                    \DataTypeTok{min_delta =} \FloatTok{0.001}\NormalTok{,       }\CommentTok{# Improvement threshold}
                                    \DataTypeTok{patience =} \DecValTok{3}\NormalTok{,            }\CommentTok{# Nb epochs with no improvmt }
                                    \DataTypeTok{verbose =} \DecValTok{0}              \CommentTok{# No warnings}
\NormalTok{                                    )}
\NormalTok{        )}
\NormalTok{    )}
\KeywordTok{plot}\NormalTok{(fit_NN_C) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=400px]{ML_factor_files/figure-latex/NN3C-1} 

}

\caption{Output from a trained neural network (classification task) with early stopping}\label{fig:NN3C}
\end{figure}

\normalsize

There is only one major difference here compared to the previous
training call. In keras, callbacks are functions that can be used at
given stages of the learning process. In the above example, we use one
such function to stop the algorithm when no progress has been made for
some time.

When datasets are large, the training can be long, especially when batch
sizes are small and/or the number of epochs is high. It is not
guaranteed that going to the full number of epochs is useful, as the
loss or metric functions may be plateauing much sooner. Hence, can be
very convenient to stop the process if no improvement is achieved during
a specified time frame. We set the number of epochs to 20, but the
process will likely stop before that.

In the above code, the improvement is focused on validation accuracy
(``val\_acc''; one alternative is ``val\_loss''). The min\_delta value
sets the minimum improvement that needs to be attained for the algorithm
to continue. Therefore, unless the validation accuracy gains 0.001
points at each epoch, the training will stop. Nevertheless, some
flexibility is introduced via the patience parameter, which in our case
asserts that the hatling decision is made only after three consecutive
epochs with no improvement. In the option, the verbose parameter
dictates the amount of comments that is made by the function. For
simplicity, we do not want any comments, hence this value is set to
zero.

In Figure \ref{fig:NN3C}, the two graphs yield very different curves.
One reason for that is the scale of the second graph. The range of
accuracies is very narrow. Any change in this range does not represent
much variation overall. The pattern is relatively clear on the training
sample: the loss decreases while the accuracy improves. Unfortunately,
this does not translate to the testing sample which indicates that the
model does not generalize well out-of-sample.

\hypertarget{recurrent-networks}{%
\section{Recurrent networks}\label{recurrent-networks}}

\hypertarget{presentation}{%
\subsection{Presentation}\label{presentation}}

Multilayer perceptrons are feedforward networks because the data flows
from left to right with no looping in between. For some particular tasks
with sequential linkages (e.g., time-series or speech recognition), it
might be useful to keep track of what happened with the previous sample
(i.e., there is a natural ordering). One simple way to model `memory'
would be to consider the following network with only one intermediate
layer: \begin{align*}
\tilde{y}_i&=f^{(y)}\left(\sum_{j=1}^{U_1}h_{i,j}w^{(y)}_j+b^{(2)}\right) \\
h_{i} &=f^{(h)}\left(\sum_{k=1}^{U_0}x_{i,k}w^{(h,1)}_k+b^{(1)}+ \underbrace{\sum_{k=1}^{U_1}  w^{(h,2)}_{k}h_{i-1,k}}_{\text{memory part}} \right)
\end{align*}

These kinds of models are often referred to as \citet{elman1990finding}
models or to \citet{jordan1997serial} models if in the latter case
\(h_{i-1}\) is replaced by \(y_{i-1}\) in the computation of \(h_i\).
Both types of models fall under the overarching umbrella of Recurrent
Neural Networks (RNNs).

The \(h_i\) is usually called the state or the hidden layer. The
training of this model is complicated and must be done by unfolding the
network over all instances to obtain a simple feed-forward network and
train it regularly. We illustrate the unfolding principle in Figure
\ref{fig:recnet}. It shows a very deep network. The first input impacts
the first layer and then the second one via \(h_1\) and all following
layers in the same fashion. Likewise, the second inputs impacts all
layers except the first and each instance \(i-1\) is going to impact the
output \(\tilde{y}_i\) and all outputs \(\tilde{y}_j\) for \(j \ge i\).
In Figure \ref{fig:recnet}, the parameters that are trained are shown in
blue. They appear many times, in fact, at each level of the unfolded
network.

\begin{figure}[H]

{\centering \includegraphics[width=480px]{images/RN} 

}

\caption{Unfolding a recurrent network.}\label{fig:recnet}
\end{figure}

The main problem with the above architecture is the loss of memory
induced by \textbf{vanishing gradients}. Because of the depth of the
model, the chain rule used in the backpropagation with imply a large
number of products of derivatives of activation functions. Now, as is
shown in Figure \ref{fig:activationf}, these functions are very smooth
and their derivatives are most of the time smaller than one (in absolute
value). Hence, multiplying many numbers smaller than one leads to very
small figures: beyond some layers, the learning does not propagate
because the adjustments are too small.

One way to prevent this progressive discounting of the memory was
introduced in \citet{hochreiter1997long} (Long Short Term Memory - LSTM
model). This model was subsequently simplified by the authors
\citet{chung2015gated} and we present this more parcimonious model
below. The Gated Recurrent Unit is a slightly more complicated version
of the vanilla recurrent network defined above. It has the following
representation: \begin{align*}
\tilde{y}_i&=z_i\tilde{y}_{i-1}+ (1-z_i)\tanh \left(\textbf{w}_y'\textbf{x}_i+ b_y+ u_yr_i\tilde{y}_{i-1}\right) \quad \text{output (prediction)} \\
z_i &= \text{sig}(\textbf{w}_z'\textbf{x}_i+b_z+u_z\tilde{y}_{i-1})  \hspace{9mm} \text{`update gate'} \ \in (0,1)\\
r_i &= \text{sig}(\textbf{w}_r'\textbf{x}_i+b_r+u_r\tilde{y}_{i-1}) \hspace{9mm} \text{`reset gate'}  \ \in (0,1).
\end{align*} In compact form, this gives
\[\tilde{y}_i=\underbrace{z_i}_{\text{weight}}\underbrace{\tilde{y}_{i-1}}_{\text{past value}}+ \underbrace{(1-z_i)}_{\text{weight}}\underbrace{\tanh \left(\textbf{w}_y'\textbf{x}_i+ b_y+ u_yr_i\tilde{y}_{i-1}\right)}_{\text{candidate value (classical RNN)}}, \]

where the \(z_i\) decides the optimal mix between the current and past
values. For the candidate value, \(r_i\) decides which amount of
past/memory to retain. \(r_i\) is commonly referred to as the
`\emph{reset gate}' and \(z_i\) to the `\emph{update gate}'.

There are some subtleties in the training of a recurrent network.
Indeed, because of the chaining between the instances, each batch must
correspond to a coherent time-series. A logical choice is thus one batch
per asset with instances (logically) chronologically ordered. Lastly,
one option in some frameworks is to keep some memory between the batches
by passing the final value of \(\tilde{y}_i\) to the next batch (for
which it will be \(\tilde{y}_0\)). This is often referred to as the
stateful mode and should be considered meticulously. It does not seem
desirable in a portfolio prediction setting if the batch size
corresponds to all observations for each asset: there is no particular
link between assets. If the dataset is divided into several parts for
each given asset, then the training must be handled very cautiously.

Reccurrent networks and LSTM especially have been found to be good
forecasting tools in financial contexts (see, e.g.,
\citet{fischer2018deep} and \citet{wang2019portfolio}).

\hypertarget{code-and-results-3}{%
\subsection{Code and results}\label{code-and-results-3}}

Recurrent networks are theoretically more complicated compared to
multilayered perceptrons. In practice, they are also more challenging in
their implementation. Indeed, the serial linkages require more attention
compare to feedforward architectures. In an asset pricing framework, we
must separate the assets because the stock-specific time series cannot
be bundled together. The learning will be sequential, one stock at a
time.

The dimensions of variables are crucial. In keras, they are defined for
RNNs as:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The size of the batch. In our case, it will be the number of assets.
  Indeed, the recurrence relationship holds at the asset level, hence
  each asset will represent a new batch on which the model will learn.\\
\item
  The timesteps. In our case, it will simply be the number of dates.\\
\item
  The number of features. In our case, there is only one possible
  figure: the number of predictors.
\end{enumerate}

For simplicity and in order to reduce computation times, we will use the
same subset of stocks as that from Section \ref{sparseex}. This yields a
perfectly rectangular dataset in which all dates have the same number of
observations.

First, we create some new, intermediate variables. \footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data_rnn <-}\StringTok{ }\NormalTok{data_ml }\OperatorTok{%>%}\StringTok{                                  }\CommentTok{# Dedicated dataset}
\StringTok{    }\KeywordTok{filter}\NormalTok{(stock_id }\OperatorTok{%in%}\StringTok{ }\NormalTok{stock_ids_short)}
\NormalTok{training_sample_rnn <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(data_rnn, date }\OperatorTok{<}\StringTok{ }\NormalTok{separation_date)}
\NormalTok{testing_sample_rnn <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(data_rnn, date }\OperatorTok{>}\StringTok{ }\NormalTok{separation_date)}
\NormalTok{nb_stocks <-}\StringTok{ }\KeywordTok{length}\NormalTok{(stock_ids_short)                     }\CommentTok{# Nb stocks }
\NormalTok{nb_feats <-}\StringTok{ }\KeywordTok{length}\NormalTok{(features)                             }\CommentTok{# Nb features}
\NormalTok{nb_dates_train <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(training_sample) }\OperatorTok{/}\StringTok{ }\NormalTok{nb_stocks      }\CommentTok{# Nb training dates (size of sample)}
\NormalTok{nb_dates_test <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(testing_sample) }\OperatorTok{/}\StringTok{ }\NormalTok{nb_stocks        }\CommentTok{# Nb testing dates}
\end{Highlighting}
\end{Shaded}

\normalsize

Then, we construct the variables we will pass as arguments. We recall
that the data file was ordered first by stocks and then by date (see
Section \ref{dataset}). \footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train_features_rnn <-}\StringTok{ }\KeywordTok{array}\NormalTok{(NN_train_features,           }\CommentTok{# Formats the training data into array}
                            \DataTypeTok{dim =} \KeywordTok{c}\NormalTok{(nb_dates_train, nb_stocks, nb_feats)) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# Tricky order}
\StringTok{    }\KeywordTok{aperm}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))                                      }\CommentTok{# The order is: stock, date, feature }
\NormalTok{test_features_rnn <-}\StringTok{ }\KeywordTok{array}\NormalTok{(NN_test_features,             }\CommentTok{# Formats the testing data into array}
                            \DataTypeTok{dim =} \KeywordTok{c}\NormalTok{(nb_dates_test, nb_stocks, nb_feats)) }\OperatorTok{%>%}\StringTok{  }\CommentTok{# Tricky order}
\StringTok{    }\KeywordTok{aperm}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))                                      }\CommentTok{# The order is: stock, date, feature }
\NormalTok{train_labels_rnn <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(NN_train_labels) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{array}\NormalTok{(}\DataTypeTok{dim =} \KeywordTok{c}\NormalTok{(nb_dates_train, nb_stocks, }\DecValTok{1}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{aperm}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\NormalTok{test_labels_rnn <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(NN_test_labels) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{array}\NormalTok{(}\DataTypeTok{dim =} \KeywordTok{c}\NormalTok{(nb_dates_test, nb_stocks, }\DecValTok{1}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{aperm}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\normalsize

Finally, we move towards the training part. For simplicity, we only
consider a simple RNN with only one layer. The structure is outlined
below. In terms of recurrence structure, we pick a Gated Recurrent Unit.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model_RNN <-}\StringTok{ }\KeywordTok{keras_model_sequential}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{layer_gru}\NormalTok{(}\DataTypeTok{units =} \DecValTok{16}\NormalTok{,                              }\CommentTok{# Nb units in hidden layer}
              \DataTypeTok{batch_input_shape =} \KeywordTok{c}\NormalTok{(nb_stocks,         }\CommentTok{# Dimensions = tricky part!}
\NormalTok{                                    nb_dates_train, }
\NormalTok{                                    nb_feats), }
              \DataTypeTok{activation =} \StringTok{'tanh'}\NormalTok{,                     }\CommentTok{# Activation function}
              \DataTypeTok{return_sequences =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}\StringTok{             }\CommentTok{# Return all the sequence}
\StringTok{    }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{1}\NormalTok{)                             }\CommentTok{# Final aggregation layer}
\NormalTok{model_RNN }\OperatorTok{%>%}\StringTok{ }\KeywordTok{compile}\NormalTok{(}
    \DataTypeTok{loss =} \StringTok{'mean_squared_error'}\NormalTok{,                       }\CommentTok{# Loss = quadratic}
    \DataTypeTok{optimizer =} \KeywordTok{optimizer_rmsprop}\NormalTok{(),                   }\CommentTok{# Backprop}
    \DataTypeTok{metrics =} \KeywordTok{c}\NormalTok{(}\StringTok{'mean_absolute_error'}\NormalTok{)                 }\CommentTok{# Output metric MAE}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\normalsize

There are many options available for recurrent layers. For GRUs, we
refer to the keras documentation
\url{https://keras.rstudio.com/reference/layer_gru.html}. We comment
briefly on the option return\_sequences which we activate. In many
cases, the output is simply the terminal value of the sequence. If we do
not require all of the sequence to be returned, we will face a problem
in the dimensionality because the label is indeed a full sequence. Once
the structure is determined, we can move forward to the training stage.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit_RNN <-}\StringTok{ }\NormalTok{model_RNN }\OperatorTok{%>%}\StringTok{ }\KeywordTok{fit}\NormalTok{(train_features_rnn,   }\CommentTok{# Training features        }
\NormalTok{                  train_labels_rnn,                }\CommentTok{# Training labels}
                  \DataTypeTok{epochs =} \DecValTok{10}\NormalTok{,                     }\CommentTok{# Number of rounds}
                  \DataTypeTok{batch_size =}\NormalTok{ nb_stocks,          }\CommentTok{# Length of sequences}
                  \DataTypeTok{verbose =} \DecValTok{0}\NormalTok{)                     }\CommentTok{# No comments}
\KeywordTok{plot}\NormalTok{(fit_RNN)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=300px]{ML_factor_files/figure-latex/RNN2-1} 

}

\caption{Output from a trained recurrent neural network (regression task)}\label{fig:RNN2}
\end{figure}

\normalsize

Compared to our previous models, the major difference both in the ouptut
(the graph on Figure \ref{fig:RNN2}) and the input (the code) is the
absence of validation (or testing) data. One reason for that is because
keras is very restrictive on RNNs and imposes that both the training and
testing samples share the same dimensions. In our situation this is
obviously not the case, hence we must bypass this obstacle by
duplicating the model.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new_model <-}\StringTok{ }\KeywordTok{keras_model_sequential}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{layer_gru}\NormalTok{(}\DataTypeTok{units =} \DecValTok{16}\NormalTok{, }
              \DataTypeTok{batch_input_shape =} \KeywordTok{c}\NormalTok{(nb_stocks,          }\CommentTok{# New dimensions}
\NormalTok{                                    nb_dates_test, }
\NormalTok{                                    nb_feats), }
              \DataTypeTok{activation =} \StringTok{'tanh'}\NormalTok{,                      }\CommentTok{# Activation function}
              \DataTypeTok{return_sequences =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{%>%}\StringTok{              }\CommentTok{# Passing last state to next batch}
\StringTok{    }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{1}\NormalTok{)                              }\CommentTok{# Output dimension}
\NormalTok{new_model }\OperatorTok{%>%}\StringTok{ }\NormalTok{keras}\OperatorTok{::}\KeywordTok{set_weights}\NormalTok{(keras}\OperatorTok{::}\KeywordTok{get_weights}\NormalTok{(model_RNN))}
\end{Highlighting}
\end{Shaded}

\normalsize

Finally, once the new model is ready - and with the matching dimensions,
we can push forward to predicting the test values. We resort to the
predict() function and immediately compute the hit ratio obtained by the
model.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred_rnn <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(new_model, test_features_rnn, }\DataTypeTok{batch_size =}\NormalTok{ nb_stocks) }\CommentTok{# Predictions}
\KeywordTok{mean}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\KeywordTok{t}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(pred_rnn))) }\OperatorTok{*}\StringTok{ }\NormalTok{test_labels_rnn }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{)           }\CommentTok{# Hit ratio}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5017483
\end{verbatim}

\normalsize

The hit ratio is close to 50\%, hence the model does hardly better than
coin tossing.

\hypertarget{other-common-architectures}{%
\section{Other common architectures}\label{other-common-architectures}}

In this section, we present other network structures. Because they are
less mainstream and often harder to implement, we do not propose code
examples and stick to theoretical introductions.

\hypertarget{generative-aversarial-networks}{%
\subsection{Generative adversarial
networks}\label{generative-aversarial-networks}}

The idea of Generative Adversarial Networks (GANs) is to improve the
accuracy of a classical neural network by trying to fool it. This very
popular idea was introduced by \citet{goodfellow2014generative}. Imagine
you are an expert in Picasso paintings and that you boast about being
able to easily recognize any piece of work from the painter. One way to
refine your skill is to test them against a counterfeiter. A true expert
should be able to discriminate between a true original Picasso and one
emanating from a forger. This is the principle of GANs.

GANs consist in two neural networks: the first one tries to learn and
the second one tries to fool the first (induce it into error). Just like
in the example above, there are also two sets of data: one
(\(\textbf{x}\)) is true (or correct), stemming from a classical
training sample and the other one (\(\textbf{z}\)) is fake and generated
by the counterfeiter network.

In the GAN nomenclature the networks that learns is \(D\) because it's
suppose to discriminate while the forger is \(G\) because it generates
false data. In their original formulation, GANs are aimed at
classifying. To ease the presentation, we keep this scope. The
discriminant network has a simple (scalar) output: the probability that
its input comes from true data (versus fake data). The input of \(G\) is
some arbitrary noise and its output has the same shape/form as the input
of \(D\).

We state the theoretical formula of a GAN directly and comment on it
below. \(D\) and \(G\) play the following minimax game: \begin{equation}
\label{eq:GAN}
\underset{G}{\min} \ \underset{D}{\max} \ \left\{ \mathbb{E}[\log(D(\textbf{x}))]+\mathbb{E}[\log(1-D(G(\textbf{z})))] \right\}.
\end{equation}

First, let us decompose this expression in its two parts (the
optimizers). The first part (i.e.~the first max) is the classical one:
the algorithm seeks to maximize the probability of assigning the correct
label to all examples it seeks to classify. As is done in economics and
finance, the program does not maximize \(D(\textbf{x})\) itself on
average, but rather a functional form (like a utility function).

On the left side, since the expectation is driven by \(\textbf{x}\), the
objective must be increasing in the output. On the right side, where the
expectation is taken over the fake instances, the right classification
is the opposite, i.e., \(1-D(G(\textbf{z})\).

The second, overarching, part seeks to minimize the performance of the
algorithm on the simulated data: it aims at shrinking the odds that
\(D\) finds out that the data is indeed corrupt. A summarized version of
the structure of the network is provided below.

\begin{figure}
$$\left. \begin{array}{rlll} 
\text{training sample}  = \textbf{x} = \text{true data} && \\
\text{noise}= \textbf{z} \quad \overset{G}{\rightarrow} \quad  \text{fake data}  &
\end{array} \right\} \overset{D}{\rightarrow} \text{output = probability for label}
$$  \vspace{-3mm}
\caption{Scheme of a GAN.}
\end{figure}

In ML-based asset pricing, the most notable application of GANs was
introduced in \citet{chen2019deep}. Their aim is to make use of the
method of moment expression
\[\mathbb{E}[M_{t+1}r_{t+1,n}g(I_t,I_{t,n})]=0,\] which is an
application of Equation \eqref{eq:SDFGMM} where the instrumental variables
\(I_{t,n}\) are firm-dependent (e.g., characteristics and attributes)
while the \(I_t\) are macro-economic variables (aggregate dividend
yield, volatility level, credit spread, term spread, etc.). The trick is
to model the SDF as an unknown combination of assets
\(M_{t+1}=1-\sum_{n=1}^Nw(I_t,I_{t,n})r_{t+1,n}\). The primary
discriminatory network (\(D\)) is the one that approximates the SDF via
the weights \(w(I_t,I_{t,n})\). The secondary generative network is the
one that creates the moment condition through \(g(I_t,I_{t,n})\) in the
above equation.

The full specification of the network is given by the program:
\[\underset{w}{\text{min}} \ \underset{g}{\text{max}} \ \sum_{j=1}^N \left\| \mathbb{E} \left[\left(1-\sum_{n=1}^Nw(I_t,I_{t,n})r_{t+1,n} \right)r_{t+1,j}g(I_t,I_{t,j})\right] \right\|^2\]

The asset pricing equations (moments) are not treated as equalities but
as a relationship that is approximated. The network defined by
\(\textbf{w}\) is the asset pricing modeler and tries to determine the
best possible model while the network defined by \(\textbf{g}\) seeks to
find the worst possible conditions so that the model performs badly. We
refer to the original article for the full specification of both
networks. In their empirical section, \citet{chen2019deep} report that
adopting a strong structure driven by asset pricing imperatives add
values compared to a pure predictive `vanilla' approach such as the one
detailed in \citet{gu2018empirical}. The out-of-sample behavior of
decile sorted portfolios (based on the model's prediction) display a
monotonous patter with respect to the order of the deciles.

\hypertarget{autoencoders}{%
\subsection{Auto-encoders}\label{autoencoders}}

In the recent literature auto-encoders are used in \citet{huck2019large}
(portfolio management), and \citet{gu2019autoencoder} (asset pricing).\\
Autoencoders are a strange family of neural networks because they are
classified among non-supervised algorithms. In the supervised jargon,
their label is equal to the input. Like GANS, autoencoders consist of
two networks, though the structure is very different: the first network
encodes the input into some intermediary output (usually called the
code) and the second network decodes the code into a modified version of
the input.

\[\begin{array}{ccccccccc}
\textbf{x} & &\overset{E}{\longrightarrow} && \textbf{z} && \overset{D}{\longrightarrow} && \textbf{x}' \\
\text{input} && \text{encoder} && \text{code} && \text{decoder} && \text{modified input}
\end{array}\]

Because auto-encoders do not belong to the large family of supervised
algorithms, we postpone their presentation to Section \ref{ae}.

\hypertarget{a-word-on-convolutional-networks}{%
\subsection{A word on convolutional
networks}\label{a-word-on-convolutional-networks}}

Neural networks gained popularity during the 2010 decade thanks to a
series of successes in computer vision competitions. The algorithms
behind these advances are convolutional neural networks (CNN). While
they may seem a surprising choice for financial predictions, several
teams of researchers in the Computer Science field have proposed
approaches that rely on this variation of neural networks
(\citet{chen2016financial}, \citet{loreggia2016deep},
\citet{dingli2017financial}, \citet{tsantekidis2017forecasting},
\citet{hoseinzade2019cnnpred}). Hence, we briefly present the principle
in this final section on neural networks. We lay out the presentation
for CNNs of dimension two, but they are can also be used in dimension
one or three.

The reason why CNNs are useful is because they allow to progressively
reduce the dimension of a large dataset by keeping local information. An
image is a rectangle of pixels. Each pixel is usually coded via three
layers, one for each color: red, blue and green. But to keep things
simple, let's just consider one layer of, say 1,000 by 1,000 pixels with
one value for each pixel. In order to analyze the content of this image,
a \textbf{convolutional layer} will simplify by scanning the values
using rectangles with arbitrary weights.

Figure \ref{fig:cnnscheme} sketches this process (it is strongly
inspired by \citet{hoseinzade2019cnnpred}). The original data is a
matrix \((I\times K)\) \(x_{i,k}\) and the weights are also a matrix
\(w_{j,l}\) of size \((J\times L)\) with \(J<I\) and \(L<K\). The
scanning transforms each rectangle of size \((J\times L)\) into one real
number. Hence, the output has a smaller size: \((I-J+1)\times(K-L+1)\).
If \(I=K=1,000\) and \(J=L=201\), then the output has dimension
\((800\times 800)\) which is already much smaller. The output values are
given by \[o_{i,k}=\sum_{j=1}^J\sum_{l=1}^Lw_{j,l}x_{i+j-1,k+l-1}.\]

\begin{figure}[H]

{\centering \includegraphics[width=480px]{images/cnn_scheme} 

}

\caption{Scheme of a convolutional unit. Note: the dimensions are general and do not correspond to the number of squares.}\label{fig:cnnscheme}
\end{figure}

Iteratively reducing the dimension of the output via sequences of
convolutional layers like the one presented above would be costly in
computation and could give rise to overfitting because the number of
weights would be incredibly large. In order to efficiently reduce the
size of outputs, \textbf{pooling layers} are often used. The job of
pooling units is to simplify matrices by reducing them to a simple
metric such as the minimum, maximum or average value of the matrix:

\[o_{i,k}=f(x_{i+j-1,k+l-1}, 1\le j\le J, 1 \le l\le L),\] where \(f\)
is the minimum, maximum or average value. We show examples of pooling in
Figure \ref{fig:cnnpooling} below. In order to increase the speed of
compression, it is possible to add a stride to omit cells. A stride
value of \(v\) will perform the operation only every \(v\) value and
hence bypass intermediate steps. In Figure \ref{fig:cnnpooling}, the two
cases on the left do not resort to pooling, hence the reduction in
dimension is exactly equal to the size of the pooling size. When stride
is into action (right pane), the reduction is more marked. From a 1,000
by 1,000 input, a 2-by-2 pooling layer with stride 2 will yield a 500 by
500 output: the dimension is shrinked four-fold, as in the right scheme
of Figure \ref{fig:cnnpooling} .

\begin{figure}[H]

{\centering \includegraphics[width=500px]{images/cnn_pooling} 

}

\caption{Scheme of pooling units.}\label{fig:cnnpooling}
\end{figure}

With these tools in hand, it is possible to build new predictive tools.
In \citet{hoseinzade2019cnnpred}, predictors such as price quotes,
technical indicators and macro-economic data are fed to a complex neural
network with 6 layers in order to predict the sign of price variations.
While this is clearly an interesting computer science exercise, the deep
economic motivation behind this choice of architecture remain unclear.

\hypertarget{advanced-architectures}{%
\subsection{Advanced architectures}\label{advanced-architectures}}

The superiority of neural networks in tasks related to computer vision
and natural language processing is now well established. However, in
many ML tournaments in the 2010 decade, neural networks have often been
surpassed by tree-based models when dealing with tabular data. This
puzzle encouraged researchers to construct novel NN structures that are
better suited to tabular databases. Examples include
\citet{arik2019tabnet} and \citet{popov2019neural} but their ideas lie
outside the scope of this book. The interested reader can have a look at
the original papers.

\hypertarget{coding-exercises-4}{%
\section{Coding exercises}\label{coding-exercises-4}}

\hypertarget{svm}{%
\chapter{Support vector machines}\label{svm}}

While the origins of support vector machines (SVM) are old (and go back
to \citet{vapnik1963pattern}), their modern treatment was initiated in
\citet{boser1992training} and \citet{cortes1995support} (binary
classification) and \citet{drucker1997support} (regression). We refer to
\url{http://www.kernel-machines.org/books} for an exhaustive
bibliography on their theoretical and empirical properties. SVMs have
been very popular between since their creation among the machine
learning community. Nonetheless, other tools (neural networks
especially) have gained popularity and progressively replaced SVMs in
many applications like computer vision notably.

\hypertarget{svm-for-classification}{%
\section{SVM for classification}\label{svm-for-classification}}

As is often the case in machine learning, it is easier to explain a
complex tool through an illustration with binary classification. In
fact, sometimes, it is originally how the tool was designed (e.g., for
the perceptron). Let us consider a simple example in the plane , that
is, with two features. In Figure \ref{fig:svmscheme}, the goal is to
find a model that correctly classifies points: filled circles versus
empty squares.

\begin{figure}[b]

{\centering \includegraphics[width=500px]{images/svm} 

}

\caption{Diagram of binary classification with support vectors}\label{fig:svmscheme}
\end{figure}

A model consists of two weights \(\textbf{w}=(w_1,w_2)\) that load on
the variables and create a natural linear separation in the plane. In
the example above, we show three separations. The red one is not a good
classifier because there are circles and squares above and beneath it.
The blue line is a good classifier: all circles are to its left and all
squares to its right. Likewise, the green line achieves a perfect
classification score. Yet, there is a notable difference between the
two.

The grey star at the top of the graph is a mystery point and given its
location, if the data pattern holds, it should be a circle. The blue
model fails to recognize it a such while the green one succeeds. The
interesting features of the scheme are those that we have not mentioned
yet, that is, the grey dotted lines. These lines represent the no-man's
land in which no observation falls when the green model is enforced. In
this area, each strip above and below the green line can be viewed as a
margin of error for the model. Typically, the grey star is located
inside this margin.

The two margins are computed as the parallel lines that maximize the
distance between the model and the closest points that are correctly
classified (on both sides). These points are called support vectors,
which justifies the name of the technique. Obviously, the green model
has a greater margin than the blue one. The core idea of SVMs is to
maximize the margin, under the constraint that the classifier does not
make any mistake. Said differently, SVMs try to pick the most robust
model among all those that yield a correct classification.

More formally, if we numerically define circles as +1 and squares as -1,
any `good' linear model is expected to satisfy: \begin{equation}
\label{eq:svm0}
\left\{\begin{array}{lll}
\sum_{k=1}^Kw_kx_{i,k}+b \ge +1 & \text{ when } y_i=+1 \\
\sum_{k=1}^Kw_kx_{i,k}+b \le -1 & \text{ when } y_i=-1,
\end{array}\right.
\end{equation}

which can be summarized in compact form
\(y_i \times \left(\sum_{k=1}^K w_kx_{i,k}+b \right)\ge 1\). Now, the
margin between the green model and a support vector on the dashed grey
line is equal to \(||\textbf{w}||=\sqrt{\sum_{k=1}^Kw_k^2}\). This value
comes from the fact that the distance between a point \((x_0,y_0)\) and
a line parametrized by \(ax+by+c=0\) is equal to
\(d=\frac{|ax_0+by_0+c|}{\sqrt{a^2+b^2}}\). In hte case of the model
defined above (\eqref{eq:svm0}), the numerator is equal to 1 and the norm
is that of \(\textbf{w}\). Thus, the final problem is the following:

\begin{equation}
\label{eq:svm1}
\underset{\textbf{w}, b}{\text{argmin}} \ ||\textbf{w}|| \ \text{ s.t. } y_i\left(\sum_{k=1}^Kw_kx_{i,k}+b \right)\ge 1.
\end{equation}

Naturally, this problem becomes infeasible whenever the condition cannot
be satisfied, that is, when a simple line cannot perfectly separate the
labels, no matter the choice of coefficients. This is the most common
configuration and datasets are then called logically \emph{not linearly
separable}. This complicates the process but it is possible to resort to
a trick. The idea is to introduce some flexbility in \eqref{eq:svm0} by
adding correction variables that allow the conditions to be met:

\begin{equation}
\label{eq:svm2}
\left\{\begin{array}{lll}
\sum_{k=1}^Kw_kx_{i,k}+b \ge +1-\xi_i & \text{ when } y_i=+1 \\
\sum_{k=1}^Kw_kx_{i,k}+b \le -1+\xi_i & \text{ when } y_i=-1,
\end{array}\right.
\end{equation}

where the novelties, the \(\xi_i\) are positive so-called `slack'
variables that make the conditions feasible. They are illustrated in
Figure \ref{fig:svmscheme2}. In this new configuration, there is no
simple linear model that can perfectly discriminate between the two
classes.

\begin{figure}[H]

{\centering \includegraphics[width=500px]{images/svm2} 

}

\caption{Diagram of binary classification with SVM: linearly inseparable data.}\label{fig:svmscheme2}
\end{figure}

The optimization program then becomes \begin{equation}
\label{eq:svm3}
\underset{\textbf{w},b, \boldsymbol{\xi}}{\text{argmin}} \ ||\textbf{w}||+C\sum_{i=1}^I\xi_i \ \text{ s.t. } \left\{ y_i\left(\sum_{k=1}^Kw_k\phi(x_{i,k})+b \right)\ge 1-\xi_i \ \text{ and } \ \xi_i\ge 0, \ \forall i  \right\},
\end{equation} where the parameter \(C>0\) tunes the cost of
mis-classification: as \(C\) increases, errors become more penalizing.
In addition, the program can be generalized to nonlinear models, via the
kernel \(\phi\) which is applied to the input points \(x_{i,k}\).
Nonlinear kernels can help cope with patterns that are more complex than
straight lines (see Figure \ref{fig:svmscheme3}). Common kernels can be
polynomial, radial or sigmoid. The solution is found using more or less
standard techniques for constrained quadratic programs. Once the weights
\(\textbf{w}\) and bias \(b\) a set via training, a prediction for a new
vector \(\textbf{x}_j\) is simply made by computing
\(\sum_{k=1}^Kw_k\phi(x_{j,k})+b\) and choosing the class based on the
sign of the expression.

\begin{figure}[H]

{\centering \includegraphics[width=500px]{images/svm3} 

}

\caption{Examples of nonlinear kernels.}\label{fig:svmscheme3}
\end{figure}

\hypertarget{svm-for-regression}{%
\section{SVM for regression}\label{svm-for-regression}}

The ideas of classification SVM can be transposed to regression
exercises. One general formulation is the following

\begin{align}
\underset{\textbf{w},b, \boldsymbol{\xi}}{\text{argmin}} \  & ||\textbf{w}||+C\sum_{i=1}^I\left(\xi_i+\xi_i^* \right)\\
 \text{ s.t. }&  \sum_{k=1}^Kw_k\phi(x_{i,k})+b -y_i\le \epsilon+\xi_i \\ \label{eq:svm4}
&  y_i-\sum_{k=1}^Kw_k\phi(x_{i,k})-b \le \epsilon+\xi_i^* \\
&\xi_i,\xi_i^*\ge 0, \ \forall i  ,
\end{align}

and it is illustrated in Figure \ref{fig:svmscheme4}. The user specifies
a margin \(\epsilon\) and the model will try to find the linear (up to
kernal transformation) relationship between the labels \(y_i\) and the
input \(\textbf{x}_i\). Just as in the classification task, if the data
points are inside the strip, the slack variables \(\xi_i\) and
\(\xi_i^*\) are set to zero. When the points violate the threshold, the
objective function (first line of the code) is penalized. Note that
setting a large \(\epsilon\) leaves room for more error. Once the model
has been trained, a prediction for \(\textbf{x}_j\) is simply
\(\sum_{k=1}^Kw_k\phi(x_{j,k})+b\).

\begin{figure}[H]

{\centering \includegraphics[width=400px]{images/kernel} 

}

\caption{Examples of nonlinear kernels.}\label{fig:svmscheme4}
\end{figure}

The models laid out in this section are a preview of the universe of SVM
engines. One reference library that is coded in C and C++ is LIBSVM and
it is widely used by many other programming languages. The interested
reader can have a look at the corresponding article
\citet{chang2011libsvm} for more details on the SVM zoo (a more recent
November 2019 version is also available online).

\hypertarget{practice}{%
\section{Practice}\label{practice}}

In R the LIBSVM library is exploited in several packages. One of them,
\emph{e1071}, is a good choice because it also nests many other
interesting functions, especially a naive Bayes classifier that we will
use below.

In the implementation of LIBSVM, the package requires to specify the
label and features separately. For this reason, we recycle the variables
used for the boosted trees. Moreover, the training being slow, we
perform it on a subsample of these sets (first thousand instances).

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(e1071)}
\NormalTok{fit_svm <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ train_label_xgb[}\DecValTok{1}\OperatorTok{:}\DecValTok{1000}\NormalTok{],      }\CommentTok{# Train label}
               \DataTypeTok{x =}\NormalTok{ train_features_xgb[}\DecValTok{1}\OperatorTok{:}\DecValTok{1000}\NormalTok{,],  }\CommentTok{# Training features}
               \DataTypeTok{type =} \StringTok{"eps-regression"}\NormalTok{,          }\CommentTok{# SVM task type (see LIBSVM documentation)}
               \DataTypeTok{kernel =} \StringTok{"radial"}\NormalTok{,                }\CommentTok{# SVM kernel (or: linear, polynomial, sigmoid)}
               \DataTypeTok{epsilon =} \FloatTok{0.1}\NormalTok{,                    }\CommentTok{# Width of strip for errors}
               \DataTypeTok{gamma =} \FloatTok{0.5}\NormalTok{,                      }\CommentTok{# Constant in the radial kernel }
               \DataTypeTok{cost =} \FloatTok{0.1}\NormalTok{)                       }\CommentTok{# Slack variable penalisation}
\KeywordTok{mean}\NormalTok{((}\KeywordTok{predict}\NormalTok{(fit_svm, }
              \KeywordTok{select}\NormalTok{(testing_sample,features_short)) }\OperatorTok{-}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd)}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\CommentTok{# MSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.03839085
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{predict}\NormalTok{(fit_svm, }
             \KeywordTok{select}\NormalTok{(testing_sample,features_short)) }\OperatorTok{*}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{) }\CommentTok{# Hit ratio}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5222197
\end{verbatim}

\normalsize

The results are slightly better than those of the boosted trees. All
parameters are completely arbitrary, especially the choice of the
kernel. We finally turn to a classification example.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(e1071)}
\NormalTok{fit_svm_C <-}\StringTok{ }\KeywordTok{svm}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ training_sample}\OperatorTok{$}\NormalTok{R1M_Usd_C[}\DecValTok{1}\OperatorTok{:}\DecValTok{1000}\NormalTok{],   }\CommentTok{# Train label}
               \DataTypeTok{x =}\NormalTok{ training_sample[}\DecValTok{1}\OperatorTok{:}\DecValTok{1000}\NormalTok{,] }\OperatorTok{%>%}
\StringTok{                   }\KeywordTok{select}\NormalTok{(features),                      }\CommentTok{# Training features}
               \DataTypeTok{type =} \StringTok{"C-classification"}\NormalTok{,                 }\CommentTok{# SVM task type (see LIBSVM documentation)}
               \DataTypeTok{kernel =} \StringTok{"sigmoid"}\NormalTok{,                        }\CommentTok{# SVM kernel}
               \DataTypeTok{gamma =} \FloatTok{0.5}\NormalTok{,                               }\CommentTok{# Parameter in the sigmoid kernel }
               \DataTypeTok{coef0 =} \FloatTok{0.3}\NormalTok{,                               }\CommentTok{# Parameter in the sigmoid kernel }
               \DataTypeTok{cost =} \FloatTok{0.2}\NormalTok{)                                }\CommentTok{# Slack variable penalisation}
\KeywordTok{mean}\NormalTok{(}\KeywordTok{predict}\NormalTok{(fit_svm_C, }\KeywordTok{select}\NormalTok{(testing_sample,features)) }\OperatorTok{==}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd_C) }\CommentTok{# Accuracy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5008973
\end{verbatim}

\normalsize

The arbitrariness in our choice of the parameters may explain why the
predictive accuracy is so poor.

\hypertarget{coding-exercises-5}{%
\section{Coding exercises}\label{coding-exercises-5}}

\hypertarget{bayes}{%
\chapter{Bayesian methods}\label{bayes}}

This section is dedicated to the subset of machine learning that makes
prior assumptions on parameters. Before we explain how Bayes' theorem
can be applied to simple building blocks in machine learning, we
introduce some notations and concept in the subsection below. Good
references for Bayesian analysis are \citet{gelman2013bayesian} and
\citet{kruschke2014doing}. The latter, like the present book,
illustrates the concepts with many lines of R code.

\hypertarget{the-bayesian-framework}{%
\section{The Bayesian framework}\label{the-bayesian-framework}}

Up to now, the models that have been presented rely on data only. This
approach is often referred to as `\emph{frequentist}'. Given one
dataset, a frequentist will extract (i.e., estimate) a unique set of
optimal parameters and consider it to be the best model. Bayesians, on
the other hand, consider datasets as a snapshots of reality and for
them, parameters are thus random! Instead of estimating one value for
parameters, they a more ambitious and try to determine the whole
distribution of the parameter.

In order to outline how that can be achieved, we introduce basic
notations and results. The foundational concept in Bayesian analysis is
the conditional probability. Given two random sets (or events) \(A\) and
\(B\), we define the probability of \(A\) knowing \(B\) (or,
conditionally on \(B\)) as \[P[A|B]=\frac{P[A \cap B]}{P[B]},\] that is,
the probability of the intersection between the two sets divided by the
probability of \(B\). Likewise, the probability that both events occur
is equal to \(P[A \cap B] = P[A]P[B|A]\). Given \(n\) disjoint events
\(A_i\), \(i=1,...n\) such that \(\sum_{i=1}^nP(A_i)=1\), then for any
event \(B\), the law of total probabilities is (or implies)
\[P(B)=\sum_{i=1}^nP(B \cap A_i)= \sum_{i=1}^nP(B|A_i)P(A_i).\]

Given this expression, we can formulate a general version of Bayes'
theorem: \begin{equation}
\label{eq:bayes}
P(A_i|B)=\frac{P(A_i)P(B|A_i)}{P(B)}= \frac{P(A_i)P(B|A_i)}{\sum_{i=1}^nP(B|A_i)P(A_i)}.
\end{equation}

Endowed with this result, we can move forward to the core topic of this
section, which is the estimation of some parameter
\(\boldsymbol{\theta}\) (possibly a vector) given a dataset, which we
denote with \(\textbf{y})\) (following the conventions from
\citet{gelman2013bayesian}).

In Bayesian analysis, one sophistication (compared to a frequentist
approach) comes from the fact that the data is not almighty. The
distribution of the parameter \(\boldsymbol{\theta}\) will be a mix
between some prior distribution set by the statistician (the user, the
analyst) and the empirical distribution from the data. More precisely, a
simple application of Bayes' formula yields \begin{equation}
\label{eq:bayes2}
p(\boldsymbol{\theta}| \textbf{y})=\frac{p(\boldsymbol{\theta})p(\textbf{y} |\boldsymbol{\theta})}{p(\textbf{y})} \propto p(\boldsymbol{\theta})p(\textbf{y} |\boldsymbol{\theta}).
\end{equation}

The interpretation is immediate: the distribution of
\(\boldsymbol{\theta}\) knowing the data \(\textbf{y}\) is proportional
to the distribution of \(\boldsymbol{\theta}\) times the distribution of
\(\textbf{y}\) knowing \(\boldsymbol{\theta}\). The term
\(p(\textbf{y})\) is often omitted because it is simply a scaling number
that ensures that the density sums or integrates to one.

We use a slightly different notation between Equation \eqref{eq:bayes} and
Equation \eqref{eq:bayes2}. In the former, \(P\) denotes a true
probability, i.e., it is a number. In the latter, \(p\) stands for the
whole probability density function of \(\boldsymbol{\theta}\) or
\(\textbf{y}\).

The whole purpose of Bayesian analysis is to compute the so-called
\emph{posterior} distribution \(p(\boldsymbol{\theta}| \textbf{y})\) via
the \emph{prior} distribution \(p(\boldsymbol{\theta})\) and the
\emph{likelihood function} \(p(\textbf{y} |\boldsymbol{\theta})\).
Priors are sometimes qualified as informative, weakly informative or
uninformative, depending on the degree to which the user is confident on
the relevance and robustness of the prior. The simplest way to define a
non-informative prior is to set a constant (uniform) distribution over
some realistic intervals.

The most challenging part is usually the likelihood function. The
easiest way to solve the problem is to resort to a specific distribution
(possibly a parametric family) for the distribution of the data and then
consider that obsevations are i.i.d., just as in a simple maximum
likelihood inference. If we assume that new parameters for the
distributions are gathered into \(\boldsymbol{\lambda}\), then the
likelihood can be written as \begin{equation}
\label{eq:likelihood}
p(\textbf{y} |\boldsymbol{\theta}, \boldsymbol{\lambda})=\prod_{i=1}^I f_{\boldsymbol{\lambda}}(y_i; \boldsymbol{\beta}), 
\end{equation} but in this case the problem because slightly more
complex because adding new parameters change the posterior distribution
to \(p(\boldsymbol{\theta}, \boldsymbol{\lambda}|\textbf{y})\). The user
must find out the joint distribution of \(\boldsymbol{\theta}\) and
\(\boldsymbol{\lambda}\) - given \(\textbf{y})\).

\hypertarget{bayesian-sampling}{%
\section{Bayesian sampling}\label{bayesian-sampling}}

\hypertarget{gibbs-sampling}{%
\subsection{Gibbs sampling}\label{gibbs-sampling}}

One adjacent field of applications of Bayes' theorem is simulation.
Suppose we want to simulate the multivariate distribution of a random
vector \(\textbf{X}\) given by its density \(p=p(x_1,\dots,x_J)\).
Often, the full distribution is complex, but its marginals are more
accessible. Indeed, they are simpler because they depend on only one
variable (when all other values are known):
\[p(X_j=x_j|X_1= x_1,\dots,X_{j-1}=x_{j-1},X_{j+1}=x_{j+1},\dots,X_J=x_J)=p(X_j=x_j|\textbf{X}_{-j}=\textbf{x}_{-j}),\]
where we use the compact notation \(\textbf{X}_{-j}\) for all variables
except \(X_j\). One way to generate samples with law \(p\) is the
following and relies both on the knowledge of the conditionals
\(p(x_j|\textbf{x}_{-j})\) and on the notion of Markov Chain Monte
Carlo. The process is iterative and assumes that it is possible to draw
samples of the aforementioned conditionals. We write \(x_j^{m}\) for the
\(m^{th}\) sample of the \(j^{th}\) variable (\(X_j\)). The simulation
starts with a prior (or fixed, or random) sample
\(\textbf{x}^0=(x^0_1,\dots,x^0_J)\). Then, for a sufficiently large
number of times, say \(T\), new samples are drawn according to
\begin{align*}
x_1^{m+1} &= p(X_1|X_2=x_2^{m}, \dots ,X_J=x_J^m) ;\\
x_2^{m+1} &=p(X_2|X_1=x_1^{m+1}, X_3=x^{m}_3, \dots, X_J=x_J^m); \\
\dots& \\
x_J^{m+1}&= p(X_J|X_1=x_1^{m+1}, X_2=x_2^{m+1}, \dots, X_{J-1}=x_{J-1}^{m+1}).
\end{align*}

The important detail is that after each line, the value of the variable
is updated. Hence, in the second line, \(X_2\) is sampled with the
knowledge of \(X_1=x_1^{m+1}\) and in the last line, all variable except
\(X_J\) have been updated to their \(m+1^{th}\) state. The above
algorithm is called Gibbs sampling. It relates to Markov chains because
each new iteration depends only on the previous one.

Under some technical assumptions, as \(T\) increases, the distrbution of
\(\textbf{x}_T\) converges to that of \(p\). The conditions under which
the convergence occurs have been widely discussed in series of articles
in the 1990s. The interested reader can have a look for instance at
\citet{tierney1994markov}, \citet{roberts1994simple}, as well as at
section 11.7 of \citet{gelman2013bayesian}.

Sometimes, the full distribution is complex and the conditional laws are
hard to determine and to sample. Then, a more general method, called
Metropolis-Hastings, can be used that relies on the rejection method for
the simulation of random variables. For the sake of brevity, we omit the
presentation here, but the methods are outlined in section 11.2 of
\citet{gelman2013bayesian} and in Chapter 7 of
\citet{kruschke2014doing}.

\hypertarget{metropolis-hastings-sampling}{%
\subsection{Metropolis-Hastings
sampling}\label{metropolis-hastings-sampling}}

The Gibbs algorithm can be considered as a particular case of the
Metropolis-Hastings (MH) method, which, is its simplest version, was
introduced in \citet{metropolis1949monte}. The premise is similar: the
aim is to simulate random variables that follow \(p(\textbf{x})\) with
the ability to sample from of a simpler form
\(p(\textbf{x}|\textbf{y})\) which gives the probability of the future
state \(\textbf{x}\), given the past one \(\textbf{y}\).

Once an initial value for \(\textbf{x}\) has been sampled
(\(\textbf{x}_0\)), each new iteration (\(m\)) of the simulation takes
place in three stages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  generate a candidate value \(\textbf{x}'_{m+1}\) from
  \(p(\textbf{x}|\textbf{x}_m)\);\\
\item
  compute the acceptance ratio
  \(\alpha=\min\left(\frac{p(\textbf{x}'_{m+1})p(\textbf{x}_{m}|\textbf{x}'_{m+1})}{p(\textbf{x}_{m})p(\textbf{x}'_{m+1}|\textbf{x}_{m})} \right)\)
\item
  pick \(\textbf{x}_{m+1}=\textbf{x}'_{m+1}\) with probability
  \(\alpha\) or stick with the previous value
  (\(\textbf{x}_{m+1}=\textbf{x}_{m}\)) with probability \(1-\alpha\)
\end{enumerate}

The interpretation of the acceptance ratio is not straightforward in the
general case. When the sampling generator is symmetric
(\(p(\textbf{x}|\textbf{y})=(\textbf{y}|\textbf{x})\)), the candidate is
always chosen whenever \(p(\textbf{x}'_{m+1})\ge p(\textbf{x}_{m})\). If
the reverse condition holds, then it is retained with odds equal to
\(p(\textbf{x}'_{m+1})/p(\textbf{x}_{m})\), which is the ratio of
likelihoods. The more likely the new proposal, the higher the odds of
retaining it.

Often, the first simulations are discarded in order to leave time to the
chain to converge to a high probability region. This procedure (often
called `burn in') ensures that the first retained samples are located in
a zone that is likely, that is: they are more representative of the law
we are trying to simulate.

\hypertarget{bayesian-linear-regression}{%
\section{Bayesian linear regression}\label{bayesian-linear-regression}}

Because Bayesian concepts are rather abstract, it is useful to
illustrate the theoretical notions with a simple example. In a linear
model, \(y_i=\textbf{x}_i\textbf{b}+\epsilon_i\) and it is often
statistically assumed that the \(\epsilon_i\) are i.i.d. and normally
distributed with zero mean and variance \(\sigma^2\). Hence, Equation
\eqref{eq:likelihood} translates into
\[p(\boldsymbol{\epsilon}|\textbf{b}, \sigma)=\prod_{i=1}^I\frac{e^{-\frac{\epsilon_i^2}{2\sigma}}}{\sigma\sqrt{2\pi}}=(\sigma\sqrt{2\pi})^{-I}e^{-\sum_{i=1}^I\frac{\epsilon_i^2}{2\sigma^2}}.\]

In a regression analysis, the data is given both by \(\textbf{y}\) and
by \(\textbf{X}\), hence both are reported in the notations. Simply
acknowledging that \(\boldsymbol{\epsilon}=\textbf{y}-\textbf{Xb}\), we
get \begin{align}
p(\textbf{y},\textbf{X}|\textbf{b}, \sigma)=\prod_{i=1}^I\frac{e^{-\frac{\epsilon_i^2}{2\sigma}}}{\sigma\sqrt{2\pi}}=(\sigma\sqrt{2\pi})^{-I}e^{-\sum_{i=1}^I\frac{\left(y_i-\textbf{x}_i'\textbf{b}\right)^2}{2\sigma^2}}=(\sigma\sqrt{2\pi})^{-I} e^{-\frac{\left(\textbf{y}-\textbf{X}\textbf{b}\right)' \left(\textbf{y}-\textbf{X}\textbf{b}\right)}{2\sigma^2}} \nonumber \\ \label{eq:linlike}
=\underbrace{(\sigma\sqrt{2\pi})^{-I} e^{-\frac{\left(\textbf{y}-\textbf{X}\hat{\textbf{b}}\right)' \left(\textbf{y}-\textbf{X}\hat{\textbf{b}}\right)}{2\sigma^2}}}_{\text{depends on } \sigma, \text{ not } \textbf{b}}\times \underbrace{e^{-\frac{(\textbf{b}-\hat{\textbf{b}})'\textbf{X}'\textbf{X}(\textbf{b}-\hat{\textbf{b}})}{2\sigma^2}}}_{\text{ depends on both } \sigma, \text{ and } \textbf{b} },
\end{align} where the last line is very convenient because it expresses
the law as a function of the difference \(\textbf{b}-\hat{\textbf{b}}\),
where
\(\hat{\textbf{b}}=(\textbf{X}'\textbf{X})^{-1}\textbf{X}'\textbf{y}\)
seems like a natural choice for the mean of \(\textbf{b}\) - though in
the end this vector is chosen for the simple form it produces.

The above expression is the frequentist (data-based) block of the
posterior. If we want to obtain a tractable expression for the
posterior, we need to find a prior that has a form that will combine
well with the likelihood. These forms are called \emph{conjugate
priors}. A natural candidate for the right part (that depends on both
\textbf{b} and \(\sigma\)) is the multivariate Gaussian density:
\begin{equation}
\label{eq:linprior}
p[\textbf{b}|\sigma]=\sigma^{-k}e^{-\frac{(\textbf{b}-\textbf{b}_0)'\boldsymbol{\Lambda}_0(\textbf{b}-\textbf{b}_0)}{2\sigma^2}},
\end{equation} where we are obliged to condition with respect to
\(\sigma\). The density has prior mean \(\textbf{b}_0\) and prior
covariance matrix \(\boldsymbol{\Lambda}_0^{-1}\). This prior gets us
one step closer to the posterior because \begin{align}
p[\textbf{b},\sigma|\textbf{y},\textbf{X}]& \propto p[\textbf{y},\textbf{X}|\textbf{b},\sigma]p[\textbf{b},\sigma] \nonumber \\
\label{eq:cascade}
&\propto p[\textbf{y},\textbf{X}|\textbf{b},\sigma]p[\textbf{b}|\sigma]p[\sigma]. 
\end{align}

In order to fully specify the cascade of probability, we need to take
care of \(\sigma\) and set a density of the form \begin{equation}
\label{eq:linsig}
p[\sigma^2]\propto (\sigma^2)^{-1-a_0}e^{-\frac{b_0}{2\sigma^2}},
\end{equation} which is close to that of the left part of
\eqref{eq:linlike}. This corresponds to an inverse gamma distribution for
the variance with prior parameters \(a_0\) and \(b_0\) (this scalar
notation is not optimal because it can be confused with the prior mean
\(\textbf{b}_0\) so we must pay extra attention).

Now, we can simplify \(p[\textbf{b},\sigma|\textbf{y},\textbf{X}]\) with
\eqref{eq:linlike}, \eqref{eq:linprior} and \eqref{eq:linsig}: \begin{align*}
p[\textbf{b},\sigma|\textbf{y},\textbf{X}]& \propto 
(\sigma\sqrt{2\pi})^{-I} \sigma^{-2(1+a_0)} e^{-\frac{\left(\textbf{y}-\textbf{X}\hat{\textbf{b}}\right)' \left(\textbf{y}-\textbf{X}\hat{\textbf{b}}\right)}{2\sigma^2}}\times e^{-\frac{(\textbf{b}-\hat{\textbf{b}})'\textbf{X}'\textbf{X}(\textbf{b}-\hat{\textbf{b}})}{2\sigma^2}}\sigma^{-k}e^{-\frac{(\textbf{b}-\textbf{b}_0)'\boldsymbol{\Lambda}_0(\textbf{b}-\textbf{b}_0)}{2\sigma^2}}e^{-\frac{b_0}{2\sigma^2}} \\
& \propto \sigma^{-I-k-2(1+a_0)}\exp\left(-\frac{\left(\textbf{y}-\textbf{X}\hat{\textbf{b}}\right)' \left(\textbf{y}-\textbf{X}\hat{\textbf{b}}\right) + (\textbf{b}-\hat{\textbf{b}})'\textbf{X}'\textbf{X}(\textbf{b}-\hat{\textbf{b}}) + (\textbf{b}-\textbf{b}_0)'\boldsymbol{\Lambda}_0(\textbf{b}-\textbf{b}_0)+b_0}{2\sigma^2} \right) 
\end{align*}

The above expression is simply a quadratic form in \(\textbf{b}\) and it
can be rewritten after burdensome algebra in a much more compact manner:
\begin{equation}
\label{eq:linpost}
p(\textbf{b}|\textbf{y},\textbf{X},\sigma) \propto \left[\sigma^{-k}e^{-\frac{(\textbf{b}-\textbf{b}_*)'\boldsymbol{\Lambda}_*(\textbf{b}-\textbf{b}_*)}{2\sigma^2}}\right] \times \left[ (\sigma^2)^{-1-a_*}e^{-\frac{b_*}{2\sigma^2}}  \right],
\end{equation}

where \begin{align*}
\boldsymbol{\Lambda}_* &= \textbf{X}'\textbf{X}+\boldsymbol{\Lambda}_0  \\
\textbf{b}_*&=  \boldsymbol{\Lambda}_*^{-1}(\boldsymbol{\Lambda}_0\textbf{b}_0+\textbf{X}'\textbf{X}\hat{\textbf{b}}) \\
a_* & = a_0 + I/2  \\
b_* &=b_0+\frac{1}{2}\left(\textbf{y}'\textbf{y}+ \textbf{b}_0'\boldsymbol{\Lambda}_0\textbf{b}_0+\textbf{b}_*'\boldsymbol{\Lambda}_*\textbf{b}_* \right)\\
\end{align*}

This expression has two parts: the Gaussian component which relates
mostly to \(\textbf{b}\), and the inverse gamma component, entirely
dedicated to \(\sigma\). The mix between the prior and the data is
clear. The posterior covariance matrix of the Gaussian part
(\(\boldsymbol{\Lambda}_*\)) is the sum between the prior and a
quadratic form from the data. The posterior mean \(\textbf{b}_*\) is a
weighted average of the prior \(\textbf{b}_0\) and the sample estimator
\(\hat{\textbf{b}}\). Such blends of quantities estimated from data and
a user-supplied version are often called \emph{shrinkages}. The original
covariance matrix \(\textbf{X}'\textbf{X}\) is shrunk towards the prior
\(\boldsymbol{\Lambda}_0\). This can be viewed as a regularization
procedure: the pure fit originating from the data is mixed with some
`external' ingredient to give some structure to the final extimation.

The interested reader can also have a look at section 16.3 of
\citet{greene2018econometric} (the case of conjugate priors is treated
in section 16.3.2).

The formulae above can be long and risky to implement. Luckily, there is
a package that performs Bayesian inference for linear regression using
the conjugate priors. Below, we provide one example of how it works. To
simplify the code and curtail computation times, we consider two
predictors. In statistics, the precision matrix is the inverse of the
covariance matrix. In the parameters, the first two priors relate to the
Gaussian law and the last two to the inverse gamma distribution:
\[f_\text{invgamma}(x, \alpha, \beta)=\frac{\beta^\alpha}{\Gamma(\alpha)}x^{-1-\alpha}e^{-\frac{\beta}{x}},\]
where \(\alpha\) is the shape and \(\beta\) is the scale.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prior_mean <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{,}\FloatTok{0.1}\NormalTok{,}\FloatTok{0.1}\NormalTok{)                    }\CommentTok{# Average value of parameters (prior)}
\NormalTok{precision_mat <-}\StringTok{ }\KeywordTok{diag}\NormalTok{(prior_mean}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{solve}\NormalTok{()  }\CommentTok{# Inverse covariance matrix of parameters (prior)}
\NormalTok{fit_lmBayes <-}\StringTok{ }\KeywordTok{bayesLMConjugate}\NormalTok{(}
\NormalTok{    R1M_Usd }\OperatorTok{~}\StringTok{ }\NormalTok{Mkt_Cap_3M_Usd }\OperatorTok{+}\StringTok{ }\NormalTok{Pb,          }\CommentTok{# Model: size and value}
    \DataTypeTok{data =}\NormalTok{ testing_sample,                  }\CommentTok{# Data source, here, the test sample}
    \DataTypeTok{n.samples =} \DecValTok{2000}\NormalTok{,                       }\CommentTok{# Number of samples used}
    \DataTypeTok{beta.prior.mean =}\NormalTok{ prior_mean,           }\CommentTok{# Avg prior: size & value rewarded & unit beta}
    \DataTypeTok{beta.prior.precision =}\NormalTok{ precision_mat,   }\CommentTok{# Precision matrix}
    \DataTypeTok{prior.shape =} \FloatTok{0.5}\NormalTok{,                      }\CommentTok{# Shape for prior distribution of sigma}
    \DataTypeTok{prior.rate =} \FloatTok{0.5}\NormalTok{)                       }\CommentTok{# Scale for prior distribution fo sigma}
\end{Highlighting}
\end{Shaded}

\normalsize

In the above specification, we must also provide a prior for the
constant. By default, we set its average value to 0.01, which
corresponds to a 1\% average monthly return.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit_lmBayes}\OperatorTok{$}\NormalTok{p.beta.tauSq.samples[,}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as_tibble}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    `}\DataTypeTok{colnames<-}\StringTok{`}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"Intercept"}\NormalTok{, }\StringTok{"Size"}\NormalTok{, }\StringTok{"Value"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{gather}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ coefficient, }\DataTypeTok{value =}\NormalTok{ value) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ value, }\DataTypeTok{fill =}\NormalTok{ coefficient)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=320px]{ML_factor_files/figure-latex/lmBayesplot-1} 

}

\caption{Distribution of linear regression coefficients.}\label{fig:lmBayesplot}
\end{figure}

\normalsize

The distribution of the constant is firmly to the right, hence soldily
positive. For the size coefficient it is the opposite: it is negative
(small firms are more profitable). With regard to value, it is hard to
conclude, the distribution is balanced around zero.

\hypertarget{naive-bayes-classifier}{%
\section{Naive Bayes classifier}\label{naive-bayes-classifier}}

Bayes' theorem can also be easily applied to classification. We
formulate it with respect to the label and features and write
\begin{equation}
\label{eq:naivebayes}
P[\textbf{y} | \textbf{X}] = \frac{P[ \textbf{X} | \textbf{y}]P[\textbf{y}]}{P[\textbf{X}]} \propto P[ \textbf{X} | \textbf{y}]P[\textbf{y}],
\end{equation} and then split the input matrix into its column vectors
\(\textbf{X}=(\textbf{x}_1,\dots,\textbf{x}_K)\). This yields
\begin{equation}
\label{eq:naivebayes2}
P[\textbf{y} | \textbf{x}_1,\dots,\textbf{x}_K] \propto P[\textbf{x}_1,\dots,\textbf{x}_K| \textbf{y}]P\textbf{y}].
\end{equation}

The `naive' qualification of the method comes from a simplifying
assumption on the features.\footnote{This assumption can be relaxed, but
  the algorithms then become more complex and are out of the scope of
  the current book. One such example that generalizes the naive Bayes
  approach is \citet{friedman1997bayesian}.} If they are all mutually
independent, then the likelihood in the above expression can be expanded
into \begin{equation}
\label{eq:naivebayes3}
P[\textbf{y} | \textbf{x}_1,\dots,\textbf{x}_K] \propto P[\textbf{y}]\prod_{k=1}^K P[\textbf{x}_k| \textbf{y}].
\end{equation}

The next step is to be more specific about the likelihood. This can be
done non-parametrically (via kernel estimation) or with common
distributions (Gaussian for continuous data, Bernoulli for binary data).
In factor investing, the features are continuous, thus the Gaussian law
is more adequate:
\[P[x_{i,k}=z|\textbf{y}_i= c]=\frac{e^{-\frac{(z-m_c)^2}{2\sigma_c^2}}}{\sigma_c\sqrt{2\pi}},\]
where \(c\) is the value of the classes taken by \(y\) and \(\sigma_c\)
and \(m_c\) are the standard error and mean of \(x_{i,k}\), conditional
on \(y_i\) being equal to \(c\). In practice, each class is spanned, the
training set is filtered accordingly and \(\sigma_c\) and \(m_c\) are
taken to be the sample statistics. This Gaussian parametrization is
probably ill-suited to our dataset because the features are uniformly
distributed. Even after conditioning, it is unlikely that the
distribution will be even remotely close to Gaussian. Technically, this
can be overcome via a double transformation method. Given a vector of
features \(\textbf{x}_k\) with empirical cdf \(F_{\textbf{x}_k}\), the
variable \begin{equation}
\label{eq:transf}
\tilde{\textbf{x}}_k=\Phi^{-1}\left(F_{\textbf{x}_k}(\textbf{x}_k) \right),
\end{equation} will have a standard normal law whenever
\(F_{\textbf{x}_k}\) is not pathological (lies in (0,1) for instance).
If all features are independent, the transformation should not have any
impact on the correlation structure. Otherwise, we refer to the
literature on the NORmal-To-Anything (NORTA) method (see, e.g.,
\citet{chen2001initialization}, \citet{coqueret2017approximate}).

Lastly, the prior \(P[\textbf{y}]\) in Equation \eqref{eq:naivebayes3} is
often either taken to be uniform across the classes (\(1/K\) for all
\(k\)) or equal to the sample distribution.

We illustrate the naive Bayes classification tool with a simple example.
While the package \emph{e1071} embeds such a classifier, the
\emph{naivebayes} library offers more options (Gaussian, Bernoulli,
multinomial and nonparametric likelihoods). Below, since the features
are uniformly distributed, thus the transformation in \eqref{eq:transf}
amounts to apply the inverse Gaussian cdf.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(naivebayes)}
\NormalTok{gauss_features_train <-}\StringTok{ }\NormalTok{training_sample }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{select}\NormalTok{(features_short) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{as.matrix}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    `}\DataTypeTok{*}\StringTok{`}\NormalTok{(}\FloatTok{0.999}\NormalTok{) }\OperatorTok{%>%}
\StringTok{    }\OperatorTok{+}\StringTok{ }\NormalTok{(}\FloatTok{0.0001}\NormalTok{) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{qnorm}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    `}\DataTypeTok{colnames<-}\StringTok{`}\NormalTok{(features_short)}
\NormalTok{fit_NB_gauss <-}\StringTok{ }\KeywordTok{naive_bayes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ gauss_features_train,      }\CommentTok{# Transformed features}
                            \DataTypeTok{y =}\NormalTok{ training_sample}\OperatorTok{$}\NormalTok{R1M_Usd_C) }\CommentTok{# Label}
\KeywordTok{layout}\NormalTok{(}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{,}\DecValTok{7}\NormalTok{), }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DataTypeTok{byrow =} \OtherTok{TRUE}\NormalTok{), }\DataTypeTok{widths=}\KeywordTok{c}\NormalTok{(}\FloatTok{0.9}\NormalTok{,}\FloatTok{0.45}\NormalTok{))}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mar=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(fit_NB_gauss, }\DataTypeTok{prob =} \StringTok{"conditional"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=500px]{ML_factor_files/figure-latex/NB-1} 

}

\caption{Distributions of predictor variables, conditional on the class of the label. TRUE is when the instance corresponds to an above median return and FALSE to a below median return.}\label{fig:NB}
\end{figure}

\normalsize

The plots show the distributions of the features, conditionally on each
value of the label. Essentially, those are the densities
\(P[\textbf{x}_k| \textbf{y}]\). For each feature, both distributions
are very similar.

As usual, when the model has been trained, the accuracy of predictions
can be evaluated.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gauss_features_test <-}\StringTok{ }\NormalTok{testing_sample }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{select}\NormalTok{(features_short) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{as.matrix}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    `}\DataTypeTok{*}\StringTok{`}\NormalTok{(}\FloatTok{0.999}\NormalTok{) }\OperatorTok{%>%}
\StringTok{    }\OperatorTok{+}\StringTok{ }\NormalTok{(}\FloatTok{0.0001}\NormalTok{) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{qnorm}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    `}\DataTypeTok{colnames<-}\StringTok{`}\NormalTok{(features_short)}
\KeywordTok{mean}\NormalTok{(}\KeywordTok{predict}\NormalTok{(fit_NB_gauss, gauss_features_test) }\OperatorTok{==}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd_C) }\CommentTok{# Hit ratio}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.4956985
\end{verbatim}

\normalsize

The performance of the classifier is not satisfactory as it
underperforms a random guess.

\hypertarget{BART}{%
\section{Bayesian additive trees}\label{BART}}

\hypertarget{general-formulation}{%
\subsection{General formulation}\label{general-formulation}}

Bayesian additive regression trees (BARTs) are an ensemble technique
that mixes Bayesian thinking and regression trees. In spirit, they are
close to boosted trees, but they differ greatly in their implementation.
In BARTs like in Bayesian regressions, the regularization comes from the
prior. The original article is \citet{chipman2010bart} and the
implementation (in R) follows \citet{sparapani2019r}.

Formally, the model is an aggregation of \(M\) models, which we write as
\begin{equation}
\label{eq:BART}
y = \sum_{m=1}^M\mathcal{T}_m(q_m,\textbf{w}_m, \textbf{x}) + \epsilon,
\end{equation} where \(\epsilon\) is a Gaussian noise with variance
\(\sigma^2\), and the
\(\mathcal{T}_m=\mathcal{T}_m(q_m,\textbf{w}_m, \textbf{x})\) are
decision trees with structure \(q_m\) and weights vectors
\(\textbf{w}_m\). This decomposition of the tree is the one we used for
boosted trees and is illustrated in Figure \ref{fig:treeq}. \(q_m\)
codes all splits (variables chosen for the splits and levels of the
splits) and the vectors \(\textbf{w}_m\) correspond to the leaf values
(at the terminal nodes).

At the macro level, BARTs can be viewed as traditional Bayesian objects,
where the parameters \(\boldsymbol{\theta}\) are all of the unknowns
coded through \(q_m\), \(\textbf{w}_m\) and \(\sigma^2\) and where the
focus is set on determining the posterior \begin{equation}
\label{eq:bartpost}
\left(q_m,\textbf{w}_m,\sigma^2\right) | (\textbf{X}, \textbf{Y}).
\end{equation}

Given particular forms of priors for
\(\left(q_m,\textbf{w}_m,\sigma^2\right)\), the algorithm draws the
parameters using a combination of Metropolis-Hastings \emph{and} Gibbs
samplers.

\hypertarget{priors}{%
\subsection{Priors}\label{priors}}

The definition of prior in tree models is delicate and intricate. The
first important assumption is independence: independence between
\(\sigma^2\) and all other parameters and independence between trees,
that is, between couples \((q_m,\textbf{w}_m)\) and
\((q_n,\textbf{w}_n)\) for \(m\neq n\). This entails

\[P(\left(q_1,\textbf{w}_1\right),\dots,\left(q_M,\textbf{w}_M\right),\sigma^2)=P(\sigma^2)\prod_{m=1}^MP\left(q_m,\textbf{w}_m\right).\]

Moreover, it is customary (for simplicity) to separate the structure of
the tree (\(q_m\)) and the terminal weights (\(\textbf{w}_m\)), so that
\begin{equation}
\label{eq:bart1}
P(\left(q_1,\textbf{w}_1\right),\dots,\left(q_M,\textbf{w}_M\right),\sigma^2)=\underbrace{P(\sigma^2)}_{\text{noise term}}\prod_{m=1}^M\underbrace{P\left(\textbf{w}_m|q_m\right)}_{\text{tree weights}}\underbrace{P(q_m)}_{\text{tree struct.}}
\end{equation}

It remains to formulate the assumptions for each of the three parts.

We start with the trees' structures, \(q_m\). Trees are defined by their
splits (at nodes) and these splits are characterized by the splitting
variable and the splitting level. First, the size of trees is
parametrized such that a node at depth \(d\) is nonterminal with
probability given by \begin{equation}
\label{eq:bartnode}
\alpha(1+d)^{-\beta}, \quad \alpha \in (0,1), \quad \beta >0.
\end{equation} The authors recommend to set \(\alpha = 0.95\) and
\(\beta=2\). This gives a probability of 5\% to have 1 node, 55\% to
have 2 nodes, 28\% to have 3 nodes, 9\% to have 4 nodes and 3\% to have
5 nodes. Thus, the aim is to force shallow structures.

Second, the choice of splitting variables is driven by a generalized
Bernoulli (categorical) distribution which defines the odds of picking
one particular feature. In the original paper by
\citet{chipman2010bart}, the vector of probabilities was uniform (each
predictor has the same odds of being chosen for the split). This vector
can also be random and sampled from a Dirichlet distribution. The level
of the split is drawn uniformly on the set of possible values for the
chosen predictor.

Having determined the prior of structure of the tree \(q_m\), it remains
to fix the terminal values at the leaves (\(\textbf{w}_m|q_m\)). The
weights at all leaves are assumed to follow a Gaussian distribution
\(\mathcal{N}(\mu_\mu,\sigma_\mu^2)\), where
\(\mu_\mu=(y_\text{min}+y_\text{max})/2\) is the center of the range of
the label values. The variance \(\sigma_\mu^2\) is chosen such that
\(\mu_\mu\) plus or minus two times \(\sigma_\mu^2\) covers 95\% of the
range observed in the training dataset. Those are default values and can
be altered by the user.

Lastly, for computational purposes similar to those of linear
regressions, the parameter \(\sigma^2\) (the variance of \(\epsilon\) in
\eqref{eq:BART}) is assumed to follow and inverse Gamma law
\(\text{IG}(\nu/2,\lambda \nu/2)\). The parameters are by default
computed from the data so that the distribution of \(\sigma^2\) is
realistic and prevents overfitting. We refer to the original article,
section 2.2.4, for more details on this topic.

In sum, in addition to \(M\) (number of trees), the prior depends on a
small number of parameters: \(\alpha\) and \(\beta\) (for the tree
structure), \(\mu_\mu\) and \(\sigma_\mu^2\) (for the tree weights) and
\(\nu\) and \(\lambda\) (for the noise term).

\hypertarget{sampling-and-predictions}{%
\subsection{Sampling and predictions}\label{sampling-and-predictions}}

The posterior distribution in \eqref{eq:bartpost} cannot be obtained
analytically but simulations are an efficient shortcut to the model
\eqref{eq:BART}. Just as in Gibbs and Metropolis-Hastings sampling, the
distribution of simulations is expected to converge to the sought
posterior. After some burn-in sample, a prediction for a newly observed
set \(\textbf{x}_*\) will simply be the average (or median) of the
predictions from the simulations. If we assume \(S\) simulations after
burn-in, then the average is equal to
\[\tilde{y}(\textbf{x}_*):=\frac{1}{S}\sum_{s=1}^S\sum_{m=1}^M\mathcal{T}_m\left(q_m^{(s)},\textbf{w}_m^{(s)}, \textbf{x}_*\right).\]

The complex part is naturally to generate the simulations. Each tree is
sampled using the Metropolis-Hastings method: a tree is proposed, but it
replaces the existing one only under some (possibly random) criterion.
This procedure is then repeated in a Gibbs-like fashion.

Let us start with the WH building block. We seek to simulate the
conditional distribution

\[(q_m,\textbf{w}_m) \ | \ (q_{-m},\textbf{w}_{-m},\sigma^2, \textbf{y}, \textbf{x}),\]
where \(q_{-m}\) and \(\textbf{w}_{-m}\) collect the structures and
weights of all trees except for tree number \(m\). One tour de force in
BART is to simplify the above Gibbs draws to
\[(q_m,\textbf{w}_m) \ | \ (\textbf{R}_{m},\sigma^2 ),\] where
\(\textbf{R}_{m}=\textbf{y}-\sum_{l \neq m}\mathcal{T}_l(q_l,\textbf{w}_l, \textbf{x})\)
is the partial residual on a prediction that excludes the \(m^{th}\)
tree.

The new MH proposition for \(q_m\) is based on the previous tree and
there are four possible (and random) alterations to the tree:\\
- growing a terminal node (increase the complexity of the tree by adding
a supplementary leaf);\\
- pruning a pair of terminal nodes (the opposite operation);\\
- changing splitting rules.

For simplicity, the third option is often excluded. Once the tree
structure is defined (i.e., sampled), the terminal weights are
independently drawn according to a Gaussian distribution
\(\mathcal{N}(\mu_\mu, \sigma_\mu^2)\).

After the tree is sampled, the WH principle requires that it be accepted
or rejected based on some probability. This probability increases with
the odds that the new tree increases the likelihood of the model. Its
detailed computation is cumbersome and we refer to Section 2.2 in
\citet{sparapani2019r} for details on the matter.

Now, we must outline the overarching Gibbs procedure. First, the
algorithm starts with trees that are simple nodes. Then, a speficied
number of loops include the following \emph{sequential} steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  sample \((q_1,\textbf{w}_1) \ | \ (\textbf{R}_{1},\sigma^2 )\);\\
\item
  sample \((q_2,\textbf{w}_2) \ | \ (\textbf{R}_{2},\sigma^2 )\);\\
  \ldots{};\\
  M. sample \((q_M,\textbf{w}_M) \ | \ (\textbf{R}_{M},\sigma^2 )\);\\
  M+1. sample \(\sigma^2\) given the full residual
  \(\textbf{R}=\textbf{y}-\sum_{l=1}^M\mathcal{T}_l(q_l,\textbf{w}_l, \textbf{x})\)
\end{enumerate}

At each step \(m\), the residual \(\textbf{R}_{m}\) is updated with the
values from step \(m-1\). We illustrate this process in Figure
\ref{fig:bartfig} in which \(M=3\). At step 1, a partition is proposed
for the first tree, which is a simple node. In this particular case, the
tree is accepted. In this scheme, the terminal weights are omitted for
simplicity. At step two, another partition is proposed for the tree, but
it is rejected. In the third step, the proposition for the third is
accepted. After the third step, a new value for \(\sigma^2\) is drawn
and a new round of Gibbs sampling can commence.

\begin{figure}[b]

{\centering \includegraphics[width=260px]{images/bart} 

}

\caption{Diagram of the MH/Gibbs sampling of BARTs.}\label{fig:bartfig}
\end{figure}

\hypertarget{code}{%
\subsection{Code}\label{code}}

There are several R packages that implement BART methods: \emph{BART},
\emph{bartMachine} and an older one (the original), \emph{BayesTree}.
The first one is highly efficient, hence we work with it. We resort to
only a few parameters, like the power and base, which are the \(\beta\)
and \(\alpha\) defined in \eqref{eq:bartnode}. The program is a bit
verbose and delivers a few parametric details.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit_bart <-}\StringTok{ }\KeywordTok{gbart}\NormalTok{( }
    \DataTypeTok{x.train =} \KeywordTok{select}\NormalTok{(training_sample, features_short) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(), }\CommentTok{# Training features}
    \DataTypeTok{y.train =} \KeywordTok{select}\NormalTok{(training_sample, R1M_Usd) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{() ,        }\CommentTok{# Training label}
    \DataTypeTok{x.test =} \KeywordTok{select}\NormalTok{(testing_sample, features_short)  }\OperatorTok{%>%}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(),  }\CommentTok{# Testing features}
    \DataTypeTok{type =} \StringTok{"wbart"}\NormalTok{,                                                     }\CommentTok{# Option: label is continuous}
    \DataTypeTok{ntree =} \DecValTok{20}\NormalTok{,                                                         }\CommentTok{# Number of trees in the model }
    \DataTypeTok{nskip =} \DecValTok{100}\NormalTok{,                                                        }\CommentTok{# Size of burn-in sample}
    \DataTypeTok{ndpost =} \DecValTok{200}\NormalTok{,                                                       }\CommentTok{# Number of posteriors drawn}
    \DataTypeTok{power =} \DecValTok{2}\NormalTok{,                                                          }\CommentTok{# beta in the tree structure prior}
    \DataTypeTok{base =} \FloatTok{0.95}\NormalTok{)                                                        }\CommentTok{# alpha in the tree structure prior}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## *****Calling gbart: type=1
## *****Data:
## data:n,p,np: 198128, 7, 70208
## y1,yn: -0.049921, 0.024079
## x1,x[n*p]: 0.010000, 0.810000
## xp1,xp[np*p]: 0.270000, 0.880000
## *****Number of Trees: 20
## *****Number of Cut Points: 100 ... 100
## *****burn,nd,thin: 100,200,1
## *****Prior:beta,alpha,tau,nu,lambda,offset: 2,0.95,1.57391,3,2.84908e-31,0.0139209
## *****sigma: 0.000000
## *****w (weights): 1.000000 ... 1.000000
## *****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,7,0
## *****printevery: 100
## 
## MCMC
## done 0 (out of 300)
## done 100 (out of 300)
## done 200 (out of 300)
## time: 26s
## trcnt,tecnt: 200,200
\end{verbatim}

\normalsize

Once the model is trained,\footnote{In the case of BARTs, the training
  is consists exactly in the drawing of posterior samples.} we evaluated
its performance. We simply compute the hit ratio. The predictions are
embedded within the fit variable, under the name `\emph{yhat.test}'.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(fit_bart}\OperatorTok{$}\NormalTok{yhat.test }\OperatorTok{*}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.542666
\end{verbatim}

\normalsize

The performance \emph{seems} reasonable but is by no means not
impressive.

\hypertarget{valtune}{%
\chapter{Validating and tuning}\label{valtune}}

As is shown in chapters \ref{lasso} to \ref{ensemble}, ML models require
user-specified choices before they can be trained. These choices
encompass parameter values (learning rate, penalization intensity, etc.)
or architectural choices (e.g., the structure of a network). Alternative
designs in ML engines can lead to different predictions, hence selecting
a good one can be critical. We refer to the work of
\citet{probst2018tunability} for a study on the impact of hyperparameter
tuning. For some models (neural networks and boosted trees), the number
of degrees of freedom is so large that finding the right parameters can
become complicated and challenging. This chapter addresses these issues
but the reader must be aware that there is no shortcut to building good
models. Crafting an effective model is time-consuming and often the
result of many iterations.

\hypertarget{mlmetrics}{%
\section{Learning metrics}\label{mlmetrics}}

The parameter values that are set before training are called
\textbf{hyperparameters}. In order to be able to choose good
hyperparameters, it is imperative to define metrics that evaluate the
performance of ML models. As is often the case in ML, there is a
dichotomy between models that seek to predict numbers (regressions) and
those that try to forecast categories (classifications).

\hypertarget{regression-analysis}{%
\subsection{Regression analysis}\label{regression-analysis}}

Errors in regression analyses are usually evaluated in a straightforward
way. The \(L^1\) and \(L^2\) norms are mainstream; they are both easy to
interpret and to compute. The second one, the root mean squared error
(RMSE) is differentiable everywhere but harder to grasp and gives more
weight to outliers. The first one, the mean absolute error gives the
average distance to the realized value but is not differentiable at
zero. Formally, we define them as \begin{align}
 \label{eq:MAE}
\text{MAE}(\textbf{y},\tilde{\textbf{y}})&=\frac{1}{I}\sum_{i=1}^I|y_i-\tilde{y}_i|, \\  \label{eq:MSE} 
\text{MSE}(\textbf{y},\tilde{\textbf{y}})&=\frac{1}{I}\sum_{i=1}^I(y_i-\tilde{y}_i)^2, 
\end{align}

and the RMSE is simply the square root of the MSE.

These metrics are widely used outside ML to assess forecasting errors.
Below, we present other indicators that are also sometimes used to
quantify the quality of a model. In line with the linear regressions,
the \(R^2\) can be computed in any predictive exercise. \begin{equation}
\label{eq:R2} 
R^2(\textbf{y},\tilde{\textbf{y}})=1- \frac{\sum_{i=1}^I(y_i-\tilde{y}_i)^2}{\sum_{i=1}^I(y_i-\bar{y})^2},
\end{equation} where \(\bar{y}\) is the sample average of the label. One
important difference with the classical \(R^2\) is that the above
quantity can be computed on the testing sample and not on the training
sample. In this case, the \(R^2\) can be negative when the mean squared
error in the numerator is larger than the (biased) variance of the
testing sample. Sometimes, the average value \(\bar{y}\) is omitted in
the denominator (as in \citet{gu2018empirical} for instance). The
benefit of removing the average value is that it compares the
predictions of the model to a zero prediction. This is particularly
relevant with returns because the simplest prediction of all is the
constant zero value and the \(R^2\) can then measure if the model beats
this naive benchmark. A zero prediction is always preferable to a sample
average because the latter can bery much be period dependent.

Beyond the simple indicators detailed above, several exotic extensions
exist and they all consist in altering the error before taking the
averages. Two notable examples are the Mean Absolute Percentage Error
(MAPE) and the Mean Square Percentage Error (MSPE). Instead of looking
at the raw error, they compute the error relative to the original value
(to be predicted). Hence, the error is expressed in a percentage score
and the averages are simply equal to:

\begin{align} \label{eq:MAPE} 
\text{MAPE}(\textbf{y},\tilde{\textbf{y}})&=\frac{1}{I}\sum_{i=1}^I\left|\frac{y_i-\tilde{y}_i}{y_i}\right|,  \\ \label{eq:MSPE} 
\text{MSPE}(\textbf{y},\tilde{\textbf{y}})&=\frac{1}{I}\sum_{i=1}^I\left(\frac{y_i-\tilde{y}_i}{y_i}\right)^2,
\end{align}

where the latter can be scaled by a square root if need be. When the
label is positive with possibly large values, it is possible to scale
the magnitude of errors, which can be very large. One way to do this is
to resort to the Root Mean Squared Logarithmic Error (RMSLE), defined
below:

\begin{equation}
 \label{eq:RMSLE} 
\text{RMSLE}(\textbf{y},\tilde{\textbf{y}})=\sqrt{\frac{1}{I}\sum_{i=1}^I\log\left(\frac{1+y_i}{1+\tilde{y}_i}\right)}, 
\end{equation}

where it is obvious that when \(y_i=\tilde{y}_i\), the error metric is
equal to zero.

\hypertarget{classification-analysis}{%
\subsection{Classification analysis}\label{classification-analysis}}

The performance metrics for categorical outcomes are substantially
different compared to those of numerical outputs. A large proportion of
these metrics are dedicated to binary classes, though some of them can
easily be generalized to multiclass models.

We present the concepts pertaining to these metrics in an increasing
order of complexity and start with the notion of true/false
positive/negative. In binary classification, it is convenient to think
in terms of true versus false. In an investment setting, true can be
related to a positive return, or a return being above that of a
benchmark - false being the opposite.

There are then 4 types of possible results for a prediction. Two when
the prediction is right (predict true with true realization or predict
false with false outcome) and two when the prediction is wrong (predict
true with false realization and the opposite). We define the
corresponding aggregate metrics below:

\begin{itemize}
\tightlist
\item
  frequency of true positive:
  \(TP=I^{-1}\sum_{i=1}^I1_{\{y_i=\tilde{y}_i=1 \}},\)\\
\item
  frequency of true negative:
  \(TN=I^{-1}\sum_{i=1}^I1_{\{y_i=\tilde{y}_i=0 \}},\)\\
\item
  frequency of false positive:
  \(FP=I^{-1}\sum_{i=1}^I1_{\{\tilde{y}_i=1,y_i=0 \}},\)\\
\item
  frequency of false negative:
  \(FN=I^{-1}\sum_{i=1}^I1_{\{\tilde{y}_i=0,y_i=1 \}},\)
\end{itemize}

where true is conventionally encoded into 1 and false into 0. The sum of
the four figures is equal to one. These four numbers have very different
impacts on out-of-sample results, as is shown in Figure
\ref{fig:valconfusion}. In this table (also called a confusion matrix),
it is assumed that some proxy for future profitability is forecast by
the model. Each row stands for the model's prediction and each column
for the realization of the profitability. The most important cases are
those in the top row, when the model predicts a positive result because
it is likely that assets with positive profitability (possibly relative
to some benchmark) will end up in the portfolio. Of course, this is not
a problem if the asset does well (left cell), but it becomes penalizing
if the model is wrong because the portfolio will suffer.

\begin{figure}[H]

{\centering \includegraphics[width=300px]{images/confusion} 

}

\caption{Confusion matrix: summary of binary outcomes.}\label{fig:valconfusion}
\end{figure}

Among the two types of errors, the type I is the most daunting for
investors because it has a direct effect on the portfolio. The type II
error is simply a missed opportunity and is somewhat less impactful.
Finally, true negatives are those assets which are correctly excluded
from the portfolio.

From the four baseline rates, it is possible to derive other interesting
metrics:

\begin{itemize}
\tightlist
\item
  Accuracy = \(TP+TN\) is the percentage of correct forecasts;\\
\item
  Recall = \(\frac{TP}{TP+FN}\) measures the ability to detect a winning
  strategy/asset (left column analysis). Also known as sensitivity or
  true positive rate (TPR);
\item
  Precision = \(\frac{TP}{TP+FP}\) computes the probability of good
  investments (top row analysis);
\item
  Specificity = \(\frac{TN}{FP+TN}\) measures the proportion of actual
  negatives that are correctly identified as such (right column
  analysis);
\item
  Fallout = \(\frac{FP}{FP+TN}=1-\)Specificity is the probability of
  false alarm (or false positive rate), i.e., the frequence at which the
  algorithm detects falsely performing assets;\\
\item
  F-score,
  \(\mathbf{F}_1=2\frac{\text{recall}\times \text{precision}}{\text{recall}+ \text{precision}}\)
  is the harmonic average of recall and precision.
\end{itemize}

All of these items lie in the unit interval and a model is deemed to
perform better when they increase (except for fallout for which it is
the opposite). Many other indicators also exist, like the false
discovery rate or false omission rate, but they are not as mainstream
and less cited. Moreover, they are often simple functions of the ones
mentioned above.

A metric that is popular but more complex is the area under the (ROC)
curve, often referred to as AUC. The complicated part is the ROC curve
where ROC stands for Receiver Operating Characteristic; the name comes
from signal theory. We explain how it is built below.

As seen in Chapters \ref{trees} and \ref{NN}, classifiers generate
output that are probabilities that one instance belongs to one class.
These probabilities are then translates into a class by choosing the
class that has the highest value. In binary classification, the class
with a score above 0.5 basically wins.

In practice, this 0.5 threshold may not be optimal and the model could
very well correctly predict false instances when the probability is
below 0.4 and true ones otherwise. Hence, it is a natural idea to test
what happens if the decision threshold changes. The ROC curve does just
that and plots the recall as a function of the fallout when the
threshold increases from zero to one.

When the threshold is equal to 0, true positives are equal to zero
because the model never forecasts positive values. Thus, both recall and
fallout are equal to zero. When the threshold is equal to one, false
negatives shrink to zero and true negatives too, hence recall and
fallout are equal to one. The behaviour of their relationship in between
these two extremes is called the ROC curve. We provide stylized examples
below. A random classifier would fare equally good for recall and
fallout and thus the ROC curve would be a linear line from the point
(0,0) to (1,1). To prove this, imagine a sample with a \(p\in (0,1)\)
proportion of true instances and a classifier that predicts true
randomly with a probability \(p'\in (0,1)\). Then because the sample and
predictions are independent, \(TP=p'p\), \(FP = p'(1-p)\),
\(TN=(1-p')(1-p)\) and \(FN=(1-p')p\). Given the above definition, this
yields that both recall and fallout are equal to \(p'\).

\begin{figure}[H]

{\centering \includegraphics[width=300px]{images/ROCcurve} 

}

\caption{Stylized ROC curves.}\label{fig:ROCcurve}
\end{figure}

An algorithm with a ROC curve above the 45° angle is performing better
than an average classifier. Indeed, the curve can be seen as a tradeoff
between benefits (probability of detecting good strategies on the \(y\)
axis) minus costs (odds of selecting the wrong assets on the \(x\)
axis). Hence being above the 45° is paramount. The best possible
classifier has a ROC curve that goes from point (0,0) to point (0,1) to
point (1,1). At point (0,1), fallout is null, hence there are no false
positives, and recall is equal to one so that there are also no false
negatives: the model is always right. The opposite is true: at point
(1,0), the model is always wrong.

Below, we use a particular package (\emph{caTools}) to compute a ROC
curve for a given set of predictions on the testing sample.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{require}\NormalTok{(caTools))\{}\KeywordTok{install.packages}\NormalTok{(}\StringTok{"caTools"}\NormalTok{)\}}
\end{Highlighting}
\end{Shaded}

\normalsize

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caTools)  }\CommentTok{# Package for AUC computation}
\KeywordTok{colAUC}\NormalTok{(}\DataTypeTok{X =} \KeywordTok{predict}\NormalTok{(fit_RF_C, testing_sample, }\DataTypeTok{type =} \StringTok{"prob"}\NormalTok{), }
       \DataTypeTok{y =}\NormalTok{ testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd_C, }
       \DataTypeTok{plotROC =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=300px]{ML_factor_files/figure-latex/roc-1} 

}

\caption{Example of ROC curve.}\label{fig:roc}
\end{figure}

\begin{verbatim}
##                    FALSE      TRUE
## FALSE vs. TRUE 0.5003885 0.5003885
\end{verbatim}

\normalsize

In the above figure, the curve is very close to the 45° angle and the
model seems as good (or, rather, as bad) as a random classifier.

Finally, having one entire curve is not practical for comparison
purposes, hence the information of the whole curve is synthesized into
the area below the curve, i.e., the integral of the corresponding
function. The 45° angle (quadrant bisector) has an area of 0.5 (it is
half the unit square which has a unit area). Thus, any good model is
expected to have an area under the curve (AUC) above 0.5. A perfect
model has an AUC of one.

We end this subsection with a word on multiclass data. When the output
(i.e., the label) has more than two categories, things become more
complex. It is still possible to compute a confusion matrix, but the
dimension is larger and harder to interpret. The simple indicators like
\(TP\), \(TN\), etc., must be generalized in a non standard way. The
simplest metric in the case is the cross-entropy defined in Equation
(\eqref{eq:crossentropy}).

\hypertarget{validation}{%
\section{Validation}\label{validation}}

Validation is the stage at which a model is tested and tuned before it
starts to be deployed on real or live data (e.g., for trading purposes).
Needless to say that it is critical.

\hypertarget{the-variance-bias-tradeoff-theory}{%
\subsection{The variance-bias tradeoff:
theory}\label{the-variance-bias-tradeoff-theory}}

The variance-bias tradeoff is one of the core concepts in ML. To explain
it, let us assume that the data is generated by the simple model
\[y_i=f(\textbf{x}_i)+\epsilon_i, \quad   \mathbb{E}[\boldsymbol{\epsilon}]=0, \quad \mathbb{V}[\boldsymbol{\epsilon}]=\sigma^2,\]

but the model that is estimated yields

\[y_i=\hat{f}(\textbf{x}_i)+\hat{\epsilon}_i. \]

Given an unkown sample \(\textbf{x}\), the decomposition of the average
squared error is

\begin{align} \label{eq:biasvariance}
\mathbb{E}[\hat{\epsilon}^2]&=\mathbb{E}[(y-\hat{f}(\textbf{x}))^2]=\mathbb{E}[(f(\textbf{x})+\epsilon-\hat{f}(\textbf{x}))^2]   \\
&= \underbrace{\mathbb{E}[(f(\textbf{x})-\hat{f}(\textbf{x}))^2]}_{\text{total quadratic error}}+\underbrace{\mathbb{E}[\epsilon^2]}_{\text{irreducible error}} \nonumber \\
&= \mathbb{E}[\hat{f}(\textbf{x})^2]+\mathbb{E}[f(\textbf{x})^2]-2\mathbb{E}[f(\textbf{x})\hat{f}(\textbf{x})]+\sigma^2\nonumber\\
&=\mathbb{E}[\hat{f}(\textbf{x})^2]+f(\textbf{x})^2-2f(\textbf{x})\mathbb{E}[\hat{f}(\textbf{x})]+\sigma^2\nonumber\\
&=\left[ \mathbb{E}[\hat{f}(\textbf{x})^2]-\mathbb{E}[\hat{f}(\textbf{x})]^2\right]+\left[\mathbb{E}[\hat{f}(\textbf{x})]^2+f(\textbf{x})^2-2f(\textbf{x})\mathbb{E}[\hat{f}(\textbf{x})]\right]+\sigma^2\nonumber\\
&=\underbrace{\mathbb{V}[\hat{f}(\textbf{x})]}_{\text{variance of model}}+ \quad \underbrace{\mathbb{E}[(f(\textbf{x})-\hat{f}(\textbf{x}))]^2}_{\text{squared bias}}\quad +\quad\sigma^2 \nonumber
\end{align}

In the above derivation, \(f(x)\) is not random, but \(\hat{f}(x)\) is.
Also, in the second line, we assumed
\(\mathbb{E}[\epsilon(f(x)-\hat{f}(x))]=0\), which may not always hold
(though it is a very common assumption). The average squared error thus
has three components:

\begin{itemize}
\tightlist
\item
  one irreducible error (independent from the choice of a particular
  model);\\
\item
  the variance of of the model (over its predictions);
\item
  and the squared bias of the model.
\end{itemize}

The first one is immune to changes in models, so the challenge is to
minimize the sum of the other two. This is known as the variance-bias
tradeoff because reducing one often leads to increasing the other. The
goal is thus to assess when a small increase in either one can lead to a
larger decrease in the other.

There are several ways to represent this tradeoff and we display two of
them. The first one relates to archery (see Figure \ref{fig:archery})
below. The best case (top left) is when all shots are concentrated in
the middle: on average, the archer aims correctly and all the arrows are
very close to one another. The worst case (bottom right) is the exact
opposite: the average arrow is above the center of the target (the bias
is nonzero) and the dispersion of arrows is large.

\begin{figure}[H]

{\centering \includegraphics[width=300px]{images/var_bias_trade} 

}

\caption{First representation of the variance-bias tradeoff.}\label{fig:archery}
\end{figure}

The most often encountered cases in ML are the other two configurations:
either the arrows (predictions) are concentrated in a small perimeter,
but the perimeter is not the center of the target; or the arrows are one
average well distributed around the center, but they are far from it.

The second way the variance bias tradeoff is often depicted is via the
notion of model complexity. The most simple model of all is a constant
one: the prediction is always the same, for instance equal to the
average value of the label in the training set. Of course, this
prediction will often be far from the realized values of the testing set
(its bias will be large), but at least its variance is zero. On the
other side of the spectrum, a decision tree with as many leaves as there
are instances has a very complex structure. It will probably have a
smaller bias, but undoubtedly it is not obvious that this will
compensate the increase in variance incurred by the intricacy of the
model.

This facet of the tradeoff is depicted in Figure \ref{fig:varbiastrade}
below. To the left of the graph, a simple model has a small variance but
a large bias while to the right it is the opposite for a complex model.
Good models often lie somewhere in the middle, but the perfect mix is
hard to find.

\begin{figure}[H]

{\centering \includegraphics[width=300px]{images/var_bias_trade2} 

}

\caption{Second representation of the variance-bias tradeoff.}\label{fig:varbiastrade}
\end{figure}

The most tractable theoretical form of the variance-bias tradeoff is the
ridge regression.\footnote{Another angle, critical of neural networks is
  provided in \citet{geman1992neural}.} The coefficient estimates in
this type of regression are given by
\(\hat{\mathbf{b}}_\lambda=(\mathbf{X}'\mathbf{X}+\lambda \mathbf{I}_N)^{-1}\mathbf{X}'\mathbf{Y}\),
where \(\lambda\) is the penalization intensity. Assuming a \emph{true}
linear form for the data generating process
(\(\textbf{y}=\textbf{Xb}+\boldsymbol{\epsilon}\) where \(\textbf{b}\)
is unknown and \(\sigma^2\) the variance of errors), this yields
\begin{align}   \label{eq:biastrade}
\mathbb{E}[\hat{\textbf{b}}_\lambda]&=\textbf{b}-\lambda(\textbf{X}'\textbf{X}+\lambda \textbf{I}_N)^{-1} \textbf{b}, \\  \label{eq:vartrade}
\mathbb{V}[\hat{\textbf{b}}_\lambda]&=\sigma^2(\textbf{X}'\textbf{X}+\lambda \textbf{I}_N)^{-1}\textbf{X}'\textbf{X}   (\textbf{X}'\textbf{X}+\lambda \textbf{I}_N)^{-1}.
\end{align}

Basically, this means that the bias of the estimator is equal to
\(-\lambda(\textbf{X}'\textbf{X}+\lambda \textbf{I}_N)^{-1} \textbf{b}\),
which is zero in the absence of penalization (classical regression) and
converges to some finite number when \(\lambda \rightarrow \infty\),
i.e., when the model becomes constant. Note that if the estimator has a
zero bias, then predictions will too:
\(\mathbb{E}[\textbf{X}(\textbf{b}-\hat{\textbf{b}})]=\textbf{0}\).

The variance (of estimates) in the case of an unconstrained regression
is equal to
\(\mathbb{V}[\hat{\textbf{b}}]=\sigma (\textbf{X}'\textbf{X})^{-1}\). In
Equation (\eqref{eq:vartrade}), the \(\lambda\) reduces the magnitude of
figures in the inverse matrix. The overall effect is that as \(\lambda\)
increases, the variance decreases and in the limit
\(\lambda \rightarrow \infty\), the variance is zero when the model is
constant. The variance of predictions is \begin{align*}
\mathbb{V}[\textbf{X}\hat{\textbf{b}}]&=\mathbb{E}[(\textbf{X}\hat{\textbf{b}}-\mathbb{E}[\textbf{X}\hat{\textbf{b}}])(\textbf{X}\hat{\textbf{b}}-\mathbb{E}[\textbf{X}\hat{\textbf{b}}])'] \\
&= \textbf{X}\mathbb{E}[(\hat{\textbf{b}}-\mathbb{E}[\hat{\textbf{b}}])(\hat{\textbf{b}}-\mathbb{E}[\hat{\textbf{b}}])']\textbf{X}' \\
&= \textbf{X}\mathbb{V}[\hat{\textbf{b}}]\textbf{X}
\end{align*}

All in all, ridge regressions are very handy because with a single
parameter, they are able to provide a cursor that directly tunes the
variance-bias tradeoff.

It's easy to illustrate how easy it is to display the tradeoff with the
ridge regression. In the example below we recycle the ridge model
trained in Chapter \ref{lasso}.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridge_errors <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit_ridge, x_penalized_test) }\OperatorTok{-}\StringTok{                       }\CommentTok{# Errors from all models}
\StringTok{    }\NormalTok{(}\KeywordTok{rep}\NormalTok{(testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd, }\DecValTok{100}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{matrix}\NormalTok{(}\DataTypeTok{ncol =} \DecValTok{100}\NormalTok{, }\DataTypeTok{byrow =} \OtherTok{FALSE}\NormalTok{))}
\NormalTok{ridge_bias <-}\StringTok{ }\NormalTok{ridge_errors }\OperatorTok{%>%}\StringTok{ }\KeywordTok{apply}\NormalTok{(}\DecValTok{2}\NormalTok{, mean)                                }\CommentTok{# Biases}
\NormalTok{ridge_var <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit_ridge, x_penalized_test) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{apply}\NormalTok{(}\DecValTok{2}\NormalTok{, var)          }\CommentTok{# Variance}
\KeywordTok{tibble}\NormalTok{(lambda, ridge_bias}\OperatorTok{^}\DecValTok{2}\NormalTok{, ridge_var, }\DataTypeTok{total =}\NormalTok{ ridge_bias}\OperatorTok{^}\DecValTok{2}\OperatorTok{+}\NormalTok{ridge_var) }\OperatorTok{%>%}\StringTok{  }\CommentTok{# Plot}
\StringTok{    }\KeywordTok{gather}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ Error_Component, }\DataTypeTok{value =}\NormalTok{ Value, }\OperatorTok{-}\NormalTok{lambda) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ lambda, }\DataTypeTok{y =}\NormalTok{ Value, }\DataTypeTok{color =}\NormalTok{ Error_Component)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=300px]{ML_factor_files/figure-latex/ridgetrade-1} 

}

\caption{Error decomposition for a ridge regression}\label{fig:ridgetrade}
\end{figure}

\normalsize

In Figure \ref{fig:ridgetrade}, the pattern is different from the one
depicted in Figure \ref{fig:varbiastrade}. In the graph, when the
intensity lambda increases, the magnitude of parameters shrinks and the
model becomes simpler. Hence, the most simple model seems like the best
choice: adding complexity increases variance but does not improve the
bias! One possible reason for that is that features don't actually carry
much predictive value and hence a constant model is just as good as more
sophisticated ones based on unrelevant variables.

\hypertarget{the-variance-bias-tradeoff-illustration}{%
\subsection{The variance-bias tradeoff:
illustration}\label{the-variance-bias-tradeoff-illustration}}

The variance-bias tradeoff is often presented in theoretical terms that
are easy to grasp. It is nonetheless useful to demonstrate how it
operates on true algorithmic choices. Below, we take the example of
trees because their complexity is easy to evaluate. Basically, a tree
with many terminal nodes is more complex than a tree with a handful of
clusters.

We start with the parcimonious model, which we train below.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit_tree_simple <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(formula, }
             \DataTypeTok{data =}\NormalTok{ training_sample,     }\CommentTok{# Data source: training sample}
             \DataTypeTok{cp =} \FloatTok{0.0001}\NormalTok{,                }\CommentTok{# Precision: smaller = more leaves}
             \DataTypeTok{maxdepth =} \DecValTok{2}                \CommentTok{# Maximum depth (i.e. tree levels)}
\NormalTok{             ) }
\KeywordTok{rpart.plot}\NormalTok{(fit_tree_simple)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=200px]{ML_factor_files/figure-latex/treesimple-1} 

}

\caption{Simple tree.}\label{fig:treesimple}
\end{figure}

\normalsize

The model only has 4 clusters, which means that the predictions can only
take four values. The smallest one is 0.011 and encompasses a large
portion of the sample (85\%) and the largest one is 0.062 and
corresponds to only 4\% of the training sample.\\
We are then able to compute the bias and the variance of the predictions
on the \emph{testing} set.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{predict}\NormalTok{(fit_tree_simple, testing_sample) }\OperatorTok{-}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd) }\CommentTok{# Bias}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.004973917
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{var}\NormalTok{(}\KeywordTok{predict}\NormalTok{(fit_tree_simple, testing_sample))                           }\CommentTok{# Variance}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0001398003
\end{verbatim}

\normalsize

On average, the error is slightly positive, with an overall
overestimation of 0.005 . As expected, the variance is very small
(10\^{}\{-4\}).

For the complex model, we take the boosted tree that was obtained in
Section \ref{boostcode} (fit\_xgb). The model aggregates 40 trees with a
maximum depth of 4, it is thus undoubtedly more complex.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(}\KeywordTok{predict}\NormalTok{(fit_xgb, xgb_test) }\OperatorTok{-}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd) }\CommentTok{# Bias}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.00324996
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{var}\NormalTok{(}\KeywordTok{predict}\NormalTok{(fit_xgb, xgb_test))                           }\CommentTok{# Variance}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.002206797
\end{verbatim}

\normalsize

The bias is indeed smaller compared to that of the simple model, but in
exchange, the variance increases sustantially. The net effect (via the
\emph{squared bias}) is in favor of the simpler model.

\hypertarget{the-risk-of-overfitting-principle}{%
\subsection{The risk of overfitting:
principle}\label{the-risk-of-overfitting-principle}}

The notion of overfitting is one of the most important in machine
learning. When a model overfits, the accuracy of its predictions will be
disappointing, thus it is one major reason why \emph{some} strategies
fail out-of-sample. Therefore, it is important to understand not only
what overfitting is, but also how to mitigate its effects.

One recent reference on this topic and its impact on portfolio
strategies is \citet{hsu2018asset}, which builds on the work of
\citet{white2000reality}. Both these reference do not deal with ML
models, but the principle is the same. When given a dataset, a
sufficiently intense level of analysis (by a human or a machine) will
always be able to detect some patterns. Whether these patterns are
spurious are not is the key question.

In Figure \ref{fig:overfit}, we illustrate this idea with a simple
visual example. We try to find a model that maps x into y. The data
points are the small black circles. The simplest model is the constant
one (only one parameter), but with two parameters (level and slope), the
fit is already quite good. This is shown with the blue line. With a
sufficient number of parameters, it is possible to build a model that
flows through all the points. One example would be a high dimensional
polynomial. One such models is represented with the red line. Now there
seems to be a strange point in the dataset and the complex model fits
closely to match this point.

\begin{figure}[H]

{\centering \includegraphics[width=300px]{images/overfitting} 

}

\caption{Confusion matrix: summary of binary outcomes.}\label{fig:overfit}
\end{figure}

A new point is added in light green. It is fair to say that is follows
the general pattern of the other points. The simple model is not perfect
and the error is non-negligible. Nevertheless, the error stemming from
the complex model (shown with the dotted gray line) is approximately
twice as large. This simplified example shows that models that are too
close to the training data will catch idiosyncracies that will not occur
in other datasets. A good model would overlook these idiosyncracies and
stick to the enduring structure of the data.

\hypertarget{the-risk-of-overfitting-some-solutions}{%
\subsection{The risk of overfitting: some
solutions}\label{the-risk-of-overfitting-some-solutions}}

Obviously, the easiest way to avoid overfitting is to resist to the
temptation of complicated models (e.g., high dimensional neural networks
or tree ensembles).

The complexity of models is often proxied via two measures: the number
of parameters of the model and their magnitude (often synthesized
through their norm). These proxies are not perfect because some
\emph{complex} models may only require a small number of parameters (or
even small parameter values), but at least they are straightforward and
easy to handle. There is no universal way of handling overfitting.
Below, we detail a few tricks for some families of ML tools.

For \textbf{regressions}, there are two simple ways to deal with
overfitting. The first is the number of parameters, that is, the number
of predictors. Sometimes, it can be better to only select a subsample of
features, especially if some of them are highly correlated (often, a
threshold of 70\% is considered as too high for absolute correlations
between features). The second solution is penalization, which helps
reduce the magnitude of estimates and thus of the variance of
predictions.

For tree-based methods, there are a variety of ways to reduce the risk
of overfitting. When dealing with \textbf{simple trees}, the only way to
proceed is to limit the number of leaves. This can be done in many ways.
First, by imposing a maximum depth. If it is equal to \(d\), then the
tree can have at most \(2^d\) terminal nodes. It is often advised not to
go beyond \(d=6\). The complexity parameter in \emph{rpart} is another
way the shrink the size of trees. When it is large, any new split must
lead to a substantial reduction in loss. If not, the split is not deemed
useful and is thus not performed. The last two parameters are the
minimum number of instances required in each leaf and the minimum number
of instance per cluster requested in order to continue the splitting
process. The higher (i.e., the more coercive) these figures are, the
harder it is to grow complex trees.

In addition to these options, \textbf{random forests} allow to control
for the number of trees in the forest. Theoretically (see
\citet{breiman2001random}), this parameter is not supposed to impact the
variance-bias tradeoff. In practice, and for the sake of computation
times, it is not recommended to go beyong 1,000 trees. Two other
hyperparameters are the subsample size (on which each learner is
trained) and the number of features retained for learning. They do not
have a straightforward impact of bias and tradeoff, but rather on raw
performace. For instance, if subsamples are too small, the trees will
not learn enough. Same problem if the number of features is too low. On
the other hand, choosing a large number of predictors (i.e., close to
the total number) may lead to high correlations between each learner's
prediction because the overlap in information contained in the training
samples may be high.

\textbf{Boosted trees} have other options that can help alleviate the
risk of overfitting. The most obvious one is the learning rate, which
discounts the impact of each new tree by a large factor. When the
learning rate is high, the algorithm learns too fast and is prone to
sticking close to the training data. When it's low, the model learns
very progressively, which can be efficient if there are sufficiently
many trees in the ensemble. Indeed, the learning rate and the number of
trees must be chosen synchronously: if both are low, the ensemble will
learn nothing and if both are large, it will overfit. The arsenal of
boosted trees does not stop there. The penalizations, both of score
values and of the number of leaves, are naturally a tool to prevent the
model from going to deep in the particularities of the trainig sample.
Finally, constrainsts of monotonicity like those mentioned in Section
\ref{boostext} are also an efficient way to impose some structure on the
model and force it to detect particular patterns.

Lastly \textbf{neural networks} also have many options aimed at
protecting them against overfitting. Just like for boosted trees, one of
them is the penalization of weights and biases (via their norm).
Constraints, like nonnegative constraints can also help when the
modeltheoretically requires positive inputs. Finally, dropout is always
a direct way to reduce the dimension (number of parameters) of a
network.

\hypertarget{the-search-for-good-hyperparameters}{%
\section{The search for good
hyperparameters}\label{the-search-for-good-hyperparameters}}

\hypertarget{methods}{%
\subsection{Methods}\label{methods}}

Let us assume that there are \(p\) parameters to be defined before a
model is run. The simplest way to proceed is to test different values of
these parameters and choose the one that yields the best results. There
are mainly two ways to perform these tests: independently and
sequentially.

Independent tests are easy and come in two families: grid
(deterministic) search and random exploration. The advantage of a
deterministic approach is that is covers the space uniformly and makes
sure that no corners are omitted. The drawback is the computation time.
Indeed, for each parameter, it seems reasonable to test at least five
values, which makes \(5^p\) combinations. If \(p\) is small (smaller
than 3), this is manageable when the backtests are not too lengthy. When
\(p\) is large, the number of combinations may become prohibitive. This
is when random exploration can be useful because in this case, the user
speciifies the number of tests upfront and the parameters are drawn
randomly (usually uniformly). The flaw in random search is that some
areas in the parameter space may not be covered, which can be
problematic if the best choice is located there. It is nonetheless shown
in \citet{bergstra2012random} that random exploration is preferable to
grid search.

Both grid and random searches are suboptimal because they are likely to
spend time in zones of the parameter space that are irrelevant, thereby
wasting computation time. Given a number of parameter points that have
been tested, it is preferable to focus the search in areas where the
best points are the most likely. This is possible via an interative
process that adapts the search after each new point has been tested.

One other popular approach in this direction is Bayesian optimization
(BO). The central object is the objective function of the learning
process. We call this function \(O\) and it can be widely seen as a loss
function possibly combined with penalization and constraints. For
simplicity here, we will not mention the training/testing samples and
they are considered to be fixed. The variable of interest is the vector
\(\textbf{p}=(p_1,\dots,p_l)\) which synthesizes the hyperparameters
(learning rate, penalization intensities, number of models, etc.) that
have an impact on \(O\). The program we are interested in is

\begin{equation}
\label{eq:HPO}
\textbf{p}_*=\underset{\textbf{p}}{\text{argmin}} \ O(\textbf{p}).
\end{equation}

The main problem with this optimization is that the computation of
\(O(\textbf{p})\) is very costly. Therefore, it is critical to choose
each trial for \(\textbf{b}\) wisely. One key assumption of BO is that
the distribution of \(O\) is Gaussian and that \(O\) can be proxied by a
linear combination of the \(p_l\). Said differently, the aim is to build
a Bayesian linear regression between the input \(\textbf{p}\) and the
output (dependent variable) \(O\). Once a model has been estimated the
information that is concentrated in the posterior density of \(O\) is
used to make an educated guess at where to look at for new values of
\(\textbf{p}\).

This educated guess is made based on a so-called \emph{acquisition
function}. Suppose we have tested \(m\) values for \(\textbf{p}\), which
we write \(\textbf{p}^{(m)}\). The current best parameter is written
\(\textbf{p}_m^*=\underset{1\le k\le m}{\text{argmin}} \ O(\textbf{p}^{(k)})\).
If we test a new point \(\textbf{p}\), then it will lead to an
improvement only if \(O(\textbf{p})<O(\textbf{p}_m^*)\), that is if the
new objective improves the minimum value that we already know. The
average value of this improvement is \begin{equation}
\label{eq:acquisition}
\textbf{EI}_m(\textbf{p})=\mathbb{E}_m[[O(\textbf{p}_m^*)-O(\textbf{p})]_+],
\end{equation}

where the positive part \([\cdot]_+\) emphasizes that when
\(O(\textbf{p})\ge O(\textbf{p}_m^*)\), the gain is zero. The
expectation is indexed by \(m\) because it is computed with respect to
the posterior distribution of \(O(\textbf{p})\) based on the \(m\)
samples \(\textbf{p}^{(m)}\). The best choice for the next sample
\(\textbf{p}^{m+1}\) is then \begin{equation}
\label{eq:EI}
\textbf{p}^{m+1}=\underset{\textbf{p}}{\text{argmax}} \ \textbf{EI}_m(\textbf{p}),
\end{equation} which corresponds to the maximum location of the expected
improvement. Instead of the EI, the optimization can be performed on
other measures, like the probability of improvement, which is
\(\mathbb{P}_m[O(\textbf{p})<O(\textbf{p}_m^*)]\).

In compact form, the iterative process can be outlined as follows:

\begin{itemize}
\tightlist
\item
  \textbf{step 1}: compute \(O(\textbf{p}^{(m)})\) for \(m=1,\dots,M_0\)
  values of parameters.\\
\item
  \textbf{step 2a}: compute sequentially the posterior density of \(O\)
  on all available points.\\
\item
  \textbf{step 2b}: compute the optimal new point to test
  \(\textbf{p}^{m+1}\) given in Equation \eqref{eq:EI}.\\
\item
  \textbf{step 2c}: compute the new objective value
  \(O(\textbf{p}^{m+1})\).\\
\item
  \textbf{step 3}: repeat steps 2a to 2c as much as deemed reasonable
  and return the \(\textbf{p}^{m}\) that yields the smallest objective
  value.
\end{itemize}

The interested reader can have a look at \citet{snoek2012practical} and
\citet{frazier2018tutorial} for more details on the numerical facets of
this method.

Finally, for the sake of completeness, we mention a last way to tune
hyperparameters. Since the optimization scheme is
\(\underset{\textbf{p}}{\text{argmin}} \ O(\textbf{p})\), a natural way
to proceed would be to use the sensitivity of \(O\) with respect to
\(\textbf{p}\). Indeed, if the gradient
\(\frac{\partial O}{\partial p_l}\) is known, then a gradient descent
will always improve the objective value. The problem is that it is hard
to compute a reliable gradient (finite differences can become costly).
Nonetheless, some methods (e.g., \citet{maclaurin2015gradient}) have
been applied successfully to optimize over large dimensional parameter
spaces.

\hypertarget{example-grid-search}{%
\subsection{Example: grid search}\label{example-grid-search}}

In order to illustrate the process of grid search, we will try to find
the best parameters for a boosted tree. We seek to quantify the impact
of three parameters:

\begin{itemize}
\tightlist
\item
  \textbf{eta}, the learning rate,\\
\item
  \textbf{nrounds}, the number of trees that are grown,\\
\item
  \textbf{lambda}, the weight regulariser which penalises the objective
  function through the total sum of squared weights/scores.
\end{itemize}

Below, we create a grid with the values we want to test for these
parameters.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eta <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.9}\NormalTok{)         }\CommentTok{# Values for eta}
\NormalTok{nrounds <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{)                 }\CommentTok{# Values for nrounds}
\NormalTok{lambda <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{)        }\CommentTok{# Values for lambda}
\NormalTok{pars <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(eta, nrounds, lambda) }\CommentTok{# Exploring all combinations!}
\NormalTok{eta <-}\StringTok{ }\NormalTok{pars[,}\DecValTok{1}\NormalTok{]}
\NormalTok{nrounds <-}\StringTok{ }\NormalTok{pars[,}\DecValTok{2}\NormalTok{]}
\NormalTok{lambda <-}\StringTok{ }\NormalTok{pars[,}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\normalsize

Given the computational cost of grid search, we perform the exploration
on the dataset with the small number of features (which we recycle from
Chapter \ref{trees}). In order to avoid the burden of loops, we resort
to the functional programming capabilities of R, via the \emph{purrr}
package. This allows us to define a function that will lighten and
simplify the code. This function, coded below, takes data and parameter
inputs and returns an error metric for the algorithm. We choose the mean
squared error to evaluate the impact of hyperparameter values.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grid_par <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(train_matrix, test_features, test_label, eta, nrounds, lambda)\{}
\NormalTok{    fit <-}\StringTok{ }\NormalTok{train_matrix }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{xgb.train}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ .,                       }\CommentTok{# Data source (pipe input)}
                  \DataTypeTok{eta =}\NormalTok{ eta,                      }\CommentTok{# Learning rate}
                  \DataTypeTok{objective =} \StringTok{"reg:linear"}\NormalTok{,       }\CommentTok{# Objective function}
                  \DataTypeTok{max_depth =} \DecValTok{5}\NormalTok{,                  }\CommentTok{# Maximum depth of trees}
                  \DataTypeTok{lambda =}\NormalTok{ lambda,                }\CommentTok{# Penalisation of leaf values}
                  \DataTypeTok{gamma =} \FloatTok{0.1}\NormalTok{,                    }\CommentTok{# Penalisation of number of leaves}
                  \DataTypeTok{nrounds =}\NormalTok{ nrounds,              }\CommentTok{# Number of trees used}
                  \DataTypeTok{verbose =} \DecValTok{0}                     \CommentTok{# No comment from algo}
\NormalTok{        )}
    
\NormalTok{    pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit, test_features)           }\CommentTok{# Preditions based on fitted model & test values}
    \KeywordTok{return}\NormalTok{(}\KeywordTok{mean}\NormalTok{((pred}\OperatorTok{-}\NormalTok{test_label)}\OperatorTok{^}\DecValTok{2}\NormalTok{))             }\CommentTok{# Mean squared error}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\normalsize

The grid\_par function can then be processed by the functional
programming tool \textbf{pmap} that is going to perform the loop on
parameter values automatically.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# grid_par(train_matrix_xgb, xgb_test, testing_sample$R1M_Usd, 0.1, 3, 0.1) # Test of the function}
\NormalTok{grd <-}\StringTok{ }\KeywordTok{pmap}\NormalTok{(}\KeywordTok{list}\NormalTok{(eta, nrounds, lambda),             }\CommentTok{# Parameters for the grid search}
\NormalTok{            grid_par,                               }\CommentTok{# Function on which to apply the grid search}
            \DataTypeTok{train_matrix =}\NormalTok{ train_matrix_xgb,        }\CommentTok{# Input for function: training data}
            \DataTypeTok{test_features =}\NormalTok{ xgb_test,               }\CommentTok{# Input for function: test features}
            \DataTypeTok{test_label =}\NormalTok{ testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd     }\CommentTok{# Input for function: test labels (returns) }
\NormalTok{)}
\NormalTok{grd <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(eta, nrounds, lambda, }\DataTypeTok{error =} \KeywordTok{unlist}\NormalTok{(grd)) }\CommentTok{# Dataframe with all results}
\end{Highlighting}
\end{Shaded}

\normalsize

Once the squared mean errors have been gathered, it is possible to plot
them. We chose to work with 3 parameters on purpose because their
influence can be simultaneuously plotted on one graph.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grd}\OperatorTok{$}\NormalTok{eta <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(eta)                                  }\CommentTok{# Parameters as categories (for plotting)}
\NormalTok{grd }\OperatorTok{%>%}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ eta, }\DataTypeTok{y =}\NormalTok{ error, }\DataTypeTok{fill =}\NormalTok{ eta)) }\OperatorTok{+}\StringTok{      }\CommentTok{# Plot!}
\StringTok{    }\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{stat =} \StringTok{"identity"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{facet_grid}\NormalTok{(}\DataTypeTok{rows =} \KeywordTok{vars}\NormalTok{(nrounds), }\DataTypeTok{cols =} \KeywordTok{vars}\NormalTok{(lambda))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=400px]{ML_factor_files/figure-latex/gridvisu-1} 

}

\caption{Plot of error metrics (SMEs) for many parameter values. Each row of graph corresponds to nrounds and each column to lambda.}\label{fig:gridvisu}
\end{figure}

\normalsize

In Figure \ref{fig:gridvisu}, the main information is that a small
learning rate (\(\eta=0.1\)) is detrimental to the quality of the
forecasts. This remains true even when the number of trees is large
(nrounds=100), which means that the algorithm does not learn enough.

Grid search can be performed in two stages: the first stage helps locate
the zones that are of interest (with the lowest loss/objective values)
and then zoom in on these zones with refined values for the parameter on
the grid. With the results above, this would means shrinking the support
of eta to \([0.3,0.7]\) and also probably avoid the lowest values for
lambda.

\hypertarget{example-bayesian-optimization}{%
\subsection{Example: Bayesian
optimization}\label{example-bayesian-optimization}}

There are several packages in R that relate to Bayesian optimization. We
work with \emph{rBayesianOptimization}, which is general purpose but
also needs more coding involvment.

Just as for the grid search, we need to code the objective function on
which the hyperparameters will be optimized. Under
\emph{rBayesianOptimization}, the output has to have a particular form,
with a score and a prediction variable. The function will
\emph{maximize} the score, hence we will define it as minus the mean
squared error.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bayes_par_opt <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(}\DataTypeTok{train_matrix =}\NormalTok{ train_matrix_xgb,        }\CommentTok{# Input for function: training data}
            \DataTypeTok{test_features =}\NormalTok{ xgb_test,                             }\CommentTok{# Input for function: test features}
            \DataTypeTok{test_label =}\NormalTok{ testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd,                  }\CommentTok{# Input for function: test label}
\NormalTok{            eta, nrounds, lambda)\{                                }\CommentTok{# Input for function: parameters}
\NormalTok{    fit <-}\StringTok{ }\NormalTok{train_matrix }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{xgb.train}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ .,                       }\CommentTok{# Data source (pipe input)}
                  \DataTypeTok{eta =}\NormalTok{ eta,                      }\CommentTok{# Learning rate}
                  \DataTypeTok{objective =} \StringTok{"reg:linear"}\NormalTok{,       }\CommentTok{# Objective function}
                  \DataTypeTok{max_depth =} \DecValTok{5}\NormalTok{,                  }\CommentTok{# Maximum depth of trees}
                  \DataTypeTok{lambda =}\NormalTok{ lambda,                }\CommentTok{# Penalisation of leaf values}
                  \DataTypeTok{gamma =} \FloatTok{0.1}\NormalTok{,                    }\CommentTok{# Penalisation of number of leaves}
                  \DataTypeTok{nrounds =} \KeywordTok{round}\NormalTok{(nrounds),       }\CommentTok{# Number of trees used}
                  \DataTypeTok{verbose =} \DecValTok{0}                     \CommentTok{# No comment from algo}
\NormalTok{        )}

\NormalTok{    pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit, test_features)           }\CommentTok{# Preditions based on fitted model & test values}
    \KeywordTok{list}\NormalTok{(}\DataTypeTok{Score =} \OperatorTok{-}\KeywordTok{mean}\NormalTok{((pred}\OperatorTok{-}\NormalTok{test_label)}\OperatorTok{^}\DecValTok{2}\NormalTok{),      }\CommentTok{# Minus RMSE}
         \DataTypeTok{Pred =}\NormalTok{ pred)                             }\CommentTok{# Predictions on test set}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\normalsize

Once the objective function is defined, it can be fed to the Bayesian
optimizer.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rBayesianOptimization)}
\NormalTok{bayes_opt <-}\StringTok{ }\KeywordTok{BayesianOptimization}\NormalTok{(bayes_par_opt,           }\CommentTok{# Function to maximize}
                     \DataTypeTok{bounds =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{eta =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.8}\NormalTok{),      }\CommentTok{# Bounds for eta}
                                   \DataTypeTok{lambda =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\DecValTok{15}\NormalTok{),    }\CommentTok{# Bounds for lambda}
                                   \DataTypeTok{nrounds =} \KeywordTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{100}\NormalTok{)),  }\CommentTok{# Bounds for nrounds}
                     \DataTypeTok{init_points =} \DecValTok{10}\NormalTok{,            }\CommentTok{# Nb initial points for first estimation}
                     \DataTypeTok{n_iter =} \DecValTok{24}\NormalTok{,                 }\CommentTok{# Nb optimization steps/trials}
                     \DataTypeTok{acq =} \StringTok{"ei"}\NormalTok{,                  }\CommentTok{# Acquisition function = expected improvement}
                     \DataTypeTok{verbose =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Best Parameters Found: 
## Round = 26   eta = 0.3954    lambda = 11.8355    nrounds = 12.0519   Value = -0.0372
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bayes_opt}\OperatorTok{$}\NormalTok{Best_Par}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        eta     lambda    nrounds 
##  0.3953682 11.8354694 12.0519044
\end{verbatim}

\normalsize

The final parameters indicate that it is advised to resist overfitting:
small number of learners and large penalization seem to be the best
choices.

As a confirmation of these results, we plot the relationship between the
loss (up to the sign) and two hyperparameters. Each point corresponds to
a value tested in the optimization. The best values are clearly to the
left of the left graph and to the right of the right graph and the
pattern is reliably pronounced.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"ggpubr"}\NormalTok{) }\CommentTok{# Package for combining plots}
\NormalTok{plot_rounds <-}\StringTok{ }\NormalTok{bayes_opt}\OperatorTok{$}\NormalTok{History }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ nrounds, }\DataTypeTok{y =}\NormalTok{ Value)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{)}
\NormalTok{plot_lambda <-}\StringTok{ }\NormalTok{bayes_opt}\OperatorTok{$}\NormalTok{History }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ lambda, }\DataTypeTok{y =}\NormalTok{ Value)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mar =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\KeywordTok{ggarrange}\NormalTok{(plot_rounds, plot_lambda, }\DataTypeTok{ncol =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=350px]{ML_factor_files/figure-latex/bayesoptfig-1} 

}

\caption{Relationship between (minus) loss and hyperparameter values.}\label{fig:bayesoptfig}
\end{figure}

\normalsize

\hypertarget{short-discussion-on-validation-in-backtests}{%
\section{Short discussion on validation in
backtests}\label{short-discussion-on-validation-in-backtests}}

The topic of validation in backtest is more complex than it can seem.
There are in fact two scales at which it can operate, depending on
whether the forecasting model is dynamic (updated at each rebalancing)
or fixed.

Let us start with the first option. In this case, the aim is to build a
unique model and to test it on different time periods. There is an
ongoing debate on the methods that are suitable to validate a model in
that case. Usually, it make sense to test the model on successive dates,
moving forward posterior to the training. This is what makes more sense,
as it replicates what would happen in a live situation.

In machine learning, a popular approach is to split the data into \(K\)
partitions and to test \(K\) different models: each one is tested one on
of the partitions but trained on the \(K-1\) others. This so-called
cross-validation (CV) is proscribed by most experts (and common sense)
for a simple reason: most of the time, the training set encompasses data
from future dates and tests on past values. Nonetheless, some advocate
one particular form of CV that aims at making sure that there is no
informational overlap between the training and testing set (Sections 7.4
and 12.4 in \citet{de2018advances}). The premise is that if the
structure of the cross-section of returns is constant through time, then
training on future points and testing on past data is not problematic as
long as there is no overlap.

One example cited in \citet{de2018advances} is the reaction to a model
to a unseen crisis. Following the market crash of 2008, at least 11
years have followed without any major financial shake. One option to
test the reaction of a recent model to a crash would be to train it on
recent years (say 2015-2019) and test it on various points (e.g.,
months) in 2008.

The advantage of a fixed model is that validation is easy: for one set
of hyperparameters, test the model on a set of dates, and evaluate the
performance of the model. Repeat the process for other parameters and
choose the best alternative (or use Bayesian optimization).

The second major option is when the model is updated (retrained) at each
rebalancing. The underlying idea here is that the structure of returns
evolves through time and a dynamic model will capture the most recent
trends. The drawback is that validation can then take place at each
rebalancing date.

Let us recall the dimensions of backtests:\\
- number of \textbf{strategies}: possibly dozens or hundreds, or even
more;\\
- number of trading \textbf{dates}: hundreds for monthly rebalancing;\\
- number of \textbf{assets}: hundreds or thousands;\\
- number of \textbf{features}: dozens or hundreds.

Even with a lot of computational power (GPUs, etc.), training many
models over many dates is time-consuming, especially when it comes to
hyper-parameter tuning when the parameter space is large. Thus,
validating models at each trading date of the out-of-sample period is
not realistic.

One solution is to keep an early portion of the training data and to
perform a smaller scale validation on this subsample. Hyperparameters
are tested on a limited number of dates and most of the time, they
exhibit stability: satisfactory parameters for one date are usual
acceptable for the next one and the following one as well. Thus, the
full backtest can be carried out with these values when updating the
models at each period. The backtest nonetheless remains
compute-intensive because the model has to be re-trained with the most
recent data for each rebalancing moment.

\hypertarget{ensemble}{%
\chapter{Ensemble models}\label{ensemble}}

Let us be honest. When facing a prediction task, it is not obvious to
determine the best choice between ML tools: penalized regressions, tree
methods, neural networks, SVMs, etc. A natural and tempting alternative
is to combine several algorithms (or the predictions that result from
them) to try to extract value out of each engine (or learner). This
intention is not new and contributions towards this goal go back at
least to \citet{bates1969combination} (for the prupose of passenger flow
forecasting).

Below, we outline a few books on the topic of ensembles. The first four
are monographs while the last two are compilations of contributions:

\begin{itemize}
\tightlist
\item
  \citet{zhou2012ensemble}: a very didactic book that covers the main
  ideas of ensembles;\\
\item
  \citet{schapire2012boosting}: the main reference for boosting (and
  hence, ensembling) with many theoretical results and thus strong
  mathematical groundings;\\
\item
  \citet{seni2010ensemble}: an introduction dedicated to tree methods
  mainly;\\
\item
  \citet{claeskens2008model}: an overview of model selection techniques
  with a few chapter focused on model averaging;\\
\item
  \citet{zhang2012ensemble}: a collection of thematic chapters on
  ensemble learning;\\
\item
  \citet{okun2011ensembles}: examples of applications of ensembles.
\end{itemize}

In this chapter, we cover the basic ideas and concepts behind the notion
of ensembles. We refer to the above books for deeper treatments on the
topic. We underline that several ensemble methods have already been
mentioned and covered earlier, notably in Chapter \ref{trees}. Indeed,
random forests and boosted trees are examples of ensembles. Hence, other
early articles on the combination of learners are
\citet{schapire1990strength}, \citet{jacobs1991adaptive} (for neural
networks particularly), and \citet{freund1997decision}.

\hypertarget{linear-ensembles}{%
\section{Linear ensembles}\label{linear-ensembles}}

\hypertarget{principles}{%
\subsection{Principles}\label{principles}}

In this chapter we adopt the following notations. We work with \(M\)
models where \(\tilde{y}_{i,m}\) is the prediction of model \(m\) for
instance \(i\) and errors \(\epsilon_{i,m}=y_i-\tilde{y}_{i,m}\) are
stacked into a \((I\times M)\) matrix \(\textbf{E}\). A linear
combination of models has sample errors equal to \(\textbf{Ew}\), where
\(\textbf{w}=w_m\) are the weights assigned to each model and we assume
\(\textbf{w}'\textbf{1}_M=1\). Minimizing the total (squared) error is
thus a simple quadratic program with unique constraint. The Lagrange
function is
\(L(\textbf{w})=\textbf{w}'\textbf{E}'\textbf{E}\textbf{w}-\lambda (\textbf{w}'\textbf{1}_M-1)\)
and hence
\[\frac{\partial}{\partial \textbf{w}}L(\textbf{w})=\textbf{E}'\textbf{E}\textbf{w}-\lambda \textbf{1}_M=0 \quad \Leftrightarrow \quad \textbf{w}=\lambda(\textbf{E}'\textbf{E})^{-1}\textbf{1}_M,\]

and the constraint imposes
\(\textbf{w}^*=\frac{(\textbf{E}'\textbf{E})^{-1}\textbf{1}_M}{(\textbf{1}_M'\textbf{E}'\textbf{E})^{-1}\textbf{1}_M}\).
This form is similar to that of minimum variance portfolios. If errors
are unbiased (\(\textbf{1}_I'\textbf{E}=\textbf{0}_M'\)), then
\(\textbf{E}'\textbf{E}\) is the covariance matrix of errors.

This expression shows an important feature of optimized linear
ensembles: they can only add value if the models tell different stories.
If two models are redundant, \(\textbf{E}'\textbf{E}\) will be close to
singular and \(\textbf{w}^*\) will arbitrage one against the other in a
spurious fashion. This is the exact same problem as when mean-variance
portfolios are constituted with highly correlated assets: in this case,
diversification fails because when things go wrong, all assets go down.
Another problem arises when the number of observations is too small
compared to the number of assets so that the covariance matrix of
returns is singular. This is not an issue for ensembles because the
number of observations will usually be much larger than the number of
models (\(I>>M\)).

In the limit when correlations increase to one, the above formulation
becomes highly unstable and ensembles cannot be trusted. One heuristic
way to see this is when \(M=2\) and \[\textbf{E}'\textbf{E}=\left[
\begin{array}{cc} \sigma_1^2 & \rho\sigma_1\sigma_2 \\
\rho\sigma_1\sigma_2 & \sigma_2^2 \\
\end{array}
\right] \quad \Leftrightarrow  \quad 
(\textbf{E}'\textbf{E})^{-1}=\frac{1}{1-\rho^2}\left[
\begin{array}{cc} \sigma_1^{-2} & -\rho(\sigma_1\sigma_2)^{-1} \\
-\rho(\sigma_1\sigma_2)^{-1} & \sigma_2^{-2} \\
\end{array}
\right]\]

so that when \(\rho \rightarrow 1\), the model with the smallest errors
(minimum \(\sigma_i^2\)) will see its weight increasing towards infinity
while the other model will have a similarly large negative weight.

One improvement proposed to circumvent this trouble, advocated in a
seminal publication (\citet{breiman1996stacked}), is to enforce
positivity constraints on the weights and solve

\[\underset{\textbf{w}}{\text{argmin}} \ \textbf{w}'\textbf{E}'\textbf{E}\textbf{w} , \quad \text{s.t.} \quad \left\{ 
\begin{array}{l} \textbf{w}'\textbf{1}_M=1 \\ w_m \ge 0 \quad \forall m \end{array}\right. .\]

Mechanically, if several models are highly correlated, the constraint
will impose that only one of them will have a nonzero weight. If there
are many models, then just a few of them will be selected by the
minimization program. In a very different context,
\citet{jagannathan2003risk} have shown the benefits of constraint in the
construction mean-variance portfolios. In our setting, the constraint
will similarly help discriminate wisely among the `best' models.

In the literature, forecast combination and model averaging (which are
synonyms of ensembles) have been tested on stock markets as early as in
\citet{von1972probabilistic}. Surprisingly, the articles were not
published in Finance journals but rather in fields such as Management
(\citet{virtanen1987forecasting}, \citet{wang2012stock}), Economics and
Econometrics (\citet{donaldson1996forecast},
\citet{clark2009improving}), Operations Reasearch
(\citet{huang2005forecasting}, \citet{leung2001using}, and, recently,
\citet{bonaccolto2019developing}), and Computer Science
(\citet{harrald1997evolving}, \citet{hassan2007fusion}).

In the general forecasting literature, many alternative (refined)
methods for combining forecasts have been studied. Trimmed opinion pools
(\citet{grushka2016ensembles}) compute averages over the predictions
that are not too extreme. We refer to \citet{gaba2017combining} for a
more exhaustive list of combinations as well as for an empirical study
of their respective efficiency. Overall, findings are mixed and the
heuristic simple average is, as usual, hard to beat (see, e.g.,
\citet{genre2013combining})

\hypertarget{example}{%
\subsection{Example}\label{example}}

In order to build an ensemble, we must gather the predictions and the
corresponding errors into the \(\textbf{E}\) matrix. We will work with 5
models that were trained in the previous chapters: penalized regression,
simple tree, random forest, xgboost and feedforward neural network. The
training errors have zero means, hence \(\textbf{E}'\textbf{E}\) is the
covariance matrix of errors between models.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{err_pen_train <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit_pen_pred, x_penalized_train) }\OperatorTok{-}\StringTok{ }\NormalTok{training_sample}\OperatorTok{$}\NormalTok{R1M_Usd  }\CommentTok{# Regression}
\NormalTok{err_tree_train <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit_tree, training_sample) }\OperatorTok{-}\StringTok{ }\NormalTok{training_sample}\OperatorTok{$}\NormalTok{R1M_Usd       }\CommentTok{# Tree}
\NormalTok{err_RF_train <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit_RF, training_sample) }\OperatorTok{-}\StringTok{ }\NormalTok{training_sample}\OperatorTok{$}\NormalTok{R1M_Usd           }\CommentTok{# Random Forest}
\NormalTok{err_XGB_train <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit_xgb, train_matrix_xgb) }\OperatorTok{-}\StringTok{ }\NormalTok{training_sample}\OperatorTok{$}\NormalTok{R1M_Usd        }\CommentTok{# XGBoost}
\NormalTok{err_NN_train <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model, NN_train_features) }\OperatorTok{-}\StringTok{ }\NormalTok{training_sample}\OperatorTok{$}\NormalTok{R1M_Usd          }\CommentTok{# Neural Network}
\NormalTok{E <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(err_pen_train, err_tree_train, err_RF_train, err_XGB_train, err_NN_train) }\CommentTok{# The E matrix}
\KeywordTok{colnames}\NormalTok{(E) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Pen_reg"}\NormalTok{, }\StringTok{"Tree"}\NormalTok{, }\StringTok{"RF"}\NormalTok{, }\StringTok{"XGB"}\NormalTok{, }\StringTok{"NN"}\NormalTok{)                               }\CommentTok{# Column names}
\KeywordTok{cor}\NormalTok{(E)                                                                               }\CommentTok{# Corr. matrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Pen_reg      Tree        RF       XGB        NN
## Pen_reg 1.0000000 0.9982507 0.9968224 0.9475378 0.9959097
## Tree    0.9982507 1.0000000 0.9973481 0.9459733 0.9964547
## RF      0.9968224 0.9973481 1.0000000 0.9446178 0.9971627
## XGB     0.9475378 0.9459733 0.9446178 1.0000000 0.9437547
## NN      0.9959097 0.9964547 0.9971627 0.9437547 1.0000000
\end{verbatim}

\normalsize

As is shown by the correlation matrix, the models fail to generate
heterogeneity in their predictions. The minimum correlation (though
above 95\%!) is obtained by the boosted tree models. Below, we compare
the training accuracy of models by computing the average absolute value
of errors.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(}\KeywordTok{abs}\NormalTok{(E), }\DecValTok{2}\NormalTok{, mean) }\CommentTok{# Mean absolute error or columns of E }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Pen_reg       Tree         RF        XGB         NN 
## 0.08345916 0.08366795 0.08327121 0.08916813 0.08375389
\end{verbatim}

\normalsize

The best performing ML engine is the random forest. The boosted tree
model is the worst, by far. Below, we compute the optimal (non
constrained) weights for the combination of models.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{w_ensemble <-}\StringTok{ }\KeywordTok{solve}\NormalTok{(}\KeywordTok{t}\NormalTok{(E) }\OperatorTok{%*%}\StringTok{ }\NormalTok{E) }\OperatorTok{%*%}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{)                                         }\CommentTok{# Optimal weights}
\NormalTok{w_ensemble <-}\StringTok{ }\NormalTok{w_ensemble }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(w_ensemble)}
\NormalTok{w_ensemble}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                 [,1]
## Pen_reg -0.608296947
## Tree     0.017714031
## RF       1.334108376
## XGB     -0.002460814
## NN       0.258935354
\end{verbatim}

\normalsize

Because of the high correlations, the optimal weights are not balanced
and diversified: they load heavily on the random forest learner (best in
sample model) and `short' a few models in order to compensate.

Note that the weights are of course computed with \textbf{training
errors}. The optimal combination is then tested on the testing sample.
Below, we compute out-of-sample (testing) errors and their average
absolute value.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{err_pen_test <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit_pen_pred, x_penalized_test) }\OperatorTok{-}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd     }\CommentTok{# Regression}
\NormalTok{err_tree_test <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit_tree, testing_sample) }\OperatorTok{-}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd          }\CommentTok{# Tree}
\NormalTok{err_RF_test <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit_RF, testing_sample) }\OperatorTok{-}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd              }\CommentTok{# Random Forest}
\NormalTok{err_XGB_test <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit_xgb, xgb_test) }\OperatorTok{-}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd                  }\CommentTok{# XGBoost}
\NormalTok{err_NN_test <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model, NN_test_features) }\OperatorTok{-}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd             }\CommentTok{# Neural Network}
\NormalTok{E_test <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(err_pen_test, err_tree_test, err_RF_test, err_XGB_test, err_NN_test) }\CommentTok{# The E matrix}
\KeywordTok{colnames}\NormalTok{(E_test) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Pen_reg"}\NormalTok{, }\StringTok{"Tree"}\NormalTok{, }\StringTok{"RF"}\NormalTok{, }\StringTok{"XGB"}\NormalTok{, }\StringTok{"NN"}\NormalTok{)}
\KeywordTok{apply}\NormalTok{(}\KeywordTok{abs}\NormalTok{(E_test), }\DecValTok{2}\NormalTok{, mean) }\CommentTok{# Mean absolute error or columns of E }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Pen_reg       Tree         RF        XGB         NN 
## 0.06618181 0.06650492 0.06710349 0.07149006 0.06766225
\end{verbatim}

\normalsize

The boosted tree model is still the worst performing algorithm while the
simple models (regression and simple tree) are the ones that fare the
best. The most naive combination is the simple average of model and
predictions.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{err_EW_test <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(E_test, }\DecValTok{1}\NormalTok{, mean)  }\CommentTok{# Equally weighted combination}
\KeywordTok{mean}\NormalTok{(}\KeywordTok{abs}\NormalTok{(err_EW_test))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.06695977
\end{verbatim}

\normalsize

Because the errors are very correlated, the equally-weighted combination
of forecasts yields an average error which lies `in the middle' of
individual errors. The diversification benefits are too small. Let us
now test the `optimal' combination
\(\textbf{w}^*=\frac{(\textbf{E}'\textbf{E})^{-1}\textbf{1}_M}{(\textbf{1}_M'\textbf{E}'\textbf{E})^{-1}\textbf{1}_M}\).

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{err_opt_test <-}\StringTok{ }\NormalTok{E_test }\OperatorTok{%*%}\StringTok{ }\NormalTok{w_ensemble   }\CommentTok{# Optimal unconstrained combination}
\KeywordTok{mean}\NormalTok{(}\KeywordTok{abs}\NormalTok{(err_opt_test))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.06838265
\end{verbatim}

\normalsize

Again, the result is disappointing because of the lack of
diversification across models. The correlations are high not only on the
training sample, but also on the testing sample, as shown below.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(E_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Pen_reg      Tree        RF       XGB        NN
## Pen_reg 1.0000000 0.9985518 0.9968882 0.9706767 0.9958360
## Tree    0.9985518 1.0000000 0.9977644 0.9752078 0.9968229
## RF      0.9968882 0.9977644 1.0000000 0.9776554 0.9974873
## XGB     0.9706767 0.9752078 0.9776554 1.0000000 0.9776927
## NN      0.9958360 0.9968229 0.9974873 0.9776927 1.0000000
\end{verbatim}

\normalsize

The leverage from the optimal solution only exacerbates the problem and
underperforms the heuristic uniform combination. We end this section
with the constrained formulation of \citet{breiman1996stacked} using the
\emph{quadprog} package. If we write \(\mathbf{\Sigma}\) for the
covariance matrix of errors, we seek
\[\mathbf{w}^*=\underset{\mathbf{w}}{\text{argmin}} \ \mathbf{w}'\mathbf{\Sigma}\mathbf{w}, \quad \mathbf{1}'\mathbf{w}=1, \quad w_i\ge 0,\]
The constraints will be handled as:

\[\mathbf{A} \mathbf{w}= \begin{bmatrix} 
1 & 1 & 1 \\
1 & 0 & 0\\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix} \mathbf{w} \hspace{9mm} \text{ compared to} \hspace{9mm} \mathbf{b}=\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix},  \]

where the first line will be an equality and the last three will be
inequalities.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(quadprog)                       }\CommentTok{# Package for quadratic programming}
\NormalTok{Sigma <-}\StringTok{ }\KeywordTok{t}\NormalTok{(E) }\OperatorTok{%*%}\StringTok{ }\NormalTok{E                     }\CommentTok{# Unscaled covariance matrix}
\NormalTok{nb_mods <-}\StringTok{ }\KeywordTok{nrow}\NormalTok{(Sigma)                  }\CommentTok{# Number of models}
\NormalTok{w_const <-}\StringTok{ }\KeywordTok{solve.QP}\NormalTok{(}\DataTypeTok{Dmat =}\NormalTok{ Sigma,       }\CommentTok{# D matrix =  Sigma}
              \DataTypeTok{dvec =} \KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, nb_mods),   }\CommentTok{# Zero vector}
              \DataTypeTok{Amat =} \KeywordTok{rbind}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, nb_mods), }\KeywordTok{diag}\NormalTok{(nb_mods)) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{t}\NormalTok{(), }\CommentTok{# A matrix for constraints}
              \DataTypeTok{bvec =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, nb_mods)),                          }\CommentTok{# b vector for constraints}
              \DataTypeTok{meq =} \DecValTok{1}                   \CommentTok{# One line of constraints is equality, others are inequalities}
\NormalTok{              )}
\NormalTok{w_const}\OperatorTok{$}\NormalTok{solution }\OperatorTok{%>%}\StringTok{ }\KeywordTok{round}\NormalTok{(}\DecValTok{3}\NormalTok{)           }\CommentTok{# Solution}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.000 0.000 0.929 0.000 0.071
\end{verbatim}

\normalsize

Compared to the unconstrained solution, the weights ar sparse a
concentrated on one or two models, usually those with small training
sample errors.

\hypertarget{stacked-ensembles}{%
\section{Stacked ensembles}\label{stacked-ensembles}}

\hypertarget{two-stage-training}{%
\subsection{Two stage training}\label{two-stage-training}}

Stacked ensembles are a natural generalization of linear ensembles. The
idea of generalizing linear ensembles goes back at least to
\citet{wolpert1992stacked}. In the general case, the training is
performed in two stages. The first stage is the simple one, whereby the
\(M\) models are trained independently, yielding the predictions
\(\tilde{y}_{i,m}\) for instance \(i\) and model\(m\). The second step
is to consider the output of the trained models as input for a new level
of machine learning optimization. The second level predictions are
\(\breve{y}_i=h(\tilde{y}_{i,1},\dots,\tilde{y}_{i,M})\), where \(h\) is
a new learner (see Figure \ref{fig:stackscheme}). Linear ensembles are
of course stacked ensembles in which the second layer is a linear
regression.

The same techniques are then applied to minimize the error between the
true values \(y_i\) and the predicted ones \(\breve{y}_i\).

\begin{figure}[H]

{\centering \includegraphics[width=350px]{images/stack} 

}

\caption{Scheme of Stacked Ensembles.}\label{fig:stackscheme}
\end{figure}

\hypertarget{code-and-results-4}{%
\subsection{Code and results}\label{code-and-results-4}}

Below, we create a low-dimensional neural network which takes in the
individual predictions of each model and compiles them into a synthetic
forecast.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model_stack <-}\StringTok{ }\KeywordTok{keras_model_sequential}\NormalTok{()}
\NormalTok{model_stack }\OperatorTok{%>%}\StringTok{   }\CommentTok{# This defines the structure of the network, i.e. how layers are organized}
\StringTok{    }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{8}\NormalTok{, }\DataTypeTok{activation =} \StringTok{'relu'}\NormalTok{, }\DataTypeTok{input_shape =}\NormalTok{ nb_mods) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{4}\NormalTok{, }\DataTypeTok{activation =} \StringTok{'tanh'}\NormalTok{) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{1}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\normalsize

The configuration is very simple. We do not include any optional
arguments and hence the model is likely to overfit. As we seek to
predict returns, the loss function is the standard \(L^2\) norm.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model_stack }\OperatorTok{%>%}\StringTok{ }\KeywordTok{compile}\NormalTok{(                       }\CommentTok{# Model specification}
    \DataTypeTok{loss =} \StringTok{'mean_squared_error'}\NormalTok{,               }\CommentTok{# Loss function}
    \DataTypeTok{optimizer =} \KeywordTok{optimizer_rmsprop}\NormalTok{(),           }\CommentTok{# Optimisation method (weight updating)}
    \DataTypeTok{metrics =} \KeywordTok{c}\NormalTok{(}\StringTok{'mean_absolute_error'}\NormalTok{)         }\CommentTok{# Output metric}
\NormalTok{)}
\KeywordTok{summary}\NormalTok{(model_stack)                           }\CommentTok{# Model architecture}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## dense_8 (Dense)                  (None, 8)                     48          
## ___________________________________________________________________________
## dense_9 (Dense)                  (None, 4)                     36          
## ___________________________________________________________________________
## dense_10 (Dense)                 (None, 1)                     5           
## ===========================================================================
## Total params: 89
## Trainable params: 89
## Non-trainable params: 0
## ___________________________________________________________________________
\end{verbatim}

\normalsize

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y_tilde <-}\StringTok{ }\NormalTok{E }\OperatorTok{+}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rep}\NormalTok{(training_sample}\OperatorTok{$}\NormalTok{R1M_Usd, nb_mods), }\DataTypeTok{ncol =}\NormalTok{ nb_mods)    }\CommentTok{# Training predictions}
\NormalTok{y_test <-}\StringTok{ }\NormalTok{E_test }\OperatorTok{+}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rep}\NormalTok{(testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd, nb_mods), }\DataTypeTok{ncol =}\NormalTok{ nb_mods) }\CommentTok{# Testing}
\NormalTok{fit_NN_stack <-}\StringTok{ }\NormalTok{model_stack }\OperatorTok{%>%}\StringTok{ }\KeywordTok{fit}\NormalTok{(y_tilde,                                  }\CommentTok{# Training features}
\NormalTok{                     training_sample}\OperatorTok{$}\NormalTok{R1M_Usd,                                 }\CommentTok{# Training labels}
                     \DataTypeTok{epochs =} \DecValTok{6}\NormalTok{, }\DataTypeTok{batch_size =} \DecValTok{512}\NormalTok{,                            }\CommentTok{# Training parameters}
                     \DataTypeTok{validation_data =} \KeywordTok{list}\NormalTok{(y_test,                           }\CommentTok{# Test data (features)}
\NormalTok{                                            testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd)           }\CommentTok{# Test data (labels)}
\NormalTok{)}
\KeywordTok{plot}\NormalTok{(fit_NN_stack)                                                            }\CommentTok{# Plot, evidently!}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=350px]{ML_factor_files/figure-latex/stackNN2-1} 

}

\caption{Training metrics for the ensemble model.}\label{fig:stackNN2}
\end{figure}

\normalsize

The performance of the ensemble is again disappointing. The training
adds little value which means that the new overarching layer of ML does
not enhance the original predictions. Again, this is because all ML
engines seem to be capturing the same patterns and their both linear and
non-linear combinations fail to improve their performance.

\hypertarget{extensions-1}{%
\section{Extensions}\label{extensions-1}}

\hypertarget{exogenous-variables}{%
\subsection{Exogenous variables}\label{exogenous-variables}}

In a financial context, macro-economic indicators could add value to the
process. It is possible that some models perform better under certain
conditions and exogenous predictors can help introduce a flavor of
economic-driven conditionality in the predictions.

Adding macro variables to the set of predictors (here, predictions)
\(\tilde{y}_{i,m}\) could seem like one way to achieve this. However,
this would amount to mix predicted values with (possibly scaled)
economoc indicators and that would not make much sense.

One alternative outside the perimeter of ensembles is to train simple
trees on a set of macroeconomic indicators. If the labels are the
(possibly absolute) errors stemming from the original predictions, then
the trees will create clusters of homogeneous error values. This will
hint towards which conditions lead to the best and worst forecasts. We
test this idea below, using aggregate data from the Federal Reserve of
Saint Louis. A simple downloading function is available in the
\emph{quantmod} package. We download and format the data in the next
chunk. CPIAUCSL is a code for consumer price index and T10Y2YM is a code
for the term spread (10Y minus 2Y).

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(quantmod)                                     }\CommentTok{# Package that extracts the data}
\KeywordTok{library}\NormalTok{(lubridate)                                    }\CommentTok{# Package for date management}
\KeywordTok{getSymbols}\NormalTok{(}\StringTok{"CPIAUCSL"}\NormalTok{, }\DataTypeTok{src =} \StringTok{"FRED"}\NormalTok{)                  }\CommentTok{# FRED is the Fed of St Louis}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "CPIAUCSL"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{getSymbols}\NormalTok{(}\StringTok{"T10Y2YM"}\NormalTok{, }\DataTypeTok{src =} \StringTok{"FRED"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "T10Y2YM"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cpi <-}\StringTok{ }\KeywordTok{fortify}\NormalTok{(CPIAUCSL) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate}\NormalTok{ (}\DataTypeTok{inflation =}\NormalTok{ CPIAUCSL }\OperatorTok{/}\StringTok{ }\KeywordTok{lag}\NormalTok{(CPIAUCSL) }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{) }\CommentTok{# Inflation via Consumer Price Index}
\NormalTok{ts <-}\StringTok{ }\KeywordTok{fortify}\NormalTok{(T10Y2YM)                                }\CommentTok{# Term spread (10Y minus 2Y rates)}
\KeywordTok{colnames}\NormalTok{(ts)[}\DecValTok{2}\NormalTok{] <-}\StringTok{ "termspread"}                       \CommentTok{# To make things clear}
\NormalTok{ens_data <-}\StringTok{ }\NormalTok{testing_sample }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{select}\NormalTok{(date) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{cbind}\NormalTok{(err_NN_test) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Index =} \KeywordTok{make_date}\NormalTok{(}\DataTypeTok{year =}\NormalTok{ lubridate}\OperatorTok{::}\KeywordTok{year}\NormalTok{(date),       }\CommentTok{# Change date to first day of month}
                             \DataTypeTok{month =}\NormalTok{ lubridate}\OperatorTok{::}\KeywordTok{month}\NormalTok{(date), }
                             \DataTypeTok{day =} \DecValTok{1}\NormalTok{)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{left_join}\NormalTok{(cpi) }\OperatorTok{%>%}\StringTok{                                }\CommentTok{# Add CPI to the dataset}
\StringTok{    }\KeywordTok{left_join}\NormalTok{(ts)                                     }\CommentTok{# Add termspread}
\KeywordTok{head}\NormalTok{(ens_data)                                        }\CommentTok{# Show first lines}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         date  err_NN_test      Index CPIAUCSL   inflation termspread
## 1 2014-01-31 -0.150481847 2014-01-01  235.288 0.002424175       2.47
## 2 2014-02-28  0.077492399 2014-02-01  235.547 0.001100779       2.38
## 3 2014-03-31 -0.005263589 2014-03-01  236.028 0.002042055       2.32
## 4 2014-04-30 -0.067815196 2014-04-01  236.468 0.001864186       2.29
## 5 2014-05-31 -0.082144422 2014-05-01  236.918 0.001903006       2.17
## 6 2014-06-30  0.047112189 2014-06-01  237.231 0.001321132       2.15
\end{verbatim}

\normalsize

We can now build a tree that tries to explain the accuracy of models as
a function of macro variables.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rpart.plot)     }\CommentTok{# Load package for tree plotting}
\NormalTok{fit_ens <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(}\KeywordTok{abs}\NormalTok{(err_NN_test) }\OperatorTok{~}\StringTok{ }\NormalTok{inflation }\OperatorTok{+}\StringTok{ }\NormalTok{termspread, }\CommentTok{# Tree model}
                 \DataTypeTok{data =}\NormalTok{ ens_data,}
                 \DataTypeTok{cp =} \FloatTok{0.001}\NormalTok{)                                }\CommentTok{# Complexity parameter (size of tree)}
\KeywordTok{rpart.plot}\NormalTok{(fit_ens)                                         }\CommentTok{# Plot tree}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=250px]{ML_factor_files/figure-latex/r ensfred-1} 

}

\caption{Conditional performance of an ML engine.}(\#fig:r ensfred)
\end{figure}

\normalsize

The tree creates clusters which have homogeneous values of absolute
errors. One big cluster gathers 92\% of predictions (the left one) and
is the one with the smallest average. It corresponds to the periods when
the term spread is above 0.29 (in percentage points). The other two
groups (when the term spread is below 0.29\%) are determined according
to the level of inflation. If the latter is positive, then the average
absolute error is 7\%, if not, it is 12\%. This last number, the highest
of the three clusters, indicates that when the term spread is low and
the inflation negative, the model's predictions are not trustworthy
because their errors have a magnitude twice as large as in other
periods. Under these circumstances (which seem to be linked to a dire
economic environment), it may be wiser not to use ML-based forecasts.

\hypertarget{shrinking-inter-model-correlations}{%
\subsection{Shrinking inter-model
correlations}\label{shrinking-inter-model-correlations}}

As shown earlier in this chapter, one major problem with ensembles
arises when the first layer of predictions is highly correlated. In this
case, ensemble are pretty much useless. Their are several tricks that
can help reduce this correlation but the simplest and best is probably
to alter training samples. If algorithms do not see the same data, they
will probably infer different patterns.

There are several ways to split the training data so as to build
different subsets of training samples. The first dichotomy is between
random versus deterministic splits. Random splits are easy and require
only the target sample size to be fixed. Note that the training samples
can be overlapping as long as the overlap is not too large. Hence if the
original training sample has \(I\) instance and the ensemble requires
\(M\) models, then a subsample size of \(\lfloor I/M \rfloor\) may be
too conservative especially if the training sample is not very large. In
this case \(\lfloor I/\sqrt{M} \rfloor\) may be a better alternative.
Random forests are one example of ensembles built in random training
samples.

One advantage of deterministic splits is that they are easy to reproduce
and their outcome does not depend on the random seed. By the nature of
factor-based training samples, the second splitting dichotomy is between
time and assets. A split within assets is straightforward: each model is
trained on a different set of stocks. Note that the choices of sets can
be random, or dictacted by some factor-based criterion: size, momentum,
book-to-market ratio, etc.

A split in dates requires other decisions: is the data split in large
blocks (like years) and each model gets a block, which may stand for one
particular kind of market condition? Or are the training dates divided
more regularly? For instance, if there are 12 models in the ensemble,
each model can be trained on data from a given month (e.g., January for
the first models, February for the second, etc.).

Below, we train four models on four different years to see if this help
reduce the inter-model correlations. This process is a bit lengthy
because the samples and models need to be all redefined. We start by
creating the four training samples. The third model works on the small
subset of features, hence the sample is smaller.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{training_sample_}\DecValTok{2007}\NormalTok{ <-}\StringTok{ }\NormalTok{training_sample }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{filter}\NormalTok{(date }\OperatorTok{>}\StringTok{ "2006-12-31"}\NormalTok{, date }\OperatorTok{<}\StringTok{ "2008-01-01"}\NormalTok{)}
\NormalTok{training_sample_}\DecValTok{2009}\NormalTok{ <-}\StringTok{ }\NormalTok{training_sample }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{filter}\NormalTok{(date }\OperatorTok{>}\StringTok{ "2008-12-31"}\NormalTok{, date }\OperatorTok{<}\StringTok{ "2010-01-01"}\NormalTok{)}
\NormalTok{training_sample_}\DecValTok{2011}\NormalTok{ <-}\StringTok{ }\NormalTok{training_sample }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"date"}\NormalTok{,features_short, }\StringTok{"R1M_Usd"}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{filter}\NormalTok{(date }\OperatorTok{>}\StringTok{ "2010-12-31"}\NormalTok{, date }\OperatorTok{<}\StringTok{ "2012-01-01"}\NormalTok{)}
\NormalTok{training_sample_}\DecValTok{2013}\NormalTok{ <-}\StringTok{ }\NormalTok{training_sample }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{filter}\NormalTok{(date }\OperatorTok{>}\StringTok{ "2012-12-31"}\NormalTok{, date }\OperatorTok{<}\StringTok{ "2014-01-01"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\normalsize

Then, we proceed to the training of the models. The syntaxes are those
used in the previous chapters, nothing new here. We start with a
penalized regression. In all predictions below, the original testing
sample is used.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y_ens_}\DecValTok{2007}\NormalTok{ <-}\StringTok{ }\NormalTok{training_sample_}\DecValTok{2007}\OperatorTok{$}\NormalTok{R1M_Usd                                       }\CommentTok{# Dep. variable}
\NormalTok{x_ens_}\DecValTok{2007}\NormalTok{ <-}\StringTok{ }\NormalTok{training_sample_}\DecValTok{2007} \OperatorTok{%>%}\StringTok{                                           }\CommentTok{# Predictors}
\StringTok{    }\KeywordTok{select}\NormalTok{(features) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{() }
\NormalTok{fit_ens_}\DecValTok{2007}\NormalTok{ <-}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(x_ens_}\DecValTok{2007}\NormalTok{, y_ens_}\DecValTok{2007}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.1}\NormalTok{, }\DataTypeTok{lambda =} \FloatTok{0.1}\NormalTok{)        }\CommentTok{# Model}
\NormalTok{err_ens_}\DecValTok{2007}\NormalTok{ <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit_ens_}\DecValTok{2007}\NormalTok{, x_penalized_test) }\OperatorTok{-}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd }\CommentTok{# Prediction errors}
\end{Highlighting}
\end{Shaded}

\normalsize

We continue with a random forest.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit_ens_}\DecValTok{2009}\NormalTok{ <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(formula,            }\CommentTok{# Same formula as for simple trees!}
                 \DataTypeTok{data =}\NormalTok{ training_sample_}\DecValTok{2009}\NormalTok{,    }\CommentTok{# Data source: 2011 training sample}
                 \DataTypeTok{sampsize =} \DecValTok{4000}\NormalTok{,                }\CommentTok{# Size of (random) sample for each tree}
                 \DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{,                }\CommentTok{# Is the sampling done with replacement?}
                 \DataTypeTok{nodesize =} \DecValTok{100}\NormalTok{,                 }\CommentTok{# Minimum size of terminal cluster}
                 \DataTypeTok{ntree =} \DecValTok{40}\NormalTok{,                     }\CommentTok{# Nb of random trees}
                 \DataTypeTok{mtry =} \DecValTok{30}                       \CommentTok{# Nb of predictive variables for each tree}
\NormalTok{    )}
\NormalTok{err_ens_}\DecValTok{2009}\NormalTok{ <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit_ens_}\DecValTok{2009}\NormalTok{, testing_sample) }\OperatorTok{-}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd }\CommentTok{# Prediction errors}
\end{Highlighting}
\end{Shaded}

\normalsize

The third model is a boosted tree.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train_features_}\DecValTok{2011}\NormalTok{ <-}\StringTok{ }\NormalTok{training_sample_}\DecValTok{2011} \OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{select}\NormalTok{(features_short) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{()                            }\CommentTok{# Independent variable}
\NormalTok{train_label_}\DecValTok{2011}\NormalTok{ <-}\StringTok{ }\NormalTok{training_sample_}\DecValTok{2011} \OperatorTok{%>%}
\StringTok{    }\KeywordTok{select}\NormalTok{(R1M_Usd) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{()                             }\CommentTok{# Dependent variable}
\NormalTok{train_matrix_}\DecValTok{2011}\NormalTok{ <-}\StringTok{ }\KeywordTok{xgb.DMatrix}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ train_features_}\DecValTok{2011}\NormalTok{, }
                                \DataTypeTok{label =}\NormalTok{ train_label_}\DecValTok{2011}\NormalTok{)       }\CommentTok{# XGB format!}
\NormalTok{fit_ens_}\DecValTok{2011}\NormalTok{ <-}\StringTok{ }\KeywordTok{xgb.train}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ train_matrix_}\DecValTok{2011}\NormalTok{,             }\CommentTok{# Data source }
              \DataTypeTok{eta =} \FloatTok{0.4}\NormalTok{,                                        }\CommentTok{# Learning rate}
              \DataTypeTok{objective =} \StringTok{"reg:linear"}\NormalTok{,                         }\CommentTok{# Objective function}
              \DataTypeTok{max_depth =} \DecValTok{4}\NormalTok{,                                    }\CommentTok{# Maximum depth of trees}
              \DataTypeTok{nrounds =} \DecValTok{18}                                      \CommentTok{# Number of trees used}
\NormalTok{    )}
\NormalTok{err_ens_}\DecValTok{2011}\NormalTok{ <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit_ens_}\DecValTok{2011}\NormalTok{, xgb_test) }\OperatorTok{-}\StringTok{  }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd }\CommentTok{# Prediction errors}
\end{Highlighting}
\end{Shaded}

\normalsize

Finally, the last model is a simple neural network.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NN_features_}\DecValTok{2013}\NormalTok{ <-}\StringTok{ }\KeywordTok{select}\NormalTok{(training_sample_}\DecValTok{2013}\NormalTok{, features) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{() }\CommentTok{# Matrix format is important}
\NormalTok{NN_labels_}\DecValTok{2013}\NormalTok{ <-}\StringTok{ }\NormalTok{training_sample_}\DecValTok{2013}\OperatorTok{$}\NormalTok{R1M_Usd}
\NormalTok{model_ens_}\DecValTok{2013}\NormalTok{ <-}\StringTok{ }\KeywordTok{keras_model_sequential}\NormalTok{()}
\NormalTok{model_ens_}\DecValTok{2013} \OperatorTok{%>%}\StringTok{   }\CommentTok{# This defines the structure of the network, i.e. how layers are organized}
\StringTok{    }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{16}\NormalTok{, }\DataTypeTok{activation =} \StringTok{'relu'}\NormalTok{, }\DataTypeTok{input_shape =} \KeywordTok{ncol}\NormalTok{(NN_features_}\DecValTok{2013}\NormalTok{)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{8}\NormalTok{, }\DataTypeTok{activation =} \StringTok{'tanh'}\NormalTok{) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{1}\NormalTok{) }
\NormalTok{model_ens_}\DecValTok{2013} \OperatorTok{%>%}\StringTok{ }\KeywordTok{compile}\NormalTok{(                    }\CommentTok{# Model specification}
    \DataTypeTok{loss =} \StringTok{'mean_squared_error'}\NormalTok{,               }\CommentTok{# Loss function}
    \DataTypeTok{optimizer =} \KeywordTok{optimizer_rmsprop}\NormalTok{(),           }\CommentTok{# Optimisation method (weight updating)}
    \DataTypeTok{metrics =} \KeywordTok{c}\NormalTok{(}\StringTok{'mean_absolute_error'}\NormalTok{)         }\CommentTok{# Output metric}
\NormalTok{)}
\NormalTok{model_ens_}\DecValTok{2013} \OperatorTok{%>%}\StringTok{ }\KeywordTok{fit}\NormalTok{(NN_features_}\DecValTok{2013}\NormalTok{,                        }\CommentTok{# Training features}
\NormalTok{                       NN_labels_}\DecValTok{2013}\NormalTok{,                          }\CommentTok{# Training labels}
                       \DataTypeTok{epochs =} \DecValTok{9}\NormalTok{, }\DataTypeTok{batch_size =} \DecValTok{128}             \CommentTok{# Training parameters}
\NormalTok{)}
\NormalTok{err_ens_}\DecValTok{2013}\NormalTok{ <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(model_ens_}\DecValTok{2013}\NormalTok{, NN_test_features) }\OperatorTok{-}\StringTok{ }\NormalTok{testing_sample}\OperatorTok{$}\NormalTok{R1M_Usd}
\end{Highlighting}
\end{Shaded}

\normalsize

Endowed with the errors of the four models, we can compute their
correlation matrix.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{E_subtraining <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(err_ens_}\DecValTok{2007}\NormalTok{,}
\NormalTok{                        err_ens_}\DecValTok{2009}\NormalTok{,}
\NormalTok{                        err_ens_}\DecValTok{2011}\NormalTok{,}
\NormalTok{                        err_ens_}\DecValTok{2013}\NormalTok{)}
\KeywordTok{cor}\NormalTok{(E_subtraining)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              err_ens_2007 err_ens_2009 err_ens_2011 err_ens_2013
## err_ens_2007    1.0000000    0.9399344    0.6415089    0.9988782
## err_ens_2009    0.9399344    1.0000000    0.6191353    0.9455117
## err_ens_2011    0.6415089    0.6191353    1.0000000    0.6422570
## err_ens_2013    0.9988782    0.9455117    0.6422570    1.0000000
\end{verbatim}

\normalsize

The results are overall disappointing. Only one model manages to extract
patterns that are somewhat different from the other ones, resulting in a
70\% correlation across the board. One possible explanation could be
that the models capture mainly noise and little signal. Working with
long term labels like annual returns could help improve diversification
across models.

\hypertarget{exercise}{%
\section{Exercise}\label{exercise}}

Create an ensemble with XXXXXX models trained on XXXXXX data.

\hypertarget{backtest}{%
\chapter{Portfolio backtesting}\label{backtest}}

In this section, we introduce the notations and framework that will be
used when analysing and comparing investment strategies. Portfolio
backtesting is often conceived and perceived as a quest to find the best
strategy - or at least a solidly profitable one. When carried out
thoroughly, this possibly long endeavour may entice the layman to
confuse a fluke for a robust policy. Two papers published back-to-back
warn against the perils of data snooping.

\citet{fabozzi2018being} acknowledge that only strategies that work make
it to the public, while thousands (at least) have been tested. Picking
the pleasing outlier (the only strategy that seemed to work) is likely
to generate disappointment when switching to real life trading. In a
similar vein, \citet{arnott2019backtesting} provide a list of principles
and safeguards that any analyst should follow to avoid any type of error
when backtesting strategy. The worst type is arguably false positives
whereby strategies are found (often by cherrypicking) to outperform in
one very particular setting, but will likely fail in live trading.

In addition to these recommendations on portfolio constructions,
\citet{arnott2019alice} also warn against the perils of blindly
investing in smart beta products related to academic factors. Plainly,
expectations should not be set too high or face the risk of being
disappointed. Another takeaway from their article is that economic
cycles have a strong impact on factor returns: correlations change
quickly and drawdowns can be magnified in times of major downturns.

Backtesting is more complicated than it seems and it is easy to make
small mistakes that lead to \emph{apparently} good portfolio policies.
This chapter lays out a rigorous approach to this exercise, discusses a
few caveats, and proposes a lengthy example.

\hypertarget{protocol}{%
\section{Setting the protocol}\label{protocol}}

We consider a dataset with three dimensions: time \(t=1,\dots,T\),
assets \(n=1,\dots,N\) and characteristics \(k=1,\dots,K\). One of these
attributes must be the price of asset \(n\) at time \(t\), which we will
denote \(p_{t,n}\). From that, the computation of the arithmetic return
is straightforward (\(r_{t,n}=p_{t,n}/p_{t-1,n}-1\)) and so is any
heuristic measure of profitability. For simplicity, we assume that time
points are equidistant or uniform, i.e., that \(t\) is the index of a
trading day or of a month for example. If each point in time \(t\) has
data available for all assets, then this makes a dataset with
\(I=T\times N\) rows.

The dataset is first split in two: the out-of-sample period and the
initial buffer period. The buffer period is required to train the models
for the first portfolio composition. This period is determined by the
size of the training sample. There are two options for this size: fixed
(usually equal to 2 to 5 years) and expanding. In the first case, the
training sample will roll over time, taking into account only the most
recent data. In the second case, models are built on all of the
available data. This last option can create problems because the first
dates of the backtest are based on much smaller amounts of information
compared to the last dates. Moreover, there is an ongoing debate on
whether including the full history of returns and characteristics is
advantageous or not. Proponents argue that this allows models to see
many different market conditions. Opponents make the case that old data
is by definition outdated and thus useless and possibly misleading.

Henceforth, we choose the rolling period option for the training sample,
as depicted in Figure \ref{fig:backtestoos}.

\begin{figure}[H]

{\centering \includegraphics[width=450px]{images/backtestoos} 

}

\caption{Backtesting with rolling windows. The training set of the first period is simply the buffer period.}\label{fig:backtestoos}
\end{figure}

Two crucial design choices are the \textbf{rebalancing frequency} and
the \textbf{horizon} at which the label is computed. It is not obvious
that they should be equal but their choice should make sense. It can
seem right to train on a 12 month forward label (which capture longer
trends) and invest monthly or quarterly. However, it seems odd to do the
opposite and train on short term movements (monthly) and invest at a
long horizon.

These choices have a direct impact on how the backtest is carried out.
If we note:

\begin{itemize}
\tightlist
\item
  \(\Delta_h\) for the holding period between 2 rebalancing dates (in
  days or months);
\item
  \(\Delta_s\) for the size of the desired training sample (in days or
  months - not taking the number of assets into consideration);
\item
  \(\Delta_l\) for the horizon at which the label is computed (in days
  or months),
\end{itemize}

then the total length of the training sample should be
\(\Delta_s+\Delta_l\). Indeed, at any moment \(t\), the training sample
should stop at \(t-\Delta_l\) so that the last point corresponds to a
label that is calculated until time \(t\). This is highlighted in Figure
\ref{fig:backtestoos2} in the form of the red danger zone. We call it
the red zone because any feature which has a time index \(s\) inside the
interval \((t-\Delta_l,t]\) will engender a forward looking bias. Indeed
if a feature is indexed by \(s \in (t-\Delta_l,t]\), then by definition,
the label covers the period \([s,s+\Delta_l]\) with \(s+\Delta_l>t\). At
time \(t\), this requires knowledge of the future and is naturally not
realistic.

\begin{figure}[H]

{\centering \includegraphics[width=450px]{images/backtestoos2} 

}

\caption{The tricks in rolling training samples.}\label{fig:backtestoos2}
\end{figure}

\hypertarget{turning-signals-into-portfolio-weights}{%
\section{Turning signals into portfolio
weights}\label{turning-signals-into-portfolio-weights}}

The predictive tools outlined in Chapters \ref{lasso} to XXX are only
meant to provide a signal that is expected to provide some information
on the future profitability of assets. There are many ways that this
signal can be integrated in an investment decision.

First and foremost, there are at least two steps in the portfolio
construction process and the signal can be used at any of these stages.
Relying on the signal for both steps puts a lot of emphasis on the
predictions and should only be considered when the level of confidence
in the forecasts is high.

The first step is \textbf{selection}. While a forecasting exercise can
be carried out on a large number of assets, it is not compulsory to
invest in all of these assets. In fact, it would make sense to take
advantage of the signal to exclude those assets that are presumably
likely to underperform in the future. Often, portfolio policies have
fixed sizes that impose a constant number of assets. One heuristic way
to exploit the signal is to select the assets that have the most
favorable predictions and to discard the others. This naive idea is
often used in the asset pricing literature: portfolios are formed
according to underlying characteristics and this characteristic is
deemed interesting if the portfolios exhibit very different
profitabilities.

This is for instance an efficient way to test the relevance of the
signal. If \(Q\) portfolios \(q=1,\dots,Q\) are formed according to the
rankings of the assets with respect to the signal, then one would expect
that the out-of-sample performance of the portfolios be monotonic with
\(q\). While a rigorous test of monotonicity would require to account
for all portfolios (see, e.g., \citet{romano2013testing}), it is often
only assumed that the extreme portfolios suffice. If the difference
between portfolio number 1 and portfolio number \(Q\) is substantial,
then the signal is valuable. Whenever the investor is able to short
assets, this amounts to a dollar neutral strategy.

The second step is \textbf{weighting}. If the selection process relied
on the signal, then a simple weighting scheme is often a good idea.
Equally-weighted portfolios are known to be hard to beat (see
\citet{demiguel2007optimal}), especially compared to their cap-weighted
alternative, as is shown in \citet{plyakha2014equal}. More advanced
schemes include equal risk contributions
(\citet{maillard2010properties}) and constrained minimum variance
(\citet{coqueret2015diversified}). Both only rely on the covariance
matrix of the assets and not on any proxy for the vector of expected
returns.

For the sake of completeness, we explicit a generalization of
\citet{coqueret2015diversified} which is a generic constrained quadratic
program: \begin{equation}
\label{eq:coq}
\underset{\textbf{w}}{\text{min}} \ \frac{\lambda}{2} \textbf{w}'\boldsymbol{\Sigma}\textbf{w}-\textbf{w}'\boldsymbol{\mu} , \quad \text{s.t.} \quad \begin{array}{ll} \textbf{w}'\textbf{1}=1, \\ (\textbf{w}-\textbf{w}_-)'\boldsymbol{\Lambda}(\textbf{w}-\textbf{w}_-) \le \delta_R,\\
\textbf{w}'\textbf{w} \le \delta_D,
\end{array}
\end{equation}

where it is easy to recognize the usual mean-variance optimization in
the left-hand side. We impose three constraints on the right-hand side.
The first one is the budget constraint (weights sum to one). The second
one penalizes variations in weights (compared to the current allocation)
via a diagonal matrix \(\boldsymbol{\Lambda}\). This is a crucial point.
Portfolios are rarely constructed from scratch and are most of the time
adjustments from existing positions. In order to reduce the orders and
the corresponding transaction costs, it is possible to penalize large
variations from the existing portfolio. In the above program, the
current weights are written \(\textbf{w}_-\) and the desired ones
\(\textbf{w}\) so that \(\textbf{w}-\textbf{w}_-\) is the vector of
deviations from the current positions. The term
\((\textbf{w}-\textbf{w}_-)\boldsymbol{\Lambda}(\textbf{w}-\textbf{w}_-)\)
is an expression that characterizes the sum of squared deviations,
weighted by the diagonal coefficients \(\Lambda_{n,n}\). This can be
helpful because some assets may be more costly to trade due to liquidity
(large cap stocks are more liquid and their trading costs are lower).
When \(\delta_R\) decreases, the rotation is reduce because weights are
not allowed to deviate to much from \(\textbf{w}_-\). The last
constraint enforces diversification via the Herfindhal index of the
portfolio: the smaller \(\delta_D\), the more diversified the portfolio.

Recalling that there are \(N\) assets in the universe, the Lagrange form
of \eqref{eq:coq} is:

\begin{equation}
\label{eq:lagrangew}
L(\textbf{w})= \frac{\lambda}{2} \textbf{w}'\boldsymbol{\Sigma}\textbf{w}-\textbf{w}'\boldsymbol{\mu}-\eta (\textbf{w}'\textbf{1}_N-1)+\kappa_R ( (\textbf{w}-\textbf{w}_-)\boldsymbol{\Lambda}(\textbf{w}-\textbf{w}_-) - \delta_R)+\kappa_D(\textbf{w}'\textbf{w}-\delta_D),
\end{equation}

and the first order condition
\[\frac{\partial}{\partial \textbf{w}}L(\textbf{w})= \lambda \boldsymbol{\Sigma}\textbf{w}-\boldsymbol{\mu}-\eta\textbf{1}_N+2\kappa_R \boldsymbol{\Lambda}(\textbf{w}-\textbf{w}_-)+2\kappa_D\textbf{w}=0,\]
yields \begin{equation}
\label{eq:coqw}
\textbf{w}^*_\kappa=  (\lambda \boldsymbol{\Sigma}+2\kappa_R \boldsymbol{\Lambda} +2\kappa_D\textbf{I}_N)^{-1} \left(\boldsymbol{\mu} + \eta_{\lambda,\kappa_R,\kappa_D} \textbf{1}_N+2\kappa_R \boldsymbol{\Lambda}\textbf{w}_-\right),
\end{equation} with
\[\eta_{\lambda,\kappa_R,\kappa_D}=\frac{1- \textbf{1}_N'(\lambda\boldsymbol{\Sigma}+2\kappa_R \boldsymbol{\Lambda}+2\kappa_D\textbf{I}_N)^{-1}(\boldsymbol{\mu}+2\kappa_R\boldsymbol{\Lambda}\textbf{w}_-)}{\textbf{1}'_N(\lambda \boldsymbol{\Sigma}+2\kappa_R \boldsymbol{\Lambda}+2\kappa_D\textbf{I}_N)^{-1}\textbf{1}_N}.\]

This parameter ensures that the budget constraint is satisfied. The
optimal weights in \eqref{eq:coqw} depend on three tuning parameters:
\(\lambda\), \(\kappa_R\) and \(\kappa_D\). - When \(\lambda\) is large,
the focus is set more on risk reduction than one profit maximization
(which is often a good idea given that risk is easier to predict);\\
- When \(\kappa_R\) is large, the importance of transaction costs in
\eqref{eq:lagrangew} is high and thus, in the limit when
\(\kappa_R \rightarrow \infty\), the optimal weights are equal to the
old ones \(\textbf{w}_-\) (for finite values of the other parameters).\\
- When \(\kappa_D\) is large, the portfolio is more diversified and (all
other things equal) when \(\kappa_D \rightarrow \infty\), the weights
are all equal (to \(1/N\)).\\
- When \(\kappa_R=\kappa_D=0\), we recover the classical mean-variance
weights that are proportional to
\((\boldsymbol{\Sigma})^{-1} \boldsymbol{\mu}\).

This seemingly complex formula is in fact very flexible and tractable.
It requires some tests and adjustments before finding realistic values
for \(\lambda\), \(\kappa_R\) and \(\kappa_D\).

\hypertarget{perfmet}{%
\section{Performance metrics}\label{perfmet}}

The evaluation of performance is a key stage in a backtest. This
section, while not exhaustive, is intended to cover the most important
facets of portfolio assessment.

\hypertarget{discussion-1}{%
\subsection{Discussion}\label{discussion-1}}

While the evaluation of the accuracy of ML tools (See Section
\ref{mlmetrics}) is of course valuable, the portfolio returns are the
ultimate yardstick during a backtest. One essential element in such an
exercise is a \textbf{benchmark} because raw and absolute metrics don't
mean much one their own.

This is not only true at the portfolio level, but also at the ML engine
level. In most of the trials of the previous chapters, the MSE of the
models on the testing set revolves around 0.037. An interesting figure
is the variance of one month returns on this set, which corresponds to
the error made by a constant prediction of 0 all the time. This figure
is equal to 0.037, which means that the sophisticated algorithm don't
really improve on a naive heuristic. This benchmark is the one used in
the out-of-sample \(R^2\) of \citet{gu2018empirical}.

In portfolio choice, the most elementary allocation is the uniform one,
whereby each asset receive the same weight. This seemingly simplistic
solution is in fact an incredible benchmark, one that is hard to beat
consistently (see \citet{demiguel2007optimal} and
\citet{plyakha2014equal}). Below, we will pick this equally-weighted
portfolio as the benchmark.

\hypertarget{pure-performance-and-risk-indicators}{%
\subsection{Pure performance and risk
indicators}\label{pure-performance-and-risk-indicators}}

We then turn to the definition of the usual metrics used both by
practitioners and academics alike. Henceforth, we write
\(r^P=(r_t^P)_{1\le t\le T}\) and \(r^B=(r_t^B)_{1\le t\le T}\) for the
returns of the portfolio and those of the benchmark, respectively. When
refering to some generic returns, we simply write \(r_t\). There are
many ways to analyze them and most of them rely on their distribution.

The simplest indicator is the average return:
\[\mu_P=\mathbb{E}[r^P]\approx \frac{1}{T}\sum_{t=1}^T r_t^P, \quad \mu_B=\mathbb{E}[r^B]\approx \frac{1}{T}\sum_{t=1}^T r_t^B,\]

where, obviously, the portfolio is noteworthy if
\(\mathbb{E}[r^P]>\mathbb{E}[r^B]\). Note that we use the arithmetic
average above but the geometric one is also an option, in which case:
\[\tilde{\mu}_P\approx \left(\prod_{t=1}^T(1+r^P_t) \right)^{1/T}-1 , \quad \tilde{\mu}_B=\approx  \left(\prod_{t=1}^T(1+r^B_t) \right)^{1/T}-1.\]
The benefit of this second definition is that it takes the compounding
of returns into account and hence compensates for volatility pumping. To
see this, consider a very simple two period model with returns \(-r\)
and \(+r\). The arithmetic average is zero, but the geometric one
\(\sqrt(1-r^2)-1<0\) is negative.

Hit ratios are often referred to when the portfolio is built on a
\emph{good guess}. This can be evaluated at the asset level (the
proportion of positions in the correct direction\footnote{A long
  position in an asset with positive return or a short position in an
  asset with negative return.}) or a the portfolio level. In all cases,
the computation can be performed on raw returns or on relative returns
(e.g., compared to a benchmark). A meaningful hit ratio is the
proportion of times that a strategy beats its benchmark. This is of
course not sufficient, as many small wins can be offset by a few large
losses.

Pure performance measures are almost always accompanied by risk
measures. The second moment of returns is usually used to quantify the
magnitude of fluctuations of the portfolio. A large variance implies
sizable movements in returns, and hence in portfolio values. This is why
the standard deviation is called the volatility of the portfolio.
\[\sigma^2_P=\mathbb{V}[r^P]\approx \frac{1}{T-1}\sum_{t=1}^T (r_t^P-\mu_P)^2, \quad \sigma^2_B=\mathbb{V}[r^B]\approx \frac{1}{T-1}\sum_{t=1}^T (r_t^B-\mu_B)^2.\]

In this case, the portfolio can be preferred if it is less risky
compared to the benchmark, i.e., when \(\sigma_P^2<\sigma_B^2\).

Higher order moments of returns are sometimes used (skewness and
kurtosis), but they are far less common. We refer for instance to
\citet{harvey2010portfolio} for one method that takes them into account
in the portfolio construction process.

For some people, the volatility is an incomplete measure of risk. It can
be argued that it should be decomposed into `good' volatility (when
prices go up) versus `bad' volatility when they go down. The downward
semi-variance is computed as the variance taken over the negative
returns:
\[\sigma^2_-=\mathbb{V}[r]\approx \frac{1}{T-1}\sum_{t=1}^T (r_t-\mu_P)^21_{\{r_t<0\}}.\]

The average return and the volatility are the typical moment-based
metrics used by practitioners. Other indicators rely on different
aspects of the distribution of returns with a focus on tails and extreme
events. The Value-at-Risk (VaR) is one such example. If \(F_r\) is the
empirical cdf of returns, the VaR at a level of confidence \(\alpha\)
(often taken to be 95\%) is
\[\text{VaR}_\alpha(\textbf{r}_t)=F_r(1-\alpha).\]

It is equal to the worst case scenario if we omit the worst
\(1-\alpha\)\% returns. An even more conservative measure is the
so-called Conditional Value at Risk (CVaR), also known as expected
shortfall, which computes the average loss in the worst \(\alpha\)\%
scenarios. Its empirical evaluation is
\[\frac{1}{\text{Card}(r_t < \text{VaR}_\alpha(\text{r}_t))}\sum_{r_t < \text{VaR}_\alpha(\text{r}_t)}r_t.\]

Going crescendo in the severity of risk measures, the ultimate
evaluation of loss is the maximum drawdown. It is equal to the maximum
loss suffered from the peak value of the strategy If we write \(P_t\)
for the time-\(t\) value of a portfolio, the drawdown is
\[D_T^P=\underset{0 \le t \le T}{\text{max}} P_t-P_T ,\] and the maximum
drawdown is
\[MD_T^P=\underset{0 \le s \le T}{\text{max}} \left(\underset{0 \le t \le s}{\text{max}} P_t-P_s, 0\right) .\]

This quantity evaluates the greatest loss over the time frame \([0,T]\)
and is thus the most conservative risk measure of all.

\hypertarget{factor-based-evaluation}{%
\subsection{Factor-based evaluation}\label{factor-based-evaluation}}

In the spirit of factor models, performance can also be assessed through
the lens of exposures. If we recall the original formulation from
Equation \eqref{eq:apt}:
\[r_{t,n}= \alpha_n+\sum_{k=1}^K\beta_{t,k,n}f_{t,k}+\epsilon_{t,n}, \]

then the estimated \(\hat{\alpha}_n\) is the performance that cannot be
explained by the other factors. When returns are \emph{excess} returns
(over the risk-free rate) and when there is only one factor, the market
factor, then this quantity is called Jensen's alpha
(\citet{jensen1968performance}). Often, it is simply referred to as
\emph{alpha}. The other estimate, \(\hat{\beta}_{t,M,n}\) (\(M\) for
market), is the market beta.

Because of the rise of factor investing, it has become customary to also
report the alpha of more exhaustive regressions. Adding the size and
value premium (as in \citet{fama1993common}) and even momentum
(\citet{carhart1997persistence}) helps understand if a strategy
generates value beyond that that can be obtained through the usual
factors.

\hypertarget{risk-adjusted-measures}{%
\subsection{Risk-adjusted measures}\label{risk-adjusted-measures}}

Now, the tradeoff between the average return and the volatility is a
cornerstone in modern finance, since \citet{markowitz1952portfolio}. The
simplest way to synthesize both metrics is via the \textbf{information
ratio}: \[IR(P,B)=\frac{\mu_{P-B}}{\sigma_{P-B}},\] where the index
\(P-B\) implies that the mean and standard deviations are computed on
the long-short portfolio with returns \(r_t^P-r_t^B\). The denominator
\(\sigma_{P-B}\) is sometimes called the \textbf{tracking error}.

The most widespread information ratio is the Sharpe ratio
(\citet{sharpe1966mutual}) for which the benchmark is some riskless
asset. Instead of directly computing the information ratio between two
portfolios or strategies, it is often customary to compare their Sharpe
ratios. Simple comparisons can benefit from statistical tests (see e.g.,
\citet{ledoit2008robust}).

More extreme risk measures can serve as denominator in risk-adjusted
indicators. The Managed Account Report ratio (MAR ratio) is for example
computed as \[MAR^P = \frac{\tilde{\mu}_P}{MD^P},\] while the Treynor
ratio is equal to \[\text{Treynor}=\frac{\mu_P}{\hat{\beta}_M},\] i.e.,
the (excess) return divided by the market beta (see
\citet{treynor1965rate}). This definition was generalized to multifactor
expositions by \citet{hubner2005generalized} into the generalized
Treynor ratio:
\[\text{GT}=\mu_P\frac{\sum_{k=1}^K\bar{f}_k}{\sum_{k=1}^K\hat{\beta}_k\bar{f}_k},\]
where the \(\bar{f}_k\) are the sample average of the factors
\(f_{t,k}\). We refer to the original article for a detailed account of
the analytical properties of this ratio.

\hypertarget{transaction-costs-and-turnover}{%
\subsection{Transaction costs and
turnover}\label{transaction-costs-and-turnover}}

Updating portfolio composition is not free. In all generality, the total
cost of one rebalancing at time \(t\) is proportional to
\(C_t=\sum_{n=1}^N | \Delta w_{t,n}|c_{t,n}\), where \(\Delta w_{t,n}\)
is the change in position for asset \(n\) and \(c_{t,n}\) the
corresponding fee. This last quantity is often hard to predict, thus it
is customary to use a proxy that depends for instance on market
capitalization (large stocks have more liquid shares and thus require
smaller fees) or bid-ask spreads (smaller spreads mean smaller fees).

As a first order approximation, it is often useful to compute the
average turnover:
\[\text{Turnover}=\frac{1}{T-1}\sum_{t=2}^T\sum_{n=1}^N|w_{t,n}-w_{t-,n}|,\]
where \(w_{t,n}\) are the desired \(t\)-time weights in the portfolio
and \(w_{t-,n}\) are the weights just before the rebalancing. The
positions of the first period (lauching weights) are exluded from the
computation by convention. Transaction costs can then be proxied as a
multiple of turnover (times some average or median cost in the
cross-section of firms). This is a first order estimate of realized
costs that does not take into consideration the evolution of the scale
of the portfolio. Nonetheless, a rough figure is much better than none
at all.

Once transaction costs (TC) have been annualized, they can be deducted
from average returns to yield a more realistic picture of profitability.
In the same vein, the transaction cost-adjusted Sharpe ratio of a
portfolio \(P\) is given by \begin{equation}
\label{eq:SRTC}
SR_{TC}=\frac{\mu_P-TC}{\sigma_P}.
\end{equation}

Transaction costs are often overlooked in academic articles but can have
a sizable impact in real life trading (see e.g.,
\citet{novy2015taxonomy}). \citet{martin2018transaction} show how to use
factor investing (and exposures) to combine and offset positions and
reduce overall fees.

\hypertarget{common-errors-and-issues}{%
\section{Common errors and issues}\label{common-errors-and-issues}}

\hypertarget{forward-looking-data}{%
\subsection{Forward looking data}\label{forward-looking-data}}

One of the most common mistakes in portfolio backtesting is to use
forward looking data. It is for instance easy to fall in the trap of the
danger zone depicted in Figure \ref{fig:backtestoos2}. In this case, the
labels used at time \(t\) are computed with knowledge of what happens at
times \(t+1\), \(t+2\), etc. It is worth triple checking every step in
the code to make sure that strategies are not built on prescient data.

\hypertarget{backtest-overfitting}{%
\subsection{Backtest overfitting}\label{backtest-overfitting}}

The second major problem is backtest overfitting. The analogy with
training set overfitting is easy to grasp. It is a well known issue and
was formalized for instance in \citet{white2000reality} and
\citet{romano2005stepwise}. In portfolio choice, we refer to
\citet{bajgrowicz2012technical} and \citet{bailey2014deflated} and the
references therein.

At any given moment, a backtest depends on one particular dataset.
Often, the result of the first backtest will not be satisfactory - for
many possible reasons. Hence, it is tempting to have another try, when
altering some parameters that were probably not optimal. This second
test may be better, but not quite good enough - yet. Thus, a in a third
trial, a new weighting scheme can be tested, along with a new
forecasting engine (more sophisticated). Iteratively, the backtester can
only end up with a strategy that performs well enough, it is just a
matter of time and trials.

One consequence of backtest overfitting is that it is illusory to hope
for the same Sharpe ratios in live trading as those obtained in the
backtest. Reasonable professionals divide the Sharpe ratio by two at
least (\citet{harvey2015backtesting}, \citet{suhonen2017quantifying}).
In \citet{bailey2014deflated}, the authors even propose a statistical
test for Sharpe ratios, provided that all metrics of all tested
strategies are stored in memory. The formula is: \begin{equation}
\label{eq:tSR}
t = \phi\left((SR-SR^*)\sqrt{\frac{T-1}{1-\gamma_3SR+\frac{\gamma_4-1}{4}SR^2}} \right),
\end{equation} where \(SR\) is the Sharpe Ratio obtained by the best
strategy, and
\[SR^*=\mathbb{E}[SR]+\sqrt{\mathbb{V}[SR]}\left((1-\gamma)\phi^{-1}\left(1-\frac{1}{N}\right)+\gamma \phi^{-1}\left(1-\frac{1}{Ne}\right)  \right),\]
is the theoretical average maximum SR. Moreover,

\begin{itemize}
\item $T$ is the number of trading dates;
\item $\gamma_3$ and $\gamma_4$ are the $skewness$ and $kurtosis$ of the returns of the chosen strategy;
\item $\phi$ is the cdf of the standard Gaussian law and $\gamma$ is the Euler-Mascheroni constant; 
\item $N$ refers to the number of strategy trials. 
\end{itemize}

If \(t\) defined above is below a certain threshold (e.g., 0.95), then
the \(SR\) cannot be deemed significant:
\textbf{the best strategy is not outstanding}. Most of the time, sadly,
that is the case. In Equation \eqref{eq:tSR}, the realized SR must be
above the theoretical maximum \(SR^*\) and the scaling factor must be
sufficiently large to push the argument inside \(\phi\) close enough to
two.

In the scientific community, test overfitting is also known as
\emph{p}-hacking. It is rather common in financial economics and the
reading of \citet{harvey2017presidential} is strongly advised to grasp
the magnitude of the phenomenon. \emph{p}-hacking is also present in
most fields that use statistical tests (see, e.g.,
\citet{head2015extent} to cite but one reference). There are several
ways to cope with \emph{p}-hacking:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  don't rely on \emph{p}-values (\citet{amrhein2019scientists});\\
\item
  use detection tools (\citet{elliott2019detecting});\\
\item
  or, finally, use advanced methods that process arrays of statistics
  (e.g., the Bayesianized versions of \emph{p}-values to include some
  prior assessment from \citet{harvey2017presidential}, or other tests
  such as those proposed in \citet{romano2005stepwise} and
  \citet{simonsohn2014p}).
\end{enumerate}

The first option is wise, but the drawback is that the decision process
is then left to another arbitrary yardstick.

\hypertarget{simple-saveguards}{%
\subsection{Simple saveguards}\label{simple-saveguards}}

As is mentioned at the beginning of the chapter, two common sense
references for backtesting are \citet{fabozzi2018being} and
\citet{arnott2019backtesting}. The pieces of advice provide in these two
articles are often judicious and thoughtful.

One additional comment pertains to the output of the backtest. One
simple, intuitive and widespread metric is the transaction cost-adjusted
Sharpe ratio defined in Equation \eqref{eq:SRTC}. In the backtest, let us
call \(SR_{TC}^B\) the corresponding value for the benchmark, which we
like to define as the equally-weighted portfolio of all assets in the
trading universe (in our dataset, roughly one thousand US equities). If
the \(SR_{TC}^P\) of the best strategy is above \(2\times SR_{TC}^B\),
then there is probably a glitch somewhere in the backtest.

This criterion holds under two assumptions: 1) a sufficiently long
enough out-of-sample period and 2) long-only portfolios. It is unlikely
that any realistic strategy can outperform a solid benchmark by a very
wide margin over the long term. Being able to improve the benchmark's
annualized return by 150 basis points (with comparable volatility) is
already a great achievement. Backtests that deliver returns 5\% above
those of the benchmark are dubious.

\hypertarget{implication-of-non-stationarity-forecasting-is-hard}{%
\section{Implication of non-stationarity: forecasting is
hard}\label{implication-of-non-stationarity-forecasting-is-hard}}

This subsection is split into two parts: in the first, we discuss the
reason that make forecasting such a difficult task and in the second we
present an important theoretical result originally developed towards
machine learning but that sheds light on any discipline confronted with
out-of-sample tests.

\hypertarget{general-comments}{%
\subsection{General comments}\label{general-comments}}

The careful reader must have noticed that throughout Chapters
\ref{lasso} to \ref{ensemble}, the performance of ML engines is
underwhelming. These disappointing results are there on purpose and
highlight the crucial truth that machine learning is no panacea, no
magic wand, no philosopher's stone that can transform data into golden
predictions. Most ML-based forecasts fail. This is in fact not only true
for very enhanced and sophisticated techniques, but also for simpler
econometric approaches (\citet{dichtl2019data}), which again underlines
the need to replicate results to challenge their validity.

One reason for that is that datasets are full of noise and extracting
the slightest amount of signal is a tough challenge (we recommend a
careful reading of the introduction of \citet{timmermann2018forecasting}
for more details on this topic). One rationale for that is the ever
time-varying nature of factor analysis in the equity space. Some factors
can perform very well during one year and then poorly the next year and
these reversals can be costly in the context of fully automated
data-based allocation processes.

In fact, this is one major difference with many fields for which ML has
made huge advances. In image recognition, numbers will always have the
same shape, and so will cats, buses, etc. Likewise, a verb will always
be a verb and syntaxes in languages do not change. This invariance,
though sometimes hard to grasp\footnote{We invite the reader to have a
  look at the thoughtful albeit theoretical paper by
  \citet{arjovsky2019invariant}.} is nonetheless key to the great
improvement both in computer vision and natural language processing.

In factor investing, there does not seem to be such invariance. There is
no factor and no (possibly nonlinear) combination of factors that can
explain and accurately forecast returns over decades long
periods.\footnote{In the thread
  \url{https://twitter.com/fchollet/status/1177633367472259072},
  François Chollet, the creator of Keras argues that ML predictions
  based on price data cannot be profitable on the long term. Given the
  wide access to financial data, it is likely that the statement holds
  for predictions stemming from factor-related data as well.} The
academic literature has yet to find such a model; but even if it did, a
simple arbitrage reasoning would invalidate its conclusions in future
datasets.

One key ingredient in the backtest of a dynamic strategy is the depth of
the sample size, i.e., how far back in time do each training sets go.
There should be some coherence between: - the trading horizon determined
by the rebalancing frequency;\\
- the training sample depth (number of years or months in the training
set);\\
- the time window on which the label is computed.

There is probably no perfect combination, but there are some odd
choices. Short-term labels and long trading horizons will plausibly not
match very well.

Online learning \& Active learning XXXXXXXXXXX \citet{settles2009active}

\hypertarget{the-no-free-lunch-theorem}{%
\subsection{The no free lunch theorem}\label{the-no-free-lunch-theorem}}

\hypertarget{example-1}{%
\section{Example}\label{example-1}}

We finally propose a full detailed example of one implementation of a
ML-based strategy run on a careful backtest. What follows is a
generalization of the content of Section \ref{sparseex}. In the same
spirit, we split the backtest in four parts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the creation/initialization of variables;\\
\item
  the definition of the strategies in one main functions;
\item
  the backtesting loop itself;\\
\item
  the performance indicators.
\end{enumerate}

Accordingly, we start with initializations.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sep_oos <-}\StringTok{ }\KeywordTok{as.Date}\NormalTok{(}\StringTok{"2007-01-01"}\NormalTok{)                            }\CommentTok{# Starting point for backtest}
\NormalTok{ticks <-}\StringTok{ }\NormalTok{data_ml}\OperatorTok{$}\NormalTok{stock_id }\OperatorTok{%>%}\StringTok{                               }\CommentTok{# List of all asset ids}
\StringTok{    }\KeywordTok{as.factor}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{levels}\NormalTok{()}
\NormalTok{N <-}\StringTok{ }\KeywordTok{length}\NormalTok{(ticks)                                          }\CommentTok{# Max number of assets}
\NormalTok{t_oos <-}\StringTok{ }\NormalTok{returns}\OperatorTok{$}\NormalTok{date[returns}\OperatorTok{$}\NormalTok{date }\OperatorTok{>}\StringTok{ }\NormalTok{sep_oos] }\OperatorTok{%>%}\StringTok{           }\CommentTok{# Out-of-sample dates }
\StringTok{    }\KeywordTok{unique}\NormalTok{() }\OperatorTok{%>%}\StringTok{                                            }\CommentTok{# Remove duplicates}
\StringTok{    }\KeywordTok{as.Date}\NormalTok{(}\DataTypeTok{origin =} \StringTok{"1970-01-01"}\NormalTok{)                          }\CommentTok{# Transform in date format}
\NormalTok{Tt <-}\StringTok{ }\KeywordTok{length}\NormalTok{(t_oos)                                         }\CommentTok{# Nb of dates, avoid T = TRUE}
\NormalTok{nb_port <-}\StringTok{ }\DecValTok{2}                                                \CommentTok{# Nb of portfolios/stragegies}
\NormalTok{portf_weights <-}\StringTok{ }\KeywordTok{array}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{dim =} \KeywordTok{c}\NormalTok{(Tt, nb_port, N))          }\CommentTok{# Initialize portfolio weights}
\NormalTok{portf_returns <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DataTypeTok{nrow =}\NormalTok{ Tt, }\DataTypeTok{ncol =}\NormalTok{ nb_port)       }\CommentTok{# Initialize portfolio returns }
\end{Highlighting}
\end{Shaded}

\normalsize

This first step is crucial, it lays the groundwork for the core of the
backtest. We consider only two strategies: one ML-based and the EW (1/N)
benchmark. The main (weighting) function will consist of these two
components, but we define the sophisticated one in a dedicated wrapper.
The ML-based weights are derived from XGBoost predictions with 80 trees,
a learning rate of 0.3 and a maximum tree depth of 4. This makes the
model complex but not exceedingly so. Once the predictions are obtained,
the weighting scheme is simple: it is an EW portfolio over the best half
of the stocks (those with above median prediction).

In the function below, all parameters (e.g., the learning rate) are
hard-coded. They can easily be passed in arguments next to the data
inputs. One very important detail is that in contrast to the rest of the
book, the label is the 12 month future return. The main reason for this
is rooted in the discussion from Section \ref{pers}.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{weights_xgb <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(train_data, test_data, features)\{ }
\NormalTok{    train_features <-}\StringTok{ }\NormalTok{train_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(features) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{()         }\CommentTok{# Indep. variable}
\NormalTok{    train_label <-}\StringTok{ }\NormalTok{train_data}\OperatorTok{$}\NormalTok{R12M_Usd }\OperatorTok{/}\StringTok{ }\KeywordTok{exp}\NormalTok{(train_data}\OperatorTok{$}\NormalTok{Vol1Y_Usd)            }\CommentTok{# Dependent variable}
\NormalTok{    train_matrix <-}\StringTok{ }\KeywordTok{xgb.DMatrix}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ train_features, }\DataTypeTok{label =}\NormalTok{ train_label)   }\CommentTok{# XGB format}
\NormalTok{    fit <-}\StringTok{ }\NormalTok{train_matrix }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{xgb.train}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ .,                       }\CommentTok{# Data source (pipe input)}
                  \DataTypeTok{eta =} \FloatTok{0.3}\NormalTok{,                      }\CommentTok{# Learning rate}
                  \DataTypeTok{objective =} \StringTok{"reg:linear"}\NormalTok{,       }\CommentTok{# Number of random trees}
                  \DataTypeTok{max_depth =} \DecValTok{4}\NormalTok{,                  }\CommentTok{# Maximum depth of trees}
                  \DataTypeTok{nrounds =} \DecValTok{80}                    \CommentTok{# Number of trees used}
\NormalTok{        )}
\NormalTok{    xgb_test <-}\StringTok{ }\NormalTok{test_data }\OperatorTok{%>%}\StringTok{                     }\CommentTok{# Test sample => XGB format}
\StringTok{        }\KeywordTok{select}\NormalTok{(features) }\OperatorTok{%>%}\StringTok{ }
\StringTok{        }\KeywordTok{as.matrix}\NormalTok{() }\OperatorTok{%>%}
\StringTok{        }\KeywordTok{xgb.DMatrix}\NormalTok{()}
    
\NormalTok{    pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit, xgb_test)                }\CommentTok{# Single prediction}
\NormalTok{    w <-}\StringTok{ }\NormalTok{pred }\OperatorTok{>}\StringTok{ }\KeywordTok{median}\NormalTok{(pred)                      }\CommentTok{# Keep only the 50% best predictions}
\NormalTok{    w}\OperatorTok{$}\NormalTok{weights <-}\StringTok{ }\NormalTok{w }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(w)}
\NormalTok{    w}\OperatorTok{$}\NormalTok{names <-}\StringTok{ }\KeywordTok{unique}\NormalTok{(test_data}\OperatorTok{$}\NormalTok{stock_id)}
    \KeywordTok{return}\NormalTok{(w)                                     }\CommentTok{# Best predictions, equally-weighted}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\normalsize

Compared to the structure proposed in Section \ref{boostcode}, the
differences are that the label is not only based on \emph{long-term}
returns, but it also relies on a volatility component. Even though the
denominator in the label is the exponential a quantile of the
volatility, it seems fair to say that it is inspired by the Sharpe ratio
and that the model seeks to explain and forecast a risk-adjusted return
instead of a \emph{raw} return. A stock with very low volatility will
have its return unchanged in the label, while a stock with very high
volatility will see its return divided by a factor close to three
(exp(1)=2.718).

This function is then embedded in the global weighting function which
only wraps two schemes: the EW benchmark and the ML-based policy.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{portf_compo <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(train_data, test_data, features, j)\{ }
    \ControlFlowTok{if}\NormalTok{(j }\OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{)\{                                 }\CommentTok{# This is the benchmark}
\NormalTok{        N <-}\StringTok{ }\NormalTok{test_data}\OperatorTok{$}\NormalTok{stock_id }\OperatorTok{%>%}\StringTok{             }\CommentTok{# Test data dictates allocation}
\StringTok{            }\KeywordTok{factor}\NormalTok{() }\OperatorTok{%>%}\StringTok{ }\KeywordTok{nlevels}\NormalTok{()}
\NormalTok{        w <-}\StringTok{ }\DecValTok{1}\OperatorTok{/}\NormalTok{N                                }\CommentTok{# EW portfolio}
\NormalTok{        w}\OperatorTok{$}\NormalTok{weights <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(w,N)}
\NormalTok{        w}\OperatorTok{$}\NormalTok{names <-}\StringTok{ }\KeywordTok{unique}\NormalTok{(test_data}\OperatorTok{$}\NormalTok{stock_id)   }\CommentTok{# Asset names}
        \KeywordTok{return}\NormalTok{(w)}
\NormalTok{    \}}
    \ControlFlowTok{if}\NormalTok{(j }\OperatorTok{==}\StringTok{ }\DecValTok{2}\NormalTok{)\{                                 }\CommentTok{# This is the ML strategy.}
        \KeywordTok{return}\NormalTok{(}\KeywordTok{weights_xgb}\NormalTok{(train_data, test_data, features))}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\normalsize

Equipped with this function, we can turn to the main backtesting loop.
Given the fact that we use a large scale model, the computation time for
the loop is large (possibly a few hours on a slow machine).

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m_offset <-}\StringTok{ }\DecValTok{12}                                          \CommentTok{# Offset in months for buffer period}
\NormalTok{train_size <-}\StringTok{ }\DecValTok{5}                                         \CommentTok{# Size of training set in years}
\ControlFlowTok{for}\NormalTok{(t }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{(}\KeywordTok{length}\NormalTok{(t_oos)}\OperatorTok{-}\DecValTok{1}\NormalTok{))\{                          }\CommentTok{# Stop before the last date: no fwd return!}
    \ControlFlowTok{if}\NormalTok{(t}\OperatorTok{%%}\DecValTok{12}\OperatorTok{==}\DecValTok{0}\NormalTok{)\{}\KeywordTok{print}\NormalTok{(t_oos[t])\}                       }\CommentTok{# Just checking the date status}
\NormalTok{    train_data <-}\StringTok{ }\NormalTok{data_ml }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(date }\OperatorTok{<}\StringTok{ }\NormalTok{t_oos[t] }\OperatorTok{-}\StringTok{ }\NormalTok{m_offset }\OperatorTok{*}\StringTok{ }\DecValTok{30}\NormalTok{,   }\CommentTok{# Rolling window with buffer}
\NormalTok{                                    date }\OperatorTok{>}\StringTok{ }\NormalTok{t_oos[t] }\OperatorTok{-}\StringTok{ }\NormalTok{m_offset }\OperatorTok{*}\StringTok{ }\DecValTok{30} \OperatorTok{-}\StringTok{ }\DecValTok{365} \OperatorTok{*}\StringTok{ }\NormalTok{train_size)    }
\NormalTok{    test_data <-}\StringTok{ }\NormalTok{data_ml }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(date }\OperatorTok{==}\StringTok{ }\NormalTok{t_oos[t],                   }\CommentTok{# Current values}
\NormalTok{                                    stock_id }\OperatorTok{%in%}\StringTok{ }\NormalTok{train_data}\OperatorTok{$}\NormalTok{stock_id)  }\CommentTok{# Common with past info!  }
\NormalTok{    realized_returns <-}\StringTok{ }\NormalTok{test_data }\OperatorTok{%>%}\StringTok{                   }\CommentTok{# Computing returns via:}
\StringTok{        }\KeywordTok{select}\NormalTok{(R1M_Usd)                                 }\CommentTok{# 1M holding holding period!}
\NormalTok{    realized_names <-}\StringTok{ }\NormalTok{test_data }\OperatorTok{%>%}\StringTok{                     }\CommentTok{# Names of assets with holding period returns}
\StringTok{        }\KeywordTok{select}\NormalTok{(stock_id) }\OperatorTok{%>%}
\StringTok{        }\KeywordTok{as.matrix}\NormalTok{() }
    \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{nb_port)\{    }
\NormalTok{        temp_weights <-}\StringTok{ }\KeywordTok{portf_compo}\NormalTok{(train_data, test_data, features, j) }\CommentTok{# Hard-coded params, beware!}
\NormalTok{        ind1 <-}\StringTok{ }\KeywordTok{match}\NormalTok{(temp_weights}\OperatorTok{$}\NormalTok{names, realized_names) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{na.omit}\NormalTok{() }\CommentTok{# Index between train & test }
\NormalTok{        ind2 <-}\StringTok{ }\KeywordTok{match}\NormalTok{(ticks, realized_names) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{na.omit}\NormalTok{()              }\CommentTok{# Index between all & test}
\NormalTok{        portf_weights[t,j,ind2] <-}\StringTok{ }\NormalTok{temp_weights}\OperatorTok{$}\NormalTok{weights[ind1]           }\CommentTok{# Locating the weights correctly}
\NormalTok{        portf_returns[t,j] <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(temp_weights}\OperatorTok{$}\NormalTok{weights[ind1] }\OperatorTok{*}\StringTok{ }\NormalTok{realized_returns) }\CommentTok{# Compute returns}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "2007-12-31"
## [1] "2008-12-31"
## [1] "2009-12-31"
## [1] "2010-12-31"
## [1] "2011-12-31"
## [1] "2012-12-31"
## [1] "2013-12-31"
## [1] "2014-12-31"
## [1] "2015-12-31"
## [1] "2016-12-31"
## [1] "2017-12-31"
\end{verbatim}

\normalsize

There are two important comments to be made on the above code. The first
comment pertains to the two parameters that are defined in the first
lines. They refer to the size of the training sample (5 years) and the
length of the buffer period shown in Figure \ref{fig:backtestoos2}. This
\textbf{buffer period is imperative} because the label is based on a
long-term (12 month) return. This lag is compulsory to avoid any forward
looking bias in the backtest.

Below, we create a function that compute the turnover (variation in
weights). It requires both the weight values as well as the returns of
all assets because the weights just before a rebalancing depend on the
weights assigned in the previous period as well as on the returns of the
assets that have altered these original weights.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{turnover <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(weights, asset_returns, t_oos)\{ }
\NormalTok{    turn <-}\StringTok{ }\DecValTok{0}
    \ControlFlowTok{for}\NormalTok{(t }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\KeywordTok{length}\NormalTok{(t_oos))\{}
\NormalTok{        realised_returns <-}\StringTok{ }\NormalTok{returns }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(date }\OperatorTok{==}\StringTok{ }\NormalTok{t_oos[t]) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{date)}
\NormalTok{        prior_weights <-}\StringTok{ }\NormalTok{weights[t}\DecValTok{-1}\NormalTok{,] }\OperatorTok{*}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{realised_returns) }\CommentTok{# Before rebalancing}
\NormalTok{        turn <-}\StringTok{ }\NormalTok{turn }\OperatorTok{+}\StringTok{ }\KeywordTok{apply}\NormalTok{(}\KeywordTok{abs}\NormalTok{(weights[t,] }\OperatorTok{-}\StringTok{ }\NormalTok{prior_weights}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(prior_weights)),}\DecValTok{1}\NormalTok{,sum)}
\NormalTok{    \}}
    \KeywordTok{return}\NormalTok{(turn}\OperatorTok{/}\NormalTok{(}\KeywordTok{length}\NormalTok{(t_oos)}\OperatorTok{-}\DecValTok{1}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\normalsize

Once turnover is defined, we embed it into a function that computes
several key indicators.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{perf_met <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(portf_returns, weights, asset_returns, t_oos)\{ }
\NormalTok{    avg_ret <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(portf_returns, }\DataTypeTok{na.rm =}\NormalTok{ T)                     }\CommentTok{# Arithmetic mean }
\NormalTok{    vol <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(portf_returns, }\DataTypeTok{na.rm =}\NormalTok{ T)                           }\CommentTok{# Volatility}
\NormalTok{    Sharpe_ratio <-}\StringTok{ }\NormalTok{avg_ret }\OperatorTok{/}\StringTok{ }\NormalTok{vol                                 }\CommentTok{# Sharpe ratio}
\NormalTok{    VaR_}\DecValTok{5}\NormalTok{ <-}\StringTok{ }\KeywordTok{quantile}\NormalTok{(portf_returns, }\FloatTok{0.05}\NormalTok{)                        }\CommentTok{# Value-at-risk}
\NormalTok{    turn <-}\StringTok{ }\DecValTok{0}                                                     \CommentTok{# Initialisation of turnover}
    \ControlFlowTok{for}\NormalTok{(t }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\KeywordTok{dim}\NormalTok{(weights)[}\DecValTok{1}\NormalTok{])\{}
\NormalTok{        realized_returns <-}\StringTok{ }\NormalTok{asset_returns }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(date }\OperatorTok{==}\StringTok{ }\NormalTok{t_oos[t]) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{date)}
\NormalTok{        prior_weights <-}\StringTok{ }\NormalTok{weights[t}\DecValTok{-1}\NormalTok{,] }\OperatorTok{*}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{realized_returns)}
\NormalTok{        turn <-}\StringTok{ }\NormalTok{turn }\OperatorTok{+}\StringTok{ }\KeywordTok{apply}\NormalTok{(}\KeywordTok{abs}\NormalTok{(weights[t,] }\OperatorTok{-}\StringTok{ }\NormalTok{prior_weights}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(prior_weights)),}\DecValTok{1}\NormalTok{,sum)}
\NormalTok{    \}}
\NormalTok{    turn <-}\StringTok{ }\NormalTok{turn}\OperatorTok{/}\NormalTok{(}\KeywordTok{length}\NormalTok{(t_oos)}\OperatorTok{-}\DecValTok{1}\NormalTok{)                                }\CommentTok{# Average over time}
\NormalTok{    met <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(avg_ret, vol, Sharpe_ratio, VaR_}\DecValTok{5}\NormalTok{, turn)    }\CommentTok{# Aggregation of all of this}
    \KeywordTok{rownames}\NormalTok{(met) <-}\StringTok{ "metrics"}
    \KeywordTok{return}\NormalTok{(met)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\normalsize

Lastly, we build a function that loops on the various strategies.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{perf_met_multi <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(portf_returns, weights, asset_returns, t_oos, strat_name)\{}
\NormalTok{    J <-}\StringTok{ }\KeywordTok{dim}\NormalTok{(weights)[}\DecValTok{2}\NormalTok{]              }\CommentTok{# Number of strategies }
\NormalTok{    met <-}\StringTok{ }\KeywordTok{c}\NormalTok{()                        }\CommentTok{# Initialization of metrics}
    \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{J)\{                    }\CommentTok{# One very ugly loop}
\NormalTok{        temp_met <-}\StringTok{ }\KeywordTok{perf_met}\NormalTok{(portf_returns[, j], weights[, j, ], asset_returns, t_oos)}
\NormalTok{        met <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(met, temp_met)}
\NormalTok{    \}}
    \KeywordTok{row.names}\NormalTok{(met) <-}\StringTok{ }\NormalTok{strat_name      }\CommentTok{# Stores the name of the strat}
    \KeywordTok{return}\NormalTok{(met)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\normalsize

Given the weights and returns of the portfolios, it remains to compute
the returns of the assets to plug them in the aggregate metrics
function.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{asset_returns <-}\StringTok{ }\NormalTok{data_ml }\OperatorTok{%>%}\StringTok{                          }\CommentTok{# Compute return matrix: start from original data}
\StringTok{    }\KeywordTok{select}\NormalTok{(date, stock_id, R1M_Usd) }\OperatorTok{%>%}\StringTok{               }\CommentTok{# Keep 3 attributes }
\StringTok{    }\KeywordTok{spread}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ stock_id, }\DataTypeTok{value =}\NormalTok{ R1M_Usd)           }\CommentTok{# Shape in matrix format}
\NormalTok{asset_returns[}\KeywordTok{is.na}\NormalTok{(asset_returns)] <-}\StringTok{ }\DecValTok{0}              \CommentTok{# Zero returns for missing points}

\NormalTok{met <-}\StringTok{ }\KeywordTok{perf_met_multi}\NormalTok{(}\DataTypeTok{portf_returns =}\NormalTok{ portf_returns,  }\CommentTok{# Computes performance metrics}
                      \DataTypeTok{weights =}\NormalTok{ portf_weights, }
                      \DataTypeTok{asset_returns =}\NormalTok{ asset_returns,}
                      \DataTypeTok{t_oos =}\NormalTok{ t_oos,}
                      \DataTypeTok{strat_name =} \KeywordTok{c}\NormalTok{(}\StringTok{"EW"}\NormalTok{, }\StringTok{"XGB_SR"}\NormalTok{))}
\NormalTok{met                                                   }\CommentTok{# Displays perf metrics}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            avg_ret        vol Sharpe_ratio       VaR_5      turn
## EW     0.009699314 0.05643112    0.1718788 -0.07712509 0.0711028
## XGB_SR 0.012271508 0.06232806    0.1968858 -0.08236267 0.6589811
\end{verbatim}

\normalsize

The ML-based strategy performs finally well! The gain is mostly obtained
by the average return, while the volatility is higher than that of the
benchmark. The net effect is that the Sharpe ratio is improved compared
to the benchmark. The augmentation is not breathtaking, but (hence?) it
seems reasonable. It is noteworthy to underline that turnover is
substantially higher for the sophisticated strategy. Removing costs in
the numerator (say, 0.005 times the turnover as in
\citet{goto2015improving}) only mildly reduces the superiority in Sharpe
ratio of the ML-based strategy.

Finally, it is always tempting to plot the corresponding portfolio
values.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(lubridate) }\CommentTok{# Date management}
\KeywordTok{library}\NormalTok{(cowplot)   }\CommentTok{# Plot grid management}
\NormalTok{g1 <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{date =}\NormalTok{ t_oos,  }
      \DataTypeTok{benchmark =} \KeywordTok{cumprod}\NormalTok{(}\DecValTok{1}\OperatorTok{+}\NormalTok{portf_returns[,}\DecValTok{1}\NormalTok{]),}
      \DataTypeTok{ml_based =} \KeywordTok{cumprod}\NormalTok{(}\DecValTok{1}\OperatorTok{+}\NormalTok{portf_returns[,}\DecValTok{2}\NormalTok{])) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{gather}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ strat, }\DataTypeTok{value =}\NormalTok{ value, }\OperatorTok{-}\NormalTok{date) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ date, }\DataTypeTok{y =}\NormalTok{ value, }\DataTypeTok{color =}\NormalTok{ strat)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}\KeywordTok{theme_grey}\NormalTok{()}
\NormalTok{g2 <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{year =}\NormalTok{ lubridate}\OperatorTok{::}\KeywordTok{year}\NormalTok{(t_oos),  }
             \DataTypeTok{benchmark =}\NormalTok{ portf_returns[,}\DecValTok{1}\NormalTok{],}
             \DataTypeTok{ml_based =}\NormalTok{ portf_returns[,}\DecValTok{2}\NormalTok{]) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{gather}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ strat, }\DataTypeTok{value =}\NormalTok{ value, }\OperatorTok{-}\NormalTok{year) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(year, strat) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{avg_return =} \KeywordTok{mean}\NormalTok{(value)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ year, }\DataTypeTok{y =}\NormalTok{ avg_return, }\DataTypeTok{fill =}\NormalTok{ strat)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_grey}\NormalTok{()}
\KeywordTok{plot_grid}\NormalTok{(g1,g2, }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=350px]{ML_factor_files/figure-latex/backtest6-1} \end{center}

\normalsize

Out of the 12 years of the backtest, the advanced strategy outperforms
the benchmark during 10 years. It is less hurtful in all three years of
aggregate losses (2008, 2015 and 2018). This is a satisfactory
improvement because the EW benchmark is tough to beat!

\hypertarget{coding-exercises-6}{%
\section{Coding exercises}\label{coding-exercises-6}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Code the advanced weighting function defined in Equation
  \eqref{eq:coqw}.
\item
  Test it in a small backtest and check its sensitivity to the
  parameters.
\item
  Using the functional programming package \emph{purrr},
\end{enumerate}

\hypertarget{interp}{%
\chapter{Interpretability}\label{interp}}

This chapter is dedicated to the techniques that help understand the way
models process inputs into outputs. A recent book
(\citet{molnar2019interpretable} available at
\url{https://christophm.github.io/interpretable-ml-book/}) is entirely
devoted to this topic and we highly recommend to have a look at it.
Another more introductory and less technical reference is
\citet{hall2019introduction}. Obviously, in this chapter, we will adopt
an tone which is factor-investing orientated and discuss examples
related to ML models trained on a financial dataset.

Quantitative tools that aim for interpretability of ML models are
required to satisfy two simple conditions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  That they provide information about the model.\\
\item
  That they are highly comprehensible.
\end{enumerate}

Often, these tools generate graphical outputs which are easy to read and
yield immediate conclusions.

In attempts to white-box complex machine learning models, one dichotomy
stands out:

\begin{itemize}
\tightlist
\item
  \textbf{Global models} seek to determine the relative role of features
  in the construction of the prediction once the model has been trained.
  This is done at the global level, so that the output hold \emph{on
  average} over the whole training set.\\
\item
  \textbf{Local models} aim to characterise how the model behaves around
  one particular instance by considering small variations around this
  instance. The way these variations are processed by the original model
  allows to simplify it, i.e., approximate it, e.g., in a linear
  fashion. This approximation will determine the sign and magnitude of
  the impact of each relevant feature in the vicinity of the original
  instance.
\end{itemize}

To be fair, \citet{molnar2019interpretable} proposes another partition
by splitting interpretations that depend on one particular model (e.g.,
linear regression or decision tree) versus the interpretations that can
be obtained for any kind of model. In the sequel, we present the methods
according to the global versus local dichotomy.

\hypertarget{global-interpretations}{%
\section{Global interpretations}\label{global-interpretations}}

Let us start with the simplest example of all. In a linear model,
\[y_i=\alpha+\sum_{k=1}^K\beta_kx_i^k+\epsilon_i,\] the following
elements are usually extracted from the estimation of the \(\beta_k\):

\begin{itemize}
\tightlist
\item
  the \(R^2\), which appreciates the global fit of the model (possibly
  penalized to prevent overfitting with many regressors);\\
\item
  the sign of the estimates \(\hat{\beta}_k\), which indicates the
  direction of the impact of each feature \(x^k\) on \(y\);
\item
  the \(t\)-statistics \(t_{\hat{\beta_k}}\), which evaluate the
  magnitude of this impact: regardless of its direction, large
  statistics in absolute value reveal prominent variables. Often, the
  \(t\)-statistics are translated into \(p\)-values which are computed
  under some suitable distributional assumptions.
\end{itemize}

The last two indicators are useful because they inform the user on which
features matter the most and on the sign of the effect of each
predictor. This gives a simplified view of how the model processes the
features into the output. Most tools that aim to explain black boxes
follow the same principles.

\hypertarget{variable-importance}{%
\subsection{Variable importance
(tree-based)}\label{variable-importance}}

One incredibly favorable feature of simple decision trees is their
interpretability. Their visual representation is simple and
straightforward. Just like regressions (which are another building block
in ML), simple trees are simple to comprehend and do not suffer from the
black-box rebuke that is often associated to more sophisticated tools.

Indeed, both random forests and boosted trees fail to provide perfectly
accurate accounts of what is happening inside the engine. Nonetheless,
it is possible to compute the aggregate share (or importance) of each
feature in the determination of the structure of the tree once it has
been trained.

After training, it is possible to compute, at each node \(n\) the gain
\(G(n)\) obtained by the subsequent split (if there are any, i.e., if
the node is not a terminal leaf). It is also easy to determine which
variable is chosen to perform the split to write \(\mathcal{N}_k\) the
set of nodes related to feature \(k\). Then, the global importance of
each feature is given by \[I(k)=\sum_{n\in \mathcal{N}_k}G(n),\] and it
is often rescaled so that the sum of \(I(k)\) across all \(k\) is equal
to one. In this case, \(I(k)\) measures the relative contribution of
feature \(k\) in the reduction of loss during the training. A variable
with high importance will have a greater impact on predictions.
Generally, these variables are those that are located close to the root
of the tree.

Below, we take a look at the results obtained from the tree-based models
trained in Chapter \ref{trees}. We start by recylcling the output from
the three regression models we used.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tree_VI <-}\StringTok{ }\NormalTok{fit_tree}\OperatorTok{$}\NormalTok{variable.importance  }\OperatorTok{%>%}\StringTok{                                    }\CommentTok{# VI from tree model}
\StringTok{    }\KeywordTok{as_tibble}\NormalTok{(}\DataTypeTok{rownames =} \OtherTok{NA}\NormalTok{) }\OperatorTok{%>%}\StringTok{                                                }\CommentTok{# Transform in tibble }
\StringTok{    }\KeywordTok{rownames_to_column}\NormalTok{(}\StringTok{"Feature"}\NormalTok{)                                               }\CommentTok{# Add feature column}
\NormalTok{RF_VI <-}\StringTok{ }\NormalTok{fit_RF}\OperatorTok{$}\NormalTok{importance  }\OperatorTok{%>%}\StringTok{                                                 }\CommentTok{# VI from random forest}
\StringTok{    }\KeywordTok{as_tibble}\NormalTok{(}\DataTypeTok{rownames =} \OtherTok{NA}\NormalTok{) }\OperatorTok{%>%}\StringTok{                                                }\CommentTok{# Transform in tibble }
\StringTok{    }\KeywordTok{rownames_to_column}\NormalTok{(}\StringTok{"Feature"}\NormalTok{)                                               }\CommentTok{# Add feature column}
\NormalTok{XGB_VI <-}\StringTok{ }\KeywordTok{xgb.importance}\NormalTok{(}\DataTypeTok{model =}\NormalTok{ fit_xgb)[,}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{]                                 }\CommentTok{# VI from boosted trees}
\NormalTok{VI_trees <-}\StringTok{ }\NormalTok{tree_VI }\OperatorTok{%>%}\StringTok{ }\KeywordTok{left_join}\NormalTok{(RF_VI) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{left_join}\NormalTok{(XGB_VI)                  }\CommentTok{# Aggregate the VIs}
\KeywordTok{colnames}\NormalTok{(VI_trees)[}\DecValTok{2}\OperatorTok{:}\DecValTok{4}\NormalTok{] <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Tree"}\NormalTok{, }\StringTok{"RF"}\NormalTok{, }\StringTok{"XGB"}\NormalTok{)                               }\CommentTok{# New column names}
\NormalTok{norm_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x)\{}\KeywordTok{return}\NormalTok{(x }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(x))\}                                       }\CommentTok{# Normalizing function}
\NormalTok{VI_trees }\OperatorTok{%>%}\StringTok{ }\NormalTok{na.omit }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate_if}\NormalTok{(is.numeric,  norm_}\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{                     }\CommentTok{# Plotting sequence}
\StringTok{    }\KeywordTok{gather}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ model, }\DataTypeTok{value =}\NormalTok{ value, }\OperatorTok{-}\NormalTok{Feature) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Feature, }\DataTypeTok{y =}\NormalTok{ value, }\DataTypeTok{fill =}\NormalTok{ model)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{35}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=300px]{ML_factor_files/figure-latex/VItrees-1} 

}

\caption{Variable importance for tree-based models}\label{fig:VItrees}
\end{figure}

\normalsize

In the above code, tibbles are like dataframes (they are the v2 of
dataframes so to speak). Given the way the graph is coded, Figure
\ref{fig:VItrees} is in fact misleading. Indeed, by construction, the
tree model only has a small number of features with nonzero importance.
The graph shows these variables (with the intersection of nonzero
importances of the other two models). For scale reasons, the
normalization is performed \emph{after} the subset of features is
chosen, hence the bars do not exactly correspond to the true values.
Nonetheless, we prefered to limit the number of features shown on the
graph for obvious readability concerns.

There are differences in the way the models rely on the features. For
instance, the boosted tree model gives the most importance to Ebit\_Ta,
while the other two models overlook this variable. Across all models,
market capitalization is pregnant as it appears via three definitions of
the variable. The price-to-book ratio and volatility measures are the
other two type of features that seem to matter in the training of the
models.

One defining property of random forests is that they give a chance to
all features. Indeed, by randomizing the choice of predictors, each
individual exogenous variable has a shot at explaining the label. The
scores of the features for RF seem more balanced and the spread between
the most important and least important feature is the smallest among the
three models.

\hypertarget{variable-importance-agnostic}{%
\subsection{Variable importance
(agnostic)}\label{variable-importance-agnostic}}

The idea of quantifying the importance of each feature in the learning
process can be extended to non tree-based models. We refer to the paper
mentioned in the study \citet{fisher2018all} for more information on
this stream of the literature. The premise is the same as above: the aim
is to quantify to what extent one feature contributes to the learning
process.

One way to track the added value of one particular feature is to look at
what happens if its values inside the training set are entirely
shuffled. If the original feature plays an important role in the
explanation of the dependent variable, then the shuffled version of the
feature will lead to a much higher loss.

The baseline method to assess feature importance in the general case is
the following.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Train the model on the original data and compute the associated loss
  \(l^*\).\\
\item
  For each feature \(k\), create a new training dataset in which the
  features' values and randomly permuted. Then, train the model on this
  altered sample and record the corresponding loss \(l_k\).\\
\item
  Rank the variable importance of each feature, computed as a difference
  \(\text{VI}_k=l_k-l^*\) or a ratio \(\text{VI}_k=l_k/l^*\).
\end{enumerate}

The above procedure is of course random and can be repeated so that the
importances are averages to improve the stability of the results. This
algorithm is implemented in the FeatureImp() function of the iml package
developed by the author of \citet{molnar2019interpretable}. Below, we
implement this algorithm for the features appearing on Figure
\ref{fig:VItrees}. We test this approach on ridge regressions and
recycle the variables used in Chapter \ref{lasso}. We start by the first
step: computing the loss on the original training sample.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit_ridge_}\DecValTok{0}\NormalTok{ <-}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(x_penalized_train, y_penalized_train, }\DataTypeTok{alpha =} \DecValTok{0}\NormalTok{, }\DataTypeTok{lambda =} \FloatTok{0.01}\NormalTok{) }\CommentTok{# Trained model}
\NormalTok{l_star <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((y_penalized_train}\OperatorTok{-}\KeywordTok{predict}\NormalTok{(fit_ridge_}\DecValTok{0}\NormalTok{, x_penalized_train))}\OperatorTok{^}\DecValTok{2}\NormalTok{)         }\CommentTok{# Loss}
\end{Highlighting}
\end{Shaded}

\normalsize

Next, we evaluate the loss when each of the predictors have been
sequentially shuffled. To reduce computation time, we only make one
round of shuffling.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l <-}\StringTok{ }\KeywordTok{c}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(VI_trees))\{}
\NormalTok{    temp_data <-}\StringTok{ }\NormalTok{training_sample }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(features)                             }\CommentTok{# Temp feature matrix}
\NormalTok{    temp_data[, }\KeywordTok{which}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(temp_data) }\OperatorTok{==}\StringTok{ }\KeywordTok{as.character}\NormalTok{(VI_trees[i,}\DecValTok{1}\NormalTok{]))] <-}\StringTok{     }\CommentTok{# This line shuffles the values}
\StringTok{        }\KeywordTok{sample}\NormalTok{(temp_data[, }\KeywordTok{which}\NormalTok{(}\KeywordTok{colnames}\NormalTok{(temp_data) }\OperatorTok{==}\StringTok{ }\KeywordTok{as.character}\NormalTok{(VI_trees[i,}\DecValTok{1}\NormalTok{]))]}
               \OperatorTok{%>%}\StringTok{ }\KeywordTok{pull}\NormalTok{(}\DecValTok{1}\NormalTok{), }\DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{    x_penalized_temp <-}\StringTok{ }\NormalTok{temp_data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{()                                 }\CommentTok{# Turns predictors into matrix}
\NormalTok{    fit_ridge_temp <-}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(x_penalized_temp, y_penalized_train,                 }\CommentTok{# Fits the model}
                             \DataTypeTok{alpha =} \DecValTok{0}\NormalTok{, }\DataTypeTok{lambda =} \FloatTok{0.01}\NormalTok{) }
\NormalTok{    l[i] <-}\StringTok{ }\KeywordTok{mean}\NormalTok{((y_penalized_train}\OperatorTok{-}\KeywordTok{predict}\NormalTok{(fit_ridge_temp, x_penalized_temp))}\OperatorTok{^}\DecValTok{2}\NormalTok{) }\CommentTok{# Computes loss}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\normalsize

Finally, we plot the results.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{Feature =}\NormalTok{ VI_trees[,}\DecValTok{1}\NormalTok{], }\DataTypeTok{loss =}\NormalTok{ l }\OperatorTok{-}\StringTok{ }\NormalTok{l_star) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Feature, }\DataTypeTok{y =}\NormalTok{ loss)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_col}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{35}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=450px]{ML_factor_files/figure-latex/VIglobal2-1} 

}

\caption{Variable importance for a ridge regression model.}\label{fig:VIglobal2}
\end{figure}

\normalsize

The resulting importances are in line with thoses of thre tree-based
models: the most prominent variables are volatility-based, market
capitalization-based, and the price-to-book ratio. Note that in some
cases (e.g., the ROA), the score can even be negative, which means that
the predictions are more accurate than the baseline model when the
values of the predictor are shuffled!

\hypertarget{partial-dependence-plot}{%
\subsection{Partial dependence plot}\label{partial-dependence-plot}}

Partial dependence plots (PDPs) aim at showing the relationship between
the output of a model and the value of a feature (we refer to section
8.2 of \citet{friedman2001greedy} for an early treatment of this
subject).

Let us fix a feature \(k\). We want to understand the average impact of
\(k\) on the predictions of the trained model \(\hat{f}\). In order to
do so, we assume that the feature space is random and we split it in
two: \(k\) versus \(-k\), which stands for all features except for
\(k\). The partial dependence plot is defined as

\begin{equation}
\label{eq:pdp} 
\bar{f}_k(x_k)=\mathbb{E}[\hat{f}(\textbf{x}_{-k},x_k)]=\int \hat{f}(\textbf{x}_{-k},x_k)d\mathbb{P}_{-k}(\textbf{x}_{-k}),
\end{equation}

where \(d\mathbb{P}_{-k}(\cdot)\) is the (multivariate) distribution of
the non-\(k\) features \(\textbf{x}_{-k}\). In practice, this average is
evaluated using Monte-Carlo simulations:

\begin{equation}
\label{eq:pdpMC} 
\bar{f}_k(x_k)\approx \frac{1}{M}\sum_{m=1}^M\hat{f}\left(x_k,\textbf{x}_{-k}^{(m)}\right),
\end{equation} where \(\textbf{x}_{-k}^{(m)}\) are independent samples
of the non-\(k\) features.

Theoretically, PDPs could be computed for more than one feature at a
time. In practice, this is only possible for two features (yielding a 3D
surface) and is more computationally intense.

Finally, we refer to \citet{zhao2019causal} for a theoretical discussion
on the \emph{causality} property of PDPs. Indeed, a deep look at the
construction of the PDPs suggests that they could be interpreted as a
causal representation of the feature on the model's output.

We illustrate this concept below, using the dedicated package iml
(interpretable machine learning). The model we seek to explain is the
random forest built in Section \ref{random-forests}. We recycle some
variables used therein. We choose to test the impact of the
price-to-book ratio on the outcome of the model.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(iml)                                         }\CommentTok{# One package for interpretability}
\NormalTok{mod_iml <-}\StringTok{ }\NormalTok{Predictor}\OperatorTok{$}\KeywordTok{new}\NormalTok{(fit_RF,                     }\CommentTok{# This line encapsulates the objects}
                         \DataTypeTok{data =}\NormalTok{ training_sample }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(features))}
\NormalTok{pdp_PB =}\StringTok{ }\NormalTok{FeatureEffect}\OperatorTok{$}\KeywordTok{new}\NormalTok{(mod_iml, }\DataTypeTok{feature =} \StringTok{"Pb"}\NormalTok{)  }\CommentTok{# This line computes the PDP for price-to-book ratio}
\KeywordTok{plot}\NormalTok{(pdp_PB)                                         }\CommentTok{# Plot the partial dependence.}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=450px]{ML_factor_files/figure-latex/pdp-1} 

}

\caption{Partial dependence plot for the price-to-book ratio on the random forest model}\label{fig:pdp}
\end{figure}

\normalsize

The average impact of the price-to-book ratio on the predictions is
decreasing. This was somewhat expected, given the conditional average of
the dependent variable given the price-to-book ratio. This latter
function is depicted in Figure \ref{fig:rpart3mkt} and shows a behavior
comparable to the above curve: strongly decreasing for small value of
P/B and then relatively flat.

\hypertarget{local-interpretations}{%
\section{Local interpretations}\label{local-interpretations}}

Whereas global interpretations seek to assess the impact of features on
the output \(overall\), local methods try to quantify the behavior of
the model on particular instances or the neighborhood thereof. Local
interpretability has recently gained traction and many papers have been
published on this topic. Below, we outline the most widespread
methods.\footnote{For instance, we do not mention the work of
  \citet{horel2019towards} but the interested reader can have a look at
  their work on neural networks (and also at the references cited in the
  paper).}

\hypertarget{lime}{%
\subsection{LIME}\label{lime}}

LIME (Local Interpretable Model-Agnostic Explanations) is a methodology
originally proposed by \citet{ribeiro2016should}. Their aim is to
provide a faithfull account of the model under two constraints:

\begin{itemize}
\tightlist
\item
  \textbf{simple interpretability}, which implies a limited number of
  variables with visual or textual representation. This is to make sure
  any human can easily understand the outcome of the tool;
\item
  \textbf{local faithfulness}: the explanation holds for the vicinity of
  the instance.
\end{itemize}

The original (black-box) model is \(f\) and we assume we want to
approximate its behavior around instance \(x\) with the interpretable
model
\(g\).\footnote{In the original paper, the authors dig deeper into the notion of interpretable representations. In complex machine learning settings (image recognition or natural language processing), the original features given to the model can be hard to interpret. Hence, this requires an additional translation layer because the outcome of LIME must be expressed in terms of easily understood quantities. In factor investing, the features are elementary, hence we do not need to deal with this issue).}
The simple function \(g\) belongs to a larger class \(G\). The vicinity
of \(x\) is denoted \(\pi_x\) and the complexity of \(g\) is written
\(\Omega(g)\). LIME seeks an interpretation of the form
\[\xi(x)=\underset{g \in G}{\text{argmin}} \, \mathcal{L}(f,g,\pi_x)+\Omega(g),\]
where \(\mathcal{L}(f,g,\pi_x)\) is the loss function
(error/imprecision) induced by \(g\) in the vicinity \(\pi_x\) of \(x\).
The penalisation \(\Omega(g)\) is for instance the number of leaves or
depth of a tree, or the number of predictors in a linear regression.

It now remains to define some of the above terms. The vicinity of \(x\)
is defined by \(\pi_x(z)=e^{-D(x,z)^2/\sigma^2},\) where \(D\) is some
distance measure. We underline that this function decreases when \(z\)
shifts away from \(x\).

The tricky part is the loss function. In order to minimise it, LIME
generates artificial samples close to \(x\) and averages/sums the error
on the label that the simple representation makes. For simplicity, we
assume a scalar output for \(f\), hence the formulation is the
following: \[\mathcal{L}(f,g,\pi_x)=\sum_z \pi_x(z)(f(z)-g(z))^2\] and
the errors are weighted according to their distance from the initial
instance \(x\): the closest points get the largest weights. In its most
basic implementation, the set of models \(G\) consists of all linear
models.

In Figure \ref{fig:lime}, we provide a simplified diagram of how LIME
works.

\begin{figure}[b]

{\centering \includegraphics[width=300px]{images/lime} 

}

\caption{Simplistic explanation of LIME.}\label{fig:lime}
\end{figure}

For expositional clarity, we work with only one dependent variable. The
original training sample is shown with the black points. The fitted
(trained) model is represented with the blue line and we want to
approximate how the model works around one particular instance which is
highlighted by the red square around it. In order to build the
approximation, we sample 5 new points around the instance (the 5 red
triangles). Each triangle lies on the blue line (they are model
predictions) and has a weight proportional to its size: the triangle
closest to the instance has a bigger weight. Using weighted
least-squares, we build a linear model that fits to these 5 points. This
is the outcome of the approximation. It gives the two parameters of the
model: the intercept and the slope. Both can be evaluated with standard
statistical tests.

The sign of the slope is important. It is fairly clear that if the
instance had been taken closer to \(x=0\), the slope would have probably
been almost flat and hence the predictor could be locally discarded.
Another important detail is the number of sample points. In our
explanation, we take only five, but in practice, a robust estimation
usually requires around one thousand points or more. Indeed, when too
few neighbors are sampled, the estimation risk is high and the
approximation may be rough.

We proceed with an example of implementation. There are several steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Fit a model on some training data.\\
\item
  Wrap everything using the lime() function.\\
\item
  Focus on a few predictors and see their impact over a few particular
  instances (via the explain() function).
\end{enumerate}

We start with the first step. This time, we work with a boosted tree
model.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(lime)                              }\CommentTok{# Package for LIME interpretation}
\NormalTok{params_xgb <-}\StringTok{ }\KeywordTok{list}\NormalTok{(                        }\CommentTok{# Parameters of the boosted tree}
    \DataTypeTok{max_depth =} \DecValTok{5}\NormalTok{,                         }\CommentTok{# Max depth of each tree}
    \DataTypeTok{eta =} \FloatTok{0.5}\NormalTok{,                             }\CommentTok{# Learning rate }
    \DataTypeTok{gamma =} \FloatTok{0.1}\NormalTok{,                           }\CommentTok{# Penalization}
    \DataTypeTok{colsample_bytree =} \DecValTok{1}\NormalTok{,                  }\CommentTok{# Proportion of predictors to be sampled (1 = all)}
    \DataTypeTok{min_child_weight =} \DecValTok{10}\NormalTok{,                 }\CommentTok{# Min number of instances in each node}
    \DataTypeTok{subsample =} \DecValTok{1}\NormalTok{)                         }\CommentTok{# Proportion of instance to be sampled (1 = all)}
\NormalTok{xgb_model <-}\StringTok{ }\KeywordTok{xgb.train}\NormalTok{(params_xgb,         }\CommentTok{# Training of the model}
\NormalTok{                       train_matrix_xgb,   }\CommentTok{# Training data}
                       \DataTypeTok{nrounds =} \DecValTok{10}\NormalTok{)       }\CommentTok{# Number of trees}
\end{Highlighting}
\end{Shaded}

\normalsize

Then, we head on to steps two and three. As underlined above, we resort
to the lime() and explain() functions.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{explainer <-}\StringTok{ }\KeywordTok{lime}\NormalTok{(training_sample }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(features_short), xgb_model) }\CommentTok{# Step 2.}
\NormalTok{explanation <-}\StringTok{ }\KeywordTok{explain}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ training_sample }\OperatorTok{%>%}\StringTok{          }\CommentTok{# Step 3.}
\StringTok{                           }\KeywordTok{select}\NormalTok{(features_short) }\OperatorTok{%>%}
\StringTok{                           }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{slice}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{),           }\CommentTok{# We look at the first two instances in train_sample }
                       \DataTypeTok{explainer =}\NormalTok{ explainer,           }\CommentTok{# Input: the explainer variable created above }
                       \DataTypeTok{n_permutations =} \DecValTok{900}\NormalTok{,            }\CommentTok{# Nb samples used to compute the loss function}
                       \DataTypeTok{dist_fun =} \StringTok{"euclidean"}\NormalTok{,          }\CommentTok{# Distance function. "gower" is one alternative}
                       \DataTypeTok{n_features =} \DecValTok{6}                   \CommentTok{# Number of features displayed (most importance ones)}
\NormalTok{)}
\KeywordTok{plot_features}\NormalTok{(explanation, }\DataTypeTok{ncol =} \DecValTok{1}\NormalTok{)                    }\CommentTok{# Visual display}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=450px]{ML_factor_files/figure-latex/lime1-1} \end{center}

\normalsize

In each graph (one graph corresponds to the explanation around one
instance), there are two types of information: the sign of the impact
and the magnitude of the impact. The sign is revealed with the color
(positive in green, negative in red) and the magnitude is shown with the
size of the rectangles. For the first occurence, the Ebit\_Oa variable
is associated with a strong negative effect. In the second example, the
Earning per Share has a strong positive effect on the future return.

The values on the left of the graphs show the ranges of the features
with which the local approximations were computed.

Lastly, we briefly discuss the choice of distance function chosen in the
code. It is used to evaluate the distance between the true instance and
a simulated one to weight the prediction of the sampled instance. Our
dataset comprises only numerical data, hence the Euclidean distance is a
natural choice:

\[\text{Euclidean}(\textbf{x}, \textbf{y})=\sqrt{\sum_{n=1}^N(x_i-y_i)^2}.\]
Another possible choice would be the Manhattan distance:
\[\text{Manhattan}(\textbf{x}, \textbf{y})=\sum_{n=1}^N|x_i-y_i|.\]

The problem with these two distances is that they fail to handle
categorical variables. This is where the Gower distance steps in
(\citet{gower1971general}). The distance imposes a different treatment
on features of different types (classes versus numbers essentially, but
it can also handle missing data!). For categorical features, the Gower
distance applies a binary treatment: the value is equal to 1 if the
features are equal and to zero if not (i.e., \(1_{\{x_n=y_n\}}\)). For
numerical features, the spread is quantified as
\(1-\frac{|x_n-y_n|}{R_n}\), where \(R_n\) is the maximum absolute value
the feature can take. All similarity measurements are then aggregated to
yield the final score. Note that in this case, the logic is reversed:
\(\textbf{x}\) and \(\textbf{y}\) are very close if the Gower distance
is close to one and they are far away if the distance is close to zero.

\hypertarget{shapley-values}{%
\subsection{Shapley values}\label{shapley-values}}

The approach of Shapley values is somewhat different compared to LIME
and closer in spirit to PDPs. It originates from cooperative game theory
(\citet{shapley1953value}). The rationale is the following. One way to
assess the impact (or usefulness) of a variable is to look at what
happens if we remove this variable from the dataset. If this is very
detrimental to the quality to the model (i.e., to its predictions), then
it means that the variable is substantially valuable.

The simplest way to proceed is to take all variables and remove one to
evaluate its predictive ability. Shapley values are computed on a larger
scale because they consider all possible combinations of variables to
which they add the target predictor. Formally, this gives:

\begin{equation}
\label{eq:shapley} 
\phi_k=\sum_{S \subseteq \{x_1,\dots,x_K \} \backslash x_k}\underbrace{\frac{\text{Card}(S)!(K-\text{Card}(S)-1)!}{K!}}_{\text{weight of coalition}}\underbrace{\left(\hat{f}_{S \cup \{x_k\}}(S \cup \{x_k\})-\hat{f}_S(S)\right)}_{\text{gain when adding } x_k}
\end{equation}

\(S\) is any subset of the \textbf{coalition} that doesn't include
feature \(k\) and its size is Card(\(S\)). \textbackslash{} In the
equation above, the model \(f\) must be altered because it's impossible
to evaluate \(f\) when features are missing. In this case, several
possible options:

\begin{itemize}
\item set the missing value to its average or median value (in the whole sample) so that its effect is some `average' effect 
\item directly compute an average value $\int_{\mathbb{R} f(x_1,\dots,x_k,\dots,x_K)d\mathbb{P}_{x_k}}$, where $d\mathbb{P}_{x_k}$ is the empirical distribution of $x_k$ in the sample.
\end{itemize}

Obviously, Shapley values can take a lot of time to compute if the
number of predictors is large. We refer to \citet{chen2018shapley} for a
discussion on a simplifying method that reduces computation times in
this case. Extensions of Shapley values for interpretability are studied
in \citet{lundberg2017unified}.

The implementation of Shapley values is permitted via the \emph{iml}
package. There are two restrictions compared to LIME. First, the
features must be filtered upfront because all features are shown on the
graph (which becomes illegible beyond 20 features). This is why in the
code below, we use the short list of predictors (from Section
\ref{dataset}). Second, instances are analyzed one at a time.

We start by fitting a random forest model.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit_RF_short <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(R1M_Usd }\OperatorTok{~}\NormalTok{.,    }\CommentTok{# Same formula as for simple trees!}
                 \DataTypeTok{data =}\NormalTok{ training_sample }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\KeywordTok{c}\NormalTok{(features_short), }\StringTok{"R1M_Usd"}\NormalTok{),  }\CommentTok{# Data source}
                 \DataTypeTok{sampsize =} \DecValTok{10000}\NormalTok{,          }\CommentTok{# Size of (random) sample for each tree}
                 \DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{,           }\CommentTok{# Is the sampling done with replacement?}
                 \DataTypeTok{nodesize =} \DecValTok{250}\NormalTok{,            }\CommentTok{# Minimum size of terminal cluster}
                 \DataTypeTok{ntree =} \DecValTok{40}\NormalTok{,                }\CommentTok{# Nb of random trees}
                 \DataTypeTok{mtry =} \DecValTok{4}                   \CommentTok{# Nb of predictive variables for each tree}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\normalsize

We can then analyze the behavior of the model around the first instance
of the training sample.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predictor <-}\StringTok{ }\NormalTok{Predictor}\OperatorTok{$}\KeywordTok{new}\NormalTok{(fit_RF_short,    }\CommentTok{# This wraps the model & data}
                          \DataTypeTok{data =}\NormalTok{ training_sample }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(features_short), }
                          \DataTypeTok{y =}\NormalTok{ training_sample}\OperatorTok{$}\NormalTok{R1M_Usd)}
\NormalTok{shapley <-}\StringTok{ }\NormalTok{Shapley}\OperatorTok{$}\KeywordTok{new}\NormalTok{(predictor,                        }\CommentTok{# Compute the Shapley values...}
                       \DataTypeTok{x.interest =}\NormalTok{ training_sample }\OperatorTok{%>%}\StringTok{ }
\StringTok{                           }\KeywordTok{select}\NormalTok{(features_short) }\OperatorTok{%>%}
\StringTok{                           }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{slice}\NormalTok{(}\DecValTok{1}\NormalTok{))              }\CommentTok{# On the first instance}
\KeywordTok{plot}\NormalTok{(shapley) }\OperatorTok{+}\StringTok{ }\KeywordTok{coord_fixed}\NormalTok{(}\DecValTok{1500}\NormalTok{) }\OperatorTok{+}\StringTok{                      }\CommentTok{# Plot}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{angle =} \DecValTok{35}\NormalTok{, }\DataTypeTok{hjust =} \DecValTok{1}\NormalTok{)) }\OperatorTok{+}\StringTok{ }\KeywordTok{coord_flip}\NormalTok{()          }
\end{Highlighting}
\end{Shaded}

\begin{figure}[b]
\includegraphics[width=450px]{ML_factor_files/figure-latex/shapley-1} \caption{Illustration of the Shapley method.}\label{fig:shapley}
\end{figure}

\normalsize

In the output, we XXXXXXX

\hypertarget{breakdown}{%
\subsection{Breakdown}\label{breakdown}}

Breakdown (see e.g., \citet{staniak2018explanations}) is a mixture of
ideas from PDPs and Shapley values. The core of breakdown is the relaxed
model prediction, which is close in spirit to Equation \eqref{eq:pdp}. The
difference is that we are working at the local level, i.e., on one
particular observation, say \(x^*\). We want to measure the impact of a
set of predictors on the prediction associated to \(x^*\), hence we fix
two sets \(\textbf{k}\) (fixed) and \(-\textbf{k}\) (free) and evaluate
a proxy for the average prediction of the estimated model \(\hat{f}\)
when the set \(\textbf{k}\) of predictors is fixed at the values of
\(x^*\):

\[\tilde{f}_{\textbf{k}}(x^*)=\frac{1}{M}\sum_{m=1}^M \hat{f}\left(x^{(m)}_{-\textbf{k}},x^*_{\textbf{k}} \right).\]

The \(x^{(m)}\) in the above expression are either simulated values of
instances or simply sampled values from the dataset. The notation
implies that the instance has some values replaced by those of \(x^*\),
namely those that correspond to the indices \(\textbf{k}\). When
\(\textbf{k}\) consists of all features, then
\(\tilde{f}_{\textbf{k}}(x^*)\) is equal to the raw model prediction
\(\hat{f}(x^*)\) and when \(\textbf{k}\) is empty, it is equal to the
average sample value of the label (constant prediction).

The quantity of interest is the so-called contribution of feature
\(j\notin \textbf{k}\) with respect to data point \(x^*\) and set
\(\textbf{k}\):

\[\phi_{\textbf{k}}^j(x^*)=\tilde{f}_{\textbf{k} \cup j}(x^*)-\tilde{f}_{\textbf{k}}(x^*).\]

Just as for Shapley values, the above indicator computes an average
impact when augmenting the set of predictors with feature \(j\). By
definition, it depends on the set \(\textbf{k}\), so this is one notable
difference with Shapley values. In \citet{staniak2018explanations}, the
authors devise a procedure that incrementally increases or decreases the
set \(\textbf{k}\). This greedy idea helps alleviate the burden of
computing all possible combinations of features. Moreover, a very
convenient property of their algorithm is that the sum of all
contributions is equal to the predicted value.

In order to illustrate one implementation of breakdown, we train a
random forest on a limited number of features, as shown below. This will
increase the readability of the output of the breakdown.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{formula_short <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\StringTok{"R1M_Usd ~"}\NormalTok{, }\KeywordTok{paste}\NormalTok{(features_short, }\DataTypeTok{collapse =} \StringTok{" + "}\NormalTok{)) }\CommentTok{#  Model }
\NormalTok{formula_short <-}\StringTok{ }\KeywordTok{as.formula}\NormalTok{(formula_short)                                   }\CommentTok{# Forcing formula}
\NormalTok{fit_RF_short <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(formula_short, }\CommentTok{# Same formula as before}
                 \DataTypeTok{data =} \KeywordTok{select}\NormalTok{(training_sample, }\KeywordTok{c}\NormalTok{(features_short, }\StringTok{"R1M_Usd"}\NormalTok{)), }\CommentTok{# Features + label }
                 \DataTypeTok{sampsize =} \DecValTok{10000}\NormalTok{,          }\CommentTok{# Size of (random) sample for each tree}
                 \DataTypeTok{replace =} \OtherTok{FALSE}\NormalTok{,           }\CommentTok{# Is the sampling done with replacement?}
                 \DataTypeTok{nodesize =} \DecValTok{250}\NormalTok{,            }\CommentTok{# Minimum size of terminal cluster}
                 \DataTypeTok{ntree =} \DecValTok{12}\NormalTok{,                }\CommentTok{# Nb of random trees}
                 \DataTypeTok{mtry =} \DecValTok{5}                   \CommentTok{# Nb of predictive variables for each tree}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\normalsize

Once the model is trained, the syntax for the breakdown of predictions
is very simple.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(breakDown)}
\NormalTok{explain_break <-}\StringTok{ }\KeywordTok{broken}\NormalTok{(fit_RF_short, }
\NormalTok{                        data_ml[}\DecValTok{6}\NormalTok{,] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(features_short),}
                        \DataTypeTok{data =}\NormalTok{ data_ml }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(features_short))}
\KeywordTok{plot}\NormalTok{(explain_break) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[b]
\includegraphics[width=350px]{ML_factor_files/figure-latex/breakdown-1} \caption{Example of a breakdown output.}\label{fig:breakdown}
\end{figure}

\normalsize

The graphical output is intuitively interpreted. The grey bar is the
prediction of the model at the chosen instance. Green bars signal a
positive contribution and the yellowish rectangles show the variables
with negative impact. The relative sizes indicate the importance of each
feature.

\hypertarget{causality}{%
\chapter{Two key concepts: causality and
non-stationarity}\label{causality}}

A prominent point of criticism faced by ML tools is their inability to
uncover causality relationship between features and labels because they
are mostly focused (by design) to capture correlations. Correlations are
much weaker than causality because they characterize a two way
relationship (\(\textbf{X}\leftrightarrow \textbf{y}\)) while causality
specifies a direction \(\textbf{X}\rightarrow \textbf{y}\) or
\(\textbf{X}\leftarrow \textbf{y}\). One fashionable example is
sentiment. Many academic articles seem to find that sentiment
(irrespectively of its definition) is a significant driver of future
returns. A high sentiment for a particular stock may increase the demand
for this stock and push its price up (though contrarian reasonings may
also apply: if sentiment is high, it is a sign that mean-reversion is
possibly about to happen). The reverse causation is also plausible:
returns may well cause sentiment. If a stock experiences a long period
of market growth, people become bullish about this stock and sentiment
increases (this notably comes from extrapolation, see
\citet{barberis2015x} for a theoretical model). In
\citet{coqueret2018economic}, it is found (in opposition to most
findings in this field), that the latter relationship (returns
\(\rightarrow\) sentiment) is more likely. This result is backed by
causality driven tests (see Section \ref{granger}).

Statistical causality is a large field and we refer to
\citet{pearl2009causality} for a deep dive into this topic. Recently,
researchers have sought to link causality with ML approaches (see, e.g.,
\citet{peters2017elements}, \citet{heinze2018invariant},
\citet{arjovsky2019invariant}). The key notion in their work is
\textbf{invariance}.

Often, data is collected not at once, but from difference sources at
different moments. Some relationships found in these different sources
will changes while others may remain the same. The relationships that
are invariant to changing environments are likely to stem from (and
signal) causality. One counter-example is the following (related in
\citet{beery2018recognition}): training a computer vision algorithm to
discriminate between cows and camels will lead the algorithm to focus on
grass versus sand! Thus, a picture of a camel on grass will be
classified as cow while a cow on sand would be labelled ``camel''. It is
only with pictures of these two animals in different contexts
(environments) that the learner will end up truly finding what makes a
cow and a camel. A camel will remain a camel no matter where it is
pictured: it should be recognized as such by the learner. If so, the
representation of the camel becomes invariant over all datasets and the
learner has discovered causality, i.e., the true attributes that make a
camel a camel (overall silhouette, shape of the back, face, color
(possibly misleading!), etc.).

This search for invariance makes sense for many disciplines like
computer vision or natural language processing (languages don't change
much). In finance, it is not obvious that invariance may exist. Market
conditions are known to be time-varying and the relationships between
firm characteristics also change from year to year. One solution to this
issue may simply be to embrace non-stationarity. In Chapter
\ref{backtest}, we advocate to do that by updating models as frequently
as possible with rolling training sets: this allows the predictions to
be based on the most recent trends. In Section \ref{nonstat} below, we
introduce other theoretical and practical options.

\hypertarget{causality-1}{%
\section{Causality}\label{causality-1}}

Traditional machine learning models aim to uncover relationships between
variables but do not usually specify \emph{directions} for these
relationships. One typical example is the linear regression. If we write
\(y=a+bx+\epsilon\), then it is also true that
\(x=b^{-1}(y-a-\epsilon)\), which is of course also a linear
relationship (with respect to \(y\)). These equations do not define
causation whereby \(x\) would be a clear determinant of \(y\)
(\(x \rightarrow y\), but the opposite could be false).

\hypertarget{granger}{%
\subsection{Granger causality}\label{granger}}

The most notable tool first proposed by \citet{granger1969investigating}
is probably the simplest. For simplicity, we consider only two
stationary processes, \(X_t\) and \(Y_t\). A strict definition of
causality could be the following. \(X\) can be said to cause \(Y\),
whenever, for some integer \(k\),
\[(Y_{t+1},\dots,Y_{t+k})|(\mathcal{F}_{Y,t}\cup \mathcal{F}_{X,t}) \quad  \overset{d}{\neq} \quad (Y_{t+1},\dots,Y_{t+k})|\mathcal{F}_{Y,t},\]
that is, when the distribution of future values of \(Y_t\),
conditionally on the knowledge of both processes is not the same as the
distribution with the sole knowledge of the filtration
\(\mathcal{F}_{Y,t}\). Hence \(X\) does have an impact on \(Y\) because
its trajectory alters that of \(Y\).

Now, this formulation is too vague and impossible to handle numerically,
thus we simplify the setting via a linear formulation. We keep the same
notations as Section 5 of the original paper
\citet{granger1969investigating}. The tests consists in two regressions:
\begin{align*}
X_t&=\sum_{j=1}^ma_jX_{t-j}+\sum_{j=1}^mb_jY_{t-j} + \epsilon_t \\
Y_y&=\sum_{j=1}^mc_jX_{t-j}+\sum_{j=1}^md_jY_{t-j} + \nu_t
\end{align*} where for simplicity, it is assumed that both processes
have zero mean. The usual assumptions apply: the Gaussian noises
\(\epsilon_t\) and \(\nu_t\) are uncorrelated in every possible way
(mutually and through time). The test is the following: if one \(b_j\)
is nonzero, then it is said that \(Y\) Granger-causes \(X\) and if one
\(c_j\) is nonzero, \(X\) Granger-causes \(Y\). The two are not mutually
exclusive and it is widely accepted that feedback loops can very well
occur.

Statistically, under the null hypothesis, \(b_1=\dots=b_m=0\)
(\emph{resp.} \(c_1=\dots=c_m=0\)), which can be tested using the usual
Fischer distribution. Obviously, the linear restriction can be dismissed
but the tests are then much more complex. The main financial article in
this direction is \citet{hiemstra1994testing}.

There are many R packages that embed Granger causality functionalities.
One of the most widespread is \emph{lmtest} so we work with it below.
The syntax is incredibly simple. The \emph{order} is the maximum lag
\(m\) in the above equation. We test if market capitalization averaged
over the past 6 months Granger-causes 1 month ahead returns for one
particular stock (the first in the sample).

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(lmtest)}
\NormalTok{x_granger <-}\StringTok{ }\NormalTok{training_sample }\OperatorTok{%>%}\StringTok{                            }\CommentTok{# X variable =...}
\StringTok{                               }\KeywordTok{filter}\NormalTok{(stock_id }\OperatorTok{==}\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{     }\CommentTok{# ... stock nb 1}
\StringTok{                               }\KeywordTok{pull}\NormalTok{(Mkt_Cap_6M_Usd)         }\CommentTok{# ... & Market cap}
\NormalTok{y_granger <-}\StringTok{ }\NormalTok{training_sample }\OperatorTok{%>%}\StringTok{                            }\CommentTok{# Y variable = ...}
\StringTok{                               }\KeywordTok{filter}\NormalTok{(stock_id }\OperatorTok{==}\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{     }\CommentTok{# ... stock nb 1}
\StringTok{                               }\KeywordTok{pull}\NormalTok{(R1M_Usd)                }\CommentTok{# ... & 1M return}
\NormalTok{fit_granger <-}\StringTok{ }\KeywordTok{grangertest}\NormalTok{(x_granger,                       }\CommentTok{# X variable}
\NormalTok{                           y_granger,                       }\CommentTok{# Y variable}
                           \DataTypeTok{order =} \DecValTok{6}\NormalTok{,                       }\CommentTok{# Maximmum lag}
                           \DataTypeTok{na.action =}\NormalTok{ na.omit)             }\CommentTok{# What to do with missing data}
\NormalTok{fit_granger}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Granger causality test
## 
## Model 1: y_granger ~ Lags(y_granger, 1:6) + Lags(x_granger, 1:6)
## Model 2: y_granger ~ Lags(y_granger, 1:6)
##   Res.Df Df     F    Pr(>F)    
## 1    149                       
## 2    155 -6 4.111 0.0007554 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\normalsize

The test is directional and only test if \(X\) Granger-causes \(Y\). In
order to test the reverse effect, it is required to inverse the
arguments in the function. In the output above, the \(p\)-value is very
low, hence the probability of observing similar samples as ours knowing
that \(H_0\) holds is negligible. Thus is seems that market
capitalization does Granger cause one month returns.

\hypertarget{causal-additive-models}{%
\subsection{Causal additive models}\label{causal-additive-models}}

The zoo of causal model encompasses a variety of beasts. The interested
reader can have a peek at \citet{pearl2009causality},
\citet{peters2017elements} and \citet{maathuis2018handbook} and the
references therein. It is always hard to single out one type of model in
particular so we choose one that can be explained with simple
mathematical tools.

We start with the simplest definition of a structural causal model
(SCM), where we follow here Chapter 3 of \citet{peters2017elements}. The
idea behind these models is to introduce some hierarchy (i.e., some
additional structure) in the model. Formally, this gives \begin{align*}
X&=\epsilon_X \\ 
Y&=f(X,\epsilon_Y),
\end{align*} where the \(\epsilon_X\) and \(\epsilon_Y\) are independent
noise variables. Plainly, a realization of \(X\) is drawn randomly and
has then an impact on the realization of \(Y\) via \(f\). Now this
scheme could be more complex if the number of observed variable was
larger. Imagine a third variable comes in so that \begin{align*}
X&=\epsilon_X \\ 
Y&=f(X,\epsilon_Y),\\
Z&=g(Y,\epsilon_Z)
\end{align*}

In this case, \(X\) has a causation effect on \(Y\) and then \(Y\) has a
causation effect on \(Z\). We thus have the following connexions:
\[\begin{array}{ccccccc} X & &&&\\
&\searrow & &&\\
&&Y&\rightarrow&Z. \\
&\nearrow &&\nearrow& \\
\epsilon_Y & &\epsilon_Z 
\end{array}\]

The above representation is called a graph and graph theory has its own
nomenclature, which we very briefly summarize. The variables are often
referred to as \emph{vertices} (or \emph{nodes}) and the arrows as
\emph{edges}. Because arrows have a direction, they are called
\emph{directed} edges. When two vertices are connected via an edge, they
are called \emph{adjacent}. A sequence of adjacent vertices is called a
\emph{path} and it is directed if all edges are arrows. Within a
directed path, a vertex that comes first is a parent node and the one
just after is a child node.

Graphs can be summarized by adjacency matrices. An adjacency matrix
\(\textbf{A}=A_{ij}\) is a matrix filled with zeros and ones.
\(A_{ij}=1\) whenever there is an edge from vertex \(i\) to vertex
\(j\). Usually, self-loops (\(X \rightarrow X\)) are prohibited so that
adjacency matrices have zeros on the diagonal. If we consider a
simplified version of the above graph like
\(X \rightarrow Y \rightarrow Z\), the corresponding adjacency matrix is

\[\textbf{A}=\begin{bmatrix} 
0 & 1 & 0 \\
0 & 0 & 1 \\
0& 0&0
\end{bmatrix}.\]

where letters \(X\), \(Y\), and \(Z\) are naturally ordered
alphabetically. There are only two arrows: from \(X\) to \(Y\) (first
row, second column) and from \(Y\) to \(Z\) (second row, third column).

A \emph{cycle} is a particular type of path that creates a loop, i.e.,
when the first vertice is also the last. The sequence
\(X \rightarrow Y \rightarrow Z \rightarrow X\) is a cycle. Technically,
cycles pose problems. To illustrate this, consider the simple sequence
\(X \rightarrow Y \rightarrow X\). This would imply that a realization
of \(X\) causes \(Y\) which in turn would cause the realization of
\(Y\). While Granger causality can be viewed as allowing this kind of
connexion, general causal models usually avoid cycles and work with
direct acyclic graphs (DAGs).

Equipped with these tools, we can explicit a very general form of
models: \begin{equation}
\label{eq:CAM0} 
X_j=f_j\left(\textbf{X}_{\text{pa}_D(j)},\epsilon_j  \right),
\end{equation}

where the noise variables are mutually independent. The notation
\(\text{pa}_D(j)\) refers to the set of parent nodes of vertex \(j\)
within the graph structure \(D\). Hence, \(X_j\) is a function of all of
its parents and some noise term \(\epsilon_j\). An additive causal model
is a mild simplification of the above specification:

\begin{equation}
\label{eq:CAM} 
X_j=\sum_{k\in \text{pa}_D(j)}f_{j,k}\left(\textbf{X}_{k}  \right)+\epsilon_j,
\end{equation}

where the nonlinear effect of each variable is cumulative, hence the
term `\emph{additive}'. Note that there is no time index there. In
contrast to Granger causality, there is no natural ordering. Such models
are very complex and hard to estimate. The details can be found in
\citet{buhlmann2014cam}. Fortunately, the authors have developed an R
package that determines the DAG \(D\).

Below, we build the adjacency matrix pertaining to the small set of
predictor variables plus the 1 month ahead return (on the training
sample). We use the \emph{CAM} package which has a very simple syntax.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(CAM)}
\NormalTok{fit_cam <-}\StringTok{ }\KeywordTok{CAM}\NormalTok{(training_sample }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"R1M_Usd"}\NormalTok{, features_short)))}
\NormalTok{fit_cam}\OperatorTok{$}\NormalTok{Adj}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 8 x 8 sparse Matrix of class "dgCMatrix"
##                     
## [1,] . 1 1 1 1 1 1 1
## [2,] . . . 1 . . 1 .
## [3,] . 1 . 1 . . 1 1
## [4,] . . . . . . . .
## [5,] . 1 1 1 . 1 1 1
## [6,] . 1 1 1 . . 1 1
## [7,] . . . 1 . . . .
## [8,] . 1 . 1 . . 1 .
\end{verbatim}

\normalsize

The matrix is not too sparse, which means that a the model has uncovered
many relationships between the variables within the sample. Sadly, none
are in the direction that are of interest for the prediction task that
we seek. Indeed, the first variable is the one we want to predict and
its column is empty. However, its row is full, which indicates the
reverse effect: future returns cause the predictors values, which may
seem rather counter-intuitive.

\hypertarget{nonstat}{%
\section{Dealing with changing environments}\label{nonstat}}

The most common assumption in machine learning contributions is that the
samples that are studied are i.i.d. realizations of a phenomenon that we
are trying to characterize. This constraint is natural because if the
relationship between \(X\) and \(y\) always changes, then it is very
hard to infer anything from observations. One major problem in Finance
is that this is often the case: markets, behaviors, policies, etc.,
evolve all the time. This can be seen as a consequence of absence of
arbitrage: if a trading strategy worked all the time, all agents would
eventually adopt it via herding, which would annihilate the
corresponding gains.\footnote{See for instance the papers on herding in
  factor investing: \citet{krkoska2019herding} and
  \citet{santi2018exploring}.} If the strategy is kept private, its
holder would become infinitely rich, which obviously has never happened.

There are several ways to define changes in environments. If we denote
with \(\mathbb{P}_{XY}\) the multivariate distribution of all variables,
with \(\mathbb{P}_{XY}=\mathbb{P}_{X}\mathbb{P}_{Y|X}\), then two simple
changes are possible:

\begin{itemize}
\tightlist
\item
  \textbf{covariate shift}: \(\mathbb{P}_{X}\) changes but
  \(\mathbb{P}_{Y|X}\) does not: the features have a fluctuating
  distribution, but their relationship with \(Y\) holds still;\\
\item
  \textbf{concept drift}: \(\mathbb{P}_{Y|X}\) changes but
  \(\mathbb{P}_{X}\) does not: features are stable, but their relation
  to \(Y\) is altered.
\end{itemize}

Obviously, we omit the case when both items change is too complex to
handle. In factor investing, the feature engineering process (see
Section \ref{feateng}) is parlty designed to bypass the risk of
covariate shift. Uniformization guarantees that the marginals stay the
same but correlations between features may of course change. The main
issue is probably concept drift when the way features explain the label
changes through time. In \citet{cornuejols2011apprentissage}, the
authors distinguish four types of drifts, which we reproduce in Figure
\ref{fig:conceptchange}. In factor models, changes are presumably a
combination of all four types: they can be abrupt during crashes, but
most of the time they are progressive (gradual or incremental) and never
ending (continuously recurring).

\begin{figure}[H]

{\centering \includegraphics[width=300px]{images/conceptchange} 

}

\caption{Different flavours of concept change.}\label{fig:conceptchange}
\end{figure}

Naturally, if we aknowledge that the environment changes, it appears
logical to adapt models accordingly, i.e., dynamically. This gives rise
to the so-called \textbf{stability-plasticity dilemma}. This dilemma is
a trade-off between model \textbf{reactiveness} (new instances have an
important impact on updates) versus \textbf{stability} (these instances
may not be representative of a slower trend and thus shift the model in
a suboptimal direction).

Practically, there are two ways to shift the cursor with respect to this
dilemma: alter the chronological depth of the training sample (e.g., go
further back in time) or, when it's possible, allocate more weight to
recent instances. We discuss the first option in Section \ref{protocol}
and the second is mentioned in Section \ref{adaboost} (though the
purpose in Adaboost is precisely to let the algorithm handle the
weights). In neural networks, it is possible, in all generality to
introduce instance-based weights in the computation of the loss
function, though this option is not (yet) available in keras (to the
best of our knowledge). For simple regressions, this idea is known as
\textbf{weighted least squares} wherein errors are weighted inside the
loss: \[L=\sum_{i=1}^Iw_i(y_i-\textbf{x}_i\textbf{b})^2.\] In matrix
terms,
\(L=(\textbf{y}-\textbf{Xb})'\textbf{W}(\textbf{y}-\textbf{Xb})\), where
\(\textbf{W}\) is a diagonal matrix. The gradient with respect to
\(\textbf{b}\) is equal to
\(2\textbf{X}'\textbf{WX}\textbf{b}-2\textbf{X}'\textbf{Wy}\) so that
the loss is minimized for
\(\textbf{b}^*=(\textbf{X}'\textbf{WX})^{-1}\textbf{X}'\textbf{Wy}\).
The standard least-square solution is recovered for
\(\textbf{W}=\textbf{I}\). In order to fine-tune the reactiveness of the
model, the weights must be a function that decreases as instances become
older in the sample.

There is of course no perfect solution to changing financial
environements. Below, we mention two routes that are taken in the ML
literature to overcome the problem of non-stationarity in the data
generating process. But first, we propose a clear verification that
markets do experience time-varying distributions.

\hypertarget{non-stationarity-an-obvious-illustration}{%
\subsection{Non-stationarity: an obvious
illustration}\label{non-stationarity-an-obvious-illustration}}

One of the most basic practices in (financial) econometrics is to work
with returns (relative price changes). The simple reason is that returns
seem to behave consistently through time (monthly returns are bounded,
they usually lie between -1 and +1). Prices on the other hand shift and
often, some prices never come back to past values. This makes prices
harder to study.

Stationarity is a key notion in financial econometrics: it is much
easier to characterize a phenomenon that remains the same through time.
Sadly, the distribution of returns is not stationary: both the mean and
the variance of returns change along cycles.

Below, we illustrate this fact by computing the average monthly return
for all calendar years in the whole dataset.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data_ml }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{year =} \KeywordTok{year}\NormalTok{(date)) }\OperatorTok{%>%}\StringTok{          }\CommentTok{# Create a year variable}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(year) }\OperatorTok{%>%}\StringTok{                     }\CommentTok{# Group by year}
\StringTok{    }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{avg_ret =} \KeywordTok{mean}\NormalTok{(R1M_Usd)) }\OperatorTok{%>%}\StringTok{ }\CommentTok{# Compute average return}
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ year, }\DataTypeTok{y =}\NormalTok{ avg_ret)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_col}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_grey}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=350px]{ML_factor_files/figure-latex/statplot-1} 

}

\caption{Average monthly return on a yearly basis.}\label{fig:statplot}
\end{figure}

\normalsize

These changes in the mean are also accompanied by variations in the
second moment (variance/volatility). This effect, known as volatility
clustering has been widely documented ever since the theoretical
breakthrough of \citet{engle1982autoregressive} (and even well before).
We refer for instance to \citet{cont2007volatility} for more details on
this topic. For the computation of realized volatility in R, we strongly
recommend Chapter 4 in \citet{regenstein2018reproducible}.

In terms of machine learning models, this is also true. Below, we
estimate a pure characteristic regression with one predictor, the market
capitalization averaged over the past 6 months. The label is the 6 month
forward return and the estimation is performed over every calendar year.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data_ml }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{year =} \KeywordTok{year}\NormalTok{(date)) }\OperatorTok{%>%}\StringTok{                           }\CommentTok{# Create a year variable}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(year) }\OperatorTok{%>%}\StringTok{                                      }\CommentTok{# Group by year}
\StringTok{    }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{beta_cap =} \KeywordTok{lm}\NormalTok{(R6M_Usd }\OperatorTok{~}\StringTok{ }\NormalTok{Mkt_Cap_6M_Usd) }\OperatorTok{%>%}\StringTok{   }\CommentTok{# Perform regression}
\StringTok{                  }\KeywordTok{coef}\NormalTok{() }\OperatorTok{%>%}\StringTok{                                }\CommentTok{# Extract coefs}
\StringTok{                  }\KeywordTok{t}\NormalTok{() }\OperatorTok{%>%}\StringTok{                                   }\CommentTok{# Transpose}
\StringTok{                  }\KeywordTok{data.frame}\NormalTok{() }\OperatorTok{%>%}\StringTok{                          }\CommentTok{# Format into df}
\StringTok{                  }\KeywordTok{pull}\NormalTok{(Mkt_Cap_6M_Usd)) }\OperatorTok{%>%}\StringTok{                 }\CommentTok{# Pull coef (remove intercept)}
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ year, }\DataTypeTok{y =}\NormalTok{ beta_cap)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_col}\NormalTok{() }\OperatorTok{+}\StringTok{      }\CommentTok{# Plot}
\StringTok{    }\KeywordTok{theme_grey}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=350px]{ML_factor_files/figure-latex/conceptdriftemp-1} 

}

\caption{Variations in betas with respect to 6 month market capitalization}\label{fig:conceptdriftemp}
\end{figure}

\normalsize

The figure highlights the concept drift: overall, the relationship
between capitalization and returns is negative (the \textbf{size effect}
again). Sometimes it is markedly negative, sometimes, not so much. The
ability of capitalization to explain returns is time-varying and models
must adapt accordingly.

\hypertarget{online-learning}{%
\subsection{Online learning}\label{online-learning}}

Online learning refers to a subset of machine learning in which new
information arrives progressively and the integration of this flow is
performed iteratively (the term `\emph{online}' is not linked to
internet). In order to take the latest data updates into account, it is
imperative to update the model (stating the obvious). This is clearly
the case in finance and this topic is closely related to the discussion
on learning windows in Section \ref{protocol}.

The problem is that if a 2019 model is trained on data from 2010 to
2019, the (dynamic) 2020 model will have to be re-trained with the whole
dataset including the latest points from 2020. This can be heavy and
including just the latest points in the learning process would
substantially decrease its computational cost. In neural networks, the
sequential batch updating of weights can allow a progressive change in
the model. Nonetheless, this is typically impossible for decision trees
because the splits are decided once and for all. One notable exception
is \citet{basak2004online} but in that case, the construction of the
trees differs strongly from the original algorithm (and spirit).

The simplest example of online learning is the Widrow-Hodd algorithm
(originally from \citet{widrow1960adaptive}). Originally, the idea comes
from the so-called ADALINE (ADAptive LInear NEuron) model which is a
neural network with one hidden layer with linear activation function
(i.e., like a perceptron, but with a different activation).

Suppose the model is linear, that is
\(\textbf{y}=\textbf{Xb}+\textbf{e}\) (a constant can be added to the
list of predictors) and that the amount of data is both massive and
coming in at a high frequency so that updating the model on the full
sample is proscribed because technically intractable. A simple and
heuristic way to update the values of \(\textbf{b}\) is to compute
\[\textbf{b}_{t+1} \longleftarrow \textbf{b}_t-\eta (\textbf{x}_t\textbf{b}-y_t)\textbf{x}_t,\]
where \(\textbf{x}_t\) is the row vector of instance \(t\). The
justification is simple. The quadratic error
\((\textbf{x}_t\textbf{b}-y_t)^2\) has a gradient with respect to
\(\textbf{b}\) equal to \(2(\textbf{x}_t\textbf{b}-y_t)\textbf{x}_t\)
therefore the above update is a simple example of gradient descent.
\(\eta\) must of course be quite small: if not, each new point will
considerably alter \(\textbf{b}\) resulting in a volatile model.

An exhaustive review of techniques pertaining to online learning is
presented in \citet{hoi2018online} (section 4.11 is even dedicated to
portfolio selection). The book \citet{hazan2016introduction} covers
online convex optimization which is a very close domain with a large
overlap with online learning. The presentation below is adapted from the
second and third parts of the first survey.

Datasets are indexed by time: we write \(\textbf{X}_t\) and
\(\textbf{y}_t\) for features and labels (the usual column index (\(k\))
and row index (\(i\)) will not be used in this section). Time has a
bounded horizon \(T\). The machine learning model depends on some
parameters \(\boldsymbol{\theta}\) and we denote it with
\(f_{\boldsymbol{\theta}}\). At time \(t\) (when dataset
(\(\textbf{X}_t\), \(\textbf{y}_t\)) is gathered), the loss function
\(L\) fo the trained model naturally depends on the data
(\(\textbf{X}_t\), \(\textbf{y}_t\)) and on the model via
\(\boldsymbol{\theta}_t\) which are the parameter values fitted to the
time-\(t\) data. For notational simplicity, we henceforth write
\(L_t(\boldsymbol{\theta}_t)=L(\textbf{X}_t,\textbf{y}_t,\boldsymbol{\theta}_t )\).
The key quantity in online learning is the regret over the whole time
sequence: \begin{equation}
\label{eq:regret} 
R_T=\sum_{t=1}^TL_t(\boldsymbol{\theta}_t)-\underset{\boldsymbol{\theta}^*\in \boldsymbol{\Theta}}{\inf} \ \sum_{t=1}^TL_t(\boldsymbol{\theta}^*).
\end{equation}

The regret is the total loss incurred by the models
\(\boldsymbol{\theta}_t\) minus the minimal loss that could have
obtained with full knowledge of the data sequence (hence computed in
hindsight). The basic methods in online learning are in fact quite
similar to the batch-training of neural networks. The updating of the
parameter is based on \begin{equation}
\label{eq:online1} 
\textbf{z}_{t+1}=\boldsymbol{\theta}_t-\eta_t\nabla L_t(\boldsymbol{\theta}_t),
\end{equation} where \(\nabla L_t(\boldsymbol{\theta}_t)\) denotes the
gradient of the current loss \(L_t\). One problem that can arise is when
\(\textbf{z}_{t+1}\) falls out of the bounds that are prescribed for
\(\boldsymbol{\theta}_t\). Thus, the candidate vector for the new
parameters, \(\textbf{z}_{t+1}\), is projected onto the feasible domain
which we call \(S\) here: \begin{equation}
\label{eq:online2} 
\boldsymbol{\theta}_{t+1}=\Pi_S(\textbf{z}_{t+1}), \quad \text{with} \quad \Pi_S(\textbf{u}) = \underset{\boldsymbol{\theta}\in S}{\text{argmin}} \ ||\boldsymbol{\theta}-\textbf{u}||_2.
\end{equation} Hence \(\boldsymbol{\theta}_{t+1}\) is as close as
possible to the intermediate choice \(\textbf{z}_{t+1}\). In
\citet{hazan2007logarithmic}, it is shown that under suitable
assumptions (e.g., \(L_t\) being strictly convex with bounded gradient
\(\left|\left|\underset{\boldsymbol{\theta}}{\sup} \, \nabla L_t(\boldsymbol{\theta})\right|\right|\le G\)),
the regret \(R_T\) satisfies \[R_T \le \frac{G^2}{2H}(1+\log(T)),\]
where \(H\) is a scaling factor for the learning rate (also called step
sizes): \(\eta_t=(Ht)^{-1}\).

More sophisticated online algorithms generalize \eqref{eq:online1} and
\eqref{eq:online2} by integrating the Hessian matrix
\(\nabla^2 L_t(\boldsymbol{\theta}):=[\nabla^2 L_t]_{i,j}=\frac{\partial}{\partial \boldsymbol{\theta}_i \partial \boldsymbol{\theta}_j}L_t(\partial \boldsymbol{\theta})\)
and/or by including penalizations to reduce instability in
\(\boldsymbol{\theta}_t\). We refer to Section 2 in
\citet{hoi2018online} for more details on these extensions.

An interesting stream of parameter updating is that of the
passive-aggressive algorithms (PAAs) formalized in
\citet{crammer2006online}. The base case involves classification tasks,
but we stick to the regression setting below (section 5 in
\citet{crammer2006online}). One strong limitation with PAAs is that they
rely on the set of parameters where the loss is either zero or
negligible:
\(\boldsymbol{\Theta}^*_\epsilon=\{\boldsymbol{\theta}, L_t(\boldsymbol{\theta})< \epsilon\}\).
For general loss functions and learner \(f\), this set is largely
inaccessible. Thus, the algorithms in \citet{crammer2006online} are
restricted to a particular case, namely linear \(f\) and
\(\epsilon\)-insensitive hinge loss:

\[L_\epsilon(\boldsymbol{\theta})=\left\{ \begin{array}{ll}
0 & \text{if } \ |\boldsymbol{\theta}'\textbf{x}-y|\le \epsilon \quad (\text{close enough prediction}) \\
|\boldsymbol{\theta}'\textbf{x}-y|- \epsilon & \text{if } \  |\boldsymbol{\theta}'\textbf{x}-y| >  \epsilon \quad (\text{prediction too far})
\end{array}\right.,\]

for some parameter \(\epsilon>0\). If the weight \(\boldsymbol{\theta}\)
is such that the model is close enough to the true value, then the loss
is zero, if not, it is equal to the absolute value of the error minus
\(\epsilon\). In PAA, the update of the parameter is given by
\[\boldsymbol{\theta}_{t+1}= \underset{\boldsymbol{\theta}}{\text{argmin}} ||\boldsymbol{\theta}-\boldsymbol{\theta}_t||_2^2, \quad \text{subject to} \quad L_\epsilon(\boldsymbol{\theta})=0,\]
hence the new parameter values are chosen such that two conditions are
satisfied:\\
- the loss is zero (by the definition of the loss, this means that the
model is close enough to the true value);\\
- and, the parameter is as close a possible to the previous parameter
values.

Hence, if the model is good enough, the model does not move (passive
phase), but if not, it is rapidly shifted towards values that yield
satisfactory results (aggressive phase).

We end this section with a historical note. Some of the ideas from
online learning stem from the financial literature and from the concept
of universal portfolios from \citet{cover1991universal} in particular.
The setting is the following. The function \(f\) is assumed to be linear
\(f(\textbf{x}_t)=\boldsymbol{\theta}'\textbf{x}_t\) and the data
\(\textbf{x}_t\) consists of asset returns, thus, the values are
portfolio returns as long as \(\boldsymbol{\theta}'\textbf{1}_N=1\) (the
budget constraint). The loss functions \(L_t\) correspond to concave
utility functions and the regret is reversed:
\[R_T=\underset{\boldsymbol{\theta}^*\in \boldsymbol{\Theta}}{\sup} \ \sum_{t=1}^TL_t(\textbf{r}_t'\boldsymbol{\theta}^*)-\sum_{t=1}^TL_t(\textbf{r}_t'\boldsymbol{\theta}_t),\]
where \(\textbf{r}_t'\) are the returns. Thus, the program is
transformed to maximize a concave function. Several articles (often from
the Computer Science or ML communities) have proposed solutions to this
type of problems: \citet{blum1999universal},
\citet{agarwal2006algorithms} and \citet{hazan2007logarithmic}.

\hypertarget{homogeneous-transfer-learning}{%
\subsection{Homogeneous transfer
learning}\label{homogeneous-transfer-learning}}

The next two subsections are mostly conceptual and will not be
illustrated by coded applications. The ideas behind transfer learning
and active learning can be valuable in that they can foster novel ideas.

Transfer learning has been surveyed numerous times. One classical
reference is \citet{pan2009survey}, but \citet{weiss2016survey} is more
recent and more exhaustive. Suppose we are given two datasets \(D_S\)
(source) and \(D_T\) (target). Each dataset has its own features
\(\textbf{X}^S\) and \(\textbf{X}^T\) and labels \(\textbf{y}^S\) and
\(\textbf{y}^T\). In classical supervised learning, the patterns of the
target set are learned only through \(\textbf{X}^T\) and
\(\textbf{y}^T\). Transfer learning proposes to improve the function
\(f^T\) (obtained by minimizing the fit
\(y_i^T=f^T(\textbf{x}_i^T)+\epsilon^T_i\) on the target data) via the
function \(f^S\) (from \(y_i^S=f^S(\textbf{x}_i^S)+\varepsilon^S_i\) on
the source data). Homogeneous transfer learning is when the feature
space does not change, which is the case in our setting. In asset
management, this may not always be the case if for instance new
predictors are included (e.g., based on alternative data like sentiment,
satelitte imagery, credit card logs, etc.).

There are many subcategories in transfer learning depending on what
changes between the source \(S\) and the target \(T\): is it the feature
space, the distribution of the labels, and/or the relationship between
the two? This latter case is of interest in finance because the link
with non-stationarity is evident: it is when the model \(f\) in
\(\textbf{y}=f(\textbf{X})\) changes through time. In transfer learning
jargon, it is written as
\(P[\textbf{y}^S|\textbf{X}^S]\neq P[\textbf{y}^T|\textbf{X}^T]\): the
conditional law of the label knowing the features is not the same when
switching from the source to the target. Often, the term `domain
adaptation' is used as synonym to transfert learning. Because of a data
shift, we must adapt the model to increase is accuracy. These topics are
reviewed in a series of chapters in the collection by
\citet{quionero2009dataset}.

An important and elegant result in the theory was proven by
\citet{ben2010theory} in the case of binary classification. We state it
below. We consider \(f\) and \(h\) two classifiers with values in
\(\{0,1 \}\). The average error between the two over the domain \(S\) is
defined by
\[\epsilon_S(f,h)=\mathbb{E}_S[|f(\textbf{x})-h(\textbf{x})|].\] Then,
\[\epsilon_T(f_T,h)\le \epsilon_S(f_S,h)+\underbrace{2 \sup_B|P_S(B)-P_T(B)|}_{\text{ difference between domains }} + \underbrace{ \min\left(\mathbb{E}_S[|f_S(\textbf{x})-f_T(\textbf{x})|],\mathbb{E}_T[|f_S(\textbf{x})-f_T(\textbf{x})|]\right)}_{\text{difference between the two learning tasks}}.\]

The above inequality is a bound on the generalization performance of
\(h\). If we take \(f_S\) to be the best possible classifier for \(S\)
and \(f_T\) the best for \(T\), then the error generated by \(h\) in
\(T\) is smaller than the sum of three components:\\
- the error in the \(S\) space;\\
- the distance between the two domains (by how much the data space has
shifted);\\
- the ditance between the two best models (generators).

One solution that is often mentioned in transfer learning is instance
weighting. We present it here in a general setting. In machine learning,
we seek to minimize \begin{align*}
\epsilon_T(f)=\mathbb{E}_T\left[L(\text{y},f(\textbf{X})) \right],
\end{align*} where \(L\) is some loss function that depends on the taks
(regression versus classification). This can be arranged \begin{align*}
\epsilon_T(f)&=\mathbb{E}_T \left[\frac{P_S(\textbf{y},\textbf{X})}{P_S(\textbf{y},\textbf{X})} L(\text{y},f(\textbf{X})) \right]  \\
&=\sum_{\textbf{y},\textbf{X}}P_T(\textbf{y},\textbf{X})\frac{P_S(\textbf{y},\textbf{X})}{P_S(\textbf{y},\textbf{X})} L(\text{y},f(\textbf{X})) \\
&=\mathbb{E}_S \left[\frac{P_T(\textbf{y},\textbf{X})}{P_S(\textbf{y},\textbf{X})} L(\text{y},f(\textbf{X})) \right]
\end{align*}

The key quantity is thus the transition ratio
\(\frac{P_T(\textbf{y},\textbf{X})}{P_S(\textbf{y},\textbf{X})}\)
(Radon--Nikodym derivative under some assumptions). Of course this ratio
is largely inaccessible in practice, but it is possible to find a
weighting scheme (over the instances) that yields improvements over the
error in the target space. The weighting scheme, just as in
\citet{coqueret2019training} can be binary, thereby simply excluding
some observations in the computation of the error.

More generally, the above expression can be viewed as a theoretical
invitation for user-specified instance weighting (as in
\ref{instweight}). In the asset allocation parlance, this can be viewed
as introducing views so as to which observations are the most
interesting, e.g., value stocks can be allowed to have a larger weight
in the computation of the loss. Naturally, it remains to minimize this
loss.

\hypertarget{active-learning}{%
\subsection{Active learning}\label{active-learning}}

We end this section with the notion of active learning. To the best of
our knowledge, it is not widely used in quantitative investment, but the
underlying concept is enlightening, hence we dedicate a few paragraphs
to this notion for the sake of completeness.\footnote{As a matter of
  fact, active learning is not known to be particularly useful for
  learning from non-stationary environments.}

In general supervised learning, there is sometimes an asymmetry in the
ability to gather features versus labels. For instance, it is free to
have access to images, but the labelling of the content of the image
(e.g., ``a dog'', ``a truck'', ``a pizza'', etc.) is costly because it
requires human annotation. In formal terms, \(\textbf{X}\) is cheap but
the corresponding \(\textbf{y}\) is expensive.

As is often the case when facing cost constraints, a evident solution is
greed. Ahead of the usual learning process, a filter (often called
\emph{query}) is used to decide which data to label and train on
(possibly in relationship with the ML algorithm). The labelling is
performed by a so-called \emph{oracle} (which/who knows the truth) -
usually human. This technique that focuses on the most informative
instances is referred to as \textbf{active learning}. We refer to the
surveys \citet{settles2009active} and \citet{settles2012active} for a
detailed account of this field (which we briefly summarize below). The
term \textbf{active} comes from the fact that the learner does not
passively accept data samples but actively participates to the choices
of items it learns from.

One major dichotomy in active learning pertains to the data source
\(\textbf{X}\) on which the query is based. One obvious case is when the
original sample \(\textbf{X}\) is very large and not labelled and the
learner asks for particular instances within this sample to be labelled.
The second case is when the learner has the ability to simulate/generate
its own values \(\textbf{x}_i\). This can sometimes be problematic if
the oracle does not recognize the data that is generated by the machine.
For instance, if the purpose is to label images of characters and
numbers, the learner mat generate shapes that do not correspond to any
letter or digit: the oracle cannot label it.

In active learning, one key question is: how does the learner choose the
instances to be labelled? Heuristically, the answer is: by picking those
obsrevations that maximize learning efficiency. In binary
classification, a simple criterion is the probability of belonging to
one particular class. If this probability is far from 0.5, then the
algorithm will have no difficulty of picking one class (even though it
can be wrong). The interesting case is when the probability is close to
0.5: the machine may hesitate. Thus, having the oracle label is useful
in this case because it helps the learner in a configuration it which it
is undecided.

Other methods seek to estimate the fit that can be obtained when
including particular (new) instances in the training set - and then to
optimize this fit. Recalling Section 3.1 in \citet{geman1992neural} on
the variance-bias tradeoff, we have, for a training dataset \(D\) and
one instance \(x\) (we omit bold font for simplicity),
\[\mathbb{E}\left[\left.(y-\hat{f}(x;D))^2\right|\{D,x\}\right]=\mathbb{E}\left[\left.\underbrace{(y-\mathbb{E}[y|x])^2}_{\text{indep. from }D\text{ and }\hat{f}} \right|\{D,x\} \right]+(\hat{f}(x;D)-\mathbb{E}[y|x])^2,\]
where the notation \(f(x;D)\) is used to highlight the dependence
between the model \(\hat{f}\) and the dataset \(D\): the model has been
trained on \(D\). The first term is irreducible, as it does not depend
on \(\hat{f}\). Thus, only the second term is of interest. If we take
the average of this quantity, taken over all possible values of \(D\):
\[\mathbb{E}_D\left[(\hat{f}(x;D)-\mathbb{E}[y|x])^2  \right]=\underbrace{\left(\mathbb{E}_D\left[\hat{f}(x;D)-\mathbb{E}[y|x]\right]\right)^2}_{\text{squared bias}} \ + \ \underbrace{\mathbb{E}_D\left[(\hat{f}(x,D)-\mathbb{E}_D[\hat{f}(x;D)])^2\right]}_{\text{variance}}\]
If this expression is not too complicated to compute, the learner can
query the \(x\) that minimizes the tradeoff. Thus, on average, this new
instance will be the one that yields the best learning angle (as
measured by the \(L^2\) error). Beyond this approach (which is limited
because it requires the oracle to label a possibly irrelevant instance),
many other criteria exist for querying and we refer to Section 3 from
\citet{settles2009active} for an exhaustive list.

One final question: is active learning applicable to factor investing?
One straightfoward answer is that data cannot be annotated by human
intervention. Thus, the learner cannot simulate its own instances and
ask for corresponding labels. One possible option is to provide the
learner with \(\textbf{X}\) but not \(\textbf{y}\) and keep only a
queried subset of observations with the corresponding labels. In spirit,
this is close to what is done in \citet{coqueret2019training} except
that the query is not performed by a machine but by the human user.
Indeed, it is shown in this paper that not all observations carry the
same amount of signal. Instances with `average' label values seem less
informative compared to those with extreme label values.

\hypertarget{unsup}{%
\chapter{Unsupervised learning}\label{unsup}}

All algorithms presented in Chapters \ref{lasso} to \ref{bayes} belong
to the larger class of supervised learning tools. Such tools seek to
unveil a mapping between predictors \(\textbf{X}\) and a label
\(\textbf{y}\). The supervision comes from the fact that it is asked
that the data tries to explain this particular variable \(\textbf{y}\).
Another important part of machine learning consists of unsupervised
tasks, that is, when \(\textbf{y}\) is not specified and the algortithm
tries to make sense of \(\textbf{X}\) on its own. Often, relationships
between the components of \(\textbf{X}\) are identified. This field is
much to vast to be summarized in one book, let alone one chapter. The
purpose here is to briefly explain in what ways unsupervised learning
can be used, especially in the data pre-processing phase.

\hypertarget{corpred}{%
\section{The problem with correlated predictors}\label{corpred}}

Often, it is tempting to supply all predictors to a ML-fueled predictive
engine. That may not be a good idea when some predictors are highly
correlated. To illustrate this, the simplest example is a regression on
two variables with zero mean and covariance and precisions matrices:
\[\boldsymbol{\Sigma}=\textbf{X}'\textbf{X}=\begin{bmatrix} 1 & \rho \\ \rho & 1 \end{bmatrix},  \quad \boldsymbol{\Sigma}^{-1}=\frac{1}{1-\rho^2}\begin{bmatrix} 1 & -\rho \\ -\rho & 1 \end{bmatrix}.\]
When the covariance/correlation \(\rho\) increase towards 1, the scaling
denominator in \(\boldsymbol{\Sigma}\) goes to zero and the formula
\(\hat{\boldsymbol{\beta}}=\boldsymbol{\Sigma}^{-1}\textbf{X}'\textbf{y}\)
implies that one coefficient will be highly positive and one highly
negative. The regression creates a spurious arbitrage between the two
variables. Of course, this is very inefficient a yields disastrous
results out-of-sample.

We illustrate what happens when many variables are used in the
regression below (Table \ref{tab:regbroom}). One elucidation of the
aforementioned phenomenon comes from the variables Mkt\_Cap\_12M\_Usd
and Mkt\_Cap\_6M\_Usd, which have a correlation of 99.6\% in the
training sample. Both are singled out as highly significant but their
signs are contradictory. Moreover, the magnitude of their coefficients
are very close (0.21 versus 0.18) so that their net effect cancels out.
Naturally, providing the regression with only one of these two input
would have been wiser.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(broom)                            }\CommentTok{# Package for clean regression output }
\NormalTok{training_sample }\OperatorTok{%>%}\StringTok{    }
\StringTok{    }\KeywordTok{select}\NormalTok{(}\KeywordTok{c}\NormalTok{(features,  }\StringTok{"R1M_Usd"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{   }\CommentTok{# List of variables}
\StringTok{    }\KeywordTok{lm}\NormalTok{(R1M_Usd }\OperatorTok{~}\StringTok{ }\NormalTok{. , }\DataTypeTok{data =}\NormalTok{ .) }\OperatorTok{%>%}\StringTok{        }\CommentTok{# Model: predict R1M_Usd}
\StringTok{    }\KeywordTok{tidy}\NormalTok{() }\OperatorTok{%>%}\StringTok{                            }\CommentTok{# Put output in clean format}
\StringTok{    }\KeywordTok{filter}\NormalTok{(}\KeywordTok{abs}\NormalTok{(statistic) }\OperatorTok{>}\StringTok{ }\DecValTok{3}\NormalTok{)  }\OperatorTok{%>%}\StringTok{       }\CommentTok{# Keep highly significant predictors only}
\StringTok{    }\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{(}\DataTypeTok{booktabs =} \OtherTok{TRUE}\NormalTok{,}
                 \DataTypeTok{caption =} \StringTok{"Significant predictors in the training sample."}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{table}[t]

\caption{\label{tab:regbroom}Significant predictors in the training sample.}
\centering
\begin{tabular}{lrrrr}
\toprule
term & estimate & std.error & statistic & p.value\\
\midrule
(Intercept) & 0.0405741 & 0.0053427 & 7.594323 & 0.0000000\\
Ebitda\_Margin & 0.0132374 & 0.0034927 & 3.789999 & 0.0001507\\
Ev\_Ebitda & 0.0068144 & 0.0022563 & 3.020213 & 0.0025263\\
Fa\_Ci & 0.0072308 & 0.0023465 & 3.081471 & 0.0020601\\
Fcf\_Bv & 0.0250538 & 0.0051314 & 4.882465 & 0.0000010\\
\addlinespace
Fcf\_Yld & -0.0158930 & 0.0037359 & -4.254126 & 0.0000210\\
Mkt\_Cap\_12M\_Usd & 0.2047383 & 0.0274320 & 7.463476 & 0.0000000\\
Mkt\_Cap\_6M\_Usd & -0.1797795 & 0.0459390 & -3.913443 & 0.0000910\\
Mom\_5M\_Usd & -0.0186690 & 0.0044313 & -4.212972 & 0.0000252\\
Mom\_Sharp\_11M\_Usd & 0.0178174 & 0.0046948 & 3.795131 & 0.0001476\\
\addlinespace
Ni & 0.0154609 & 0.0044966 & 3.438361 & 0.0005854\\
Ni\_Avail\_Margin & 0.0118135 & 0.0038614 & 3.059359 & 0.0022184\\
Ocf\_Bv & -0.0198113 & 0.0052939 & -3.742277 & 0.0001824\\
Pb & -0.0178971 & 0.0031285 & -5.720637 & 0.0000000\\
Pe & -0.0089908 & 0.0023539 & -3.819565 & 0.0001337\\
\addlinespace
Sales\_Ps & -0.0157856 & 0.0046278 & -3.411062 & 0.0006472\\
Vol1Y\_Usd & 0.0114250 & 0.0027923 & 4.091628 & 0.0000429\\
Vol3Y\_Usd & 0.0084587 & 0.0027952 & 3.026169 & 0.0024771\\
\bottomrule
\end{tabular}
\end{table}

\normalsize

In fact, since there are several indicators for the market
capitalization and maybe only one would suffice, but it is not obvious
to tell which one is the best choice.

In the remainder of the chapter, we present two approaches that help
reduce the number of predictors:\\
- the first one aims at creating new variables that are uncorrelated
with each other. Low correlation is favorable from an algorithmic point
of view, but the new variables lack interpretability;\\
- the second one gathers predictors into homogeneous clusters and only
one feature should be chosen out of this cluster. Here the rationale is
reversed: interpretability is favored over statistical properties
because the resulting set of features may still include high
correlations, albeit to a lesser point compared to the original one.

\hypertarget{principal-component-analysis-and-autoencoders}{%
\section{Principal component analysis and
autoencoders}\label{principal-component-analysis-and-autoencoders}}

The first method is a cornerstone in dimensionality reduction. It seeks
to determine a smaller number of factors (\(K'<K\)) such that:\\
i) the level of explanatory power remains as high as possible;\\
ii) the resulting factors are linear combinations of the original
variables;\\
iii) the resulting factors are orthogonal.

\hypertarget{a-bit-of-algebra}{%
\subsection{A bit of algebra}\label{a-bit-of-algebra}}

In this short subsection, we define some key concepts that are required
to fully understand the derivation of PCA. Henceforth, we work with
matrices (in bold fonts). An \(I \times K\) matrix \(\textbf{X}\) is
orthonormal if \(I> K\) and \(\textbf{X}'\textbf{X}=\textbf{I}_K\). When
\(I=K\), the (square) matrix is called orthogonal and
\(\textbf{X}'\textbf{X}=\textbf{X}\textbf{X}'=\textbf{I}_K\), i.e.,
\(\textbf{X}^{-1}=\textbf{X}'\).

One foundational result in matrix theory is the Singular Value
Decomposition (SVD, see, e.g., Chapter 5 in \citet{meyer2000matrix}).
The SVD is formulated as follows: any \(I \times K\) matrix
\(\textbf{X}\) can be decomposed into \begin{equation}
\label{eq:svd}
\textbf{X}=\textbf{U} \boldsymbol{\Delta} \textbf{V}',
\end{equation} where \(\textbf{U}\) (\(I\times I\)) and \(\textbf{V}\)
(\(K \times K\)) are orthogonal and \(\boldsymbol{\Delta}\)
(\(I\times K\)) is diagonal, i.e. \(\Delta{i,k}=0\) whenever
\(i\neq k\). In addition, \(\Delta{i,i}\ge 0\): the diagonal terms of
\(\boldsymbol{\Delta}\) are nonnegative.

For simplicity, we assume below that
\(\textbf{1}_I'\textbf{X}=\textbf{0}_K'\), i.e., that all columns have
zero sum (and hence zero mean).\footnote{In practice, this is not a
  major problem: since we work with features that are uniformly
  distributed, de-meaning amounts to remove 0.5 to all feature values.}
This allows to write that the covariance matrix is equal to its sample
estimate \(\boldsymbol{\Sigma}_X= \frac{1}{I-1}\textbf{X}'\textbf{X}\).

One crucial feature of covariance matrices is their symmetry. Indeed,
real-valued symmetric (square) matrices enjoy a SVD which is much more
powerful: when \(\textbf{X}\) is symmetric, there exist an orthogonal
matrix \(\textbf{Q}\) and a diagonal matrix \(\textbf{D}\) such that
\begin{equation}
\label{eq:diagonaliz} 
\textbf{X}=\textbf{Q}\textbf{DQ}'.
\end{equation} This process is called diagonalization (see Chapter 7 in
\citet{meyer2000matrix}) and conveniently applies to covariance
matrices.

\hypertarget{pca}{%
\subsection{PCA}\label{pca}}

The goal of PCA is to build a dataset \(\tilde{\textbf{X}}\) that has
fewer columns but that keeps as much information compared to the
original one, \(\textbf{X}\). The key notion is the change of base,
which is a linear transformation of \(\textbf{X}\) into \(\textbf{Y}\),
a matrix with identical dimension, via \[\textbf{Y}=\textbf{XP},\] where
\(P\) is a \(K \times K\) matrix. There are of course an infinite number
of ways to transform \(\textbf{X}\) into \(\textbf{Y}\), but two
fundamental constraints help reduce the possibilities. The first
constraint is that the columns of \(\textbf{Y}\) be uncorrelated. Having
uncorrelated features is desirable because they then all tell different
stories and have zero redundancy. The second constraint is that the
variance of the columns of \(\textbf{Y}\) is highly concentrated. This
means that a few factors (columns) will capture most of the explanatory
power (signal) while most (the others) will consist predominantly of
noise. All of this is coded in the covariance matrix of
\(\textit{Y}\):\\
- the first condition imposes that the covariance matrix be diagonal;\\
- the second condition imposes that the diagonal elements, when ranked
in decreasing magnitude, see their value decline (sharply if possible).

The covariance matrix of \(\textbf{Y}\) is \begin{equation}
\label{eq:covy} 
\boldsymbol{\Sigma}_Y=\frac{1}{I-1}\textbf{Y}'\textbf{Y}=\frac{1}{I-1}\textbf{P}'\textbf{X}'\textbf{XP}=\frac{1}{I-1}\textbf{P}'\boldsymbol{\Sigma}_X\textbf{P}
\end{equation}

In this expression, we plug the decomposition \eqref{eq:diagonaliz} of
\(\boldsymbol{\Sigma}_X\):
\[\boldsymbol{\Sigma}_Y=\frac{1}{I-1}\textbf{P}'\textbf{Q}\textbf{DQ}'\textbf{P},\]
thus picking \(\textbf{P}=\textbf{Q}\), we get
\(\boldsymbol{\Sigma}_Y=\frac{1}{I-1}\textbf{D}\), that is, a diagonal
covariance matrix for \(\textbf{Y}\). The columns of \(\textbf{Y}\) can
then be re-shuffled in decreasing order of variance so that the diagonal
elements of \(\boldsymbol{\Sigma}_Y\) progressively shrink. This is
useful because it helps locate the factors with most informational
content (the first factors). In the limit, a constant vector (with zero
variance) carries no signal.

The matrix \(\textbf{Y}\) is a linear transformation of \(\textbf{X}\),
thus, it is expected to carry the same information, even though this
information is coded differently. Since the columns are ordered
according to their relative importance, it is simple to omit some of
them. The new set of features \(\tilde{\textbf{X}}\) consists in the
first \(K'\) (with \(K'<K\)) columns of \(\textbf{Y}\).

Below, we show how to perform PCA and visualize the output with the
\emph{factoextra} package. To ease readability, we use the smaller
sample with few predictors.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca <-}\StringTok{ }\NormalTok{training_sample }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{select}\NormalTok{(features_short) }\OperatorTok{%>%}\StringTok{    }\CommentTok{# Smaller number of predictors}
\StringTok{    }\KeywordTok{prcomp}\NormalTok{()                      }\CommentTok{# Performs PCA}
\NormalTok{pca                               }\CommentTok{# Show the result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Standard deviations (1, .., p=7):
## [1] 0.4536601 0.3344080 0.2994393 0.2452000 0.2352087 0.2010782 0.1140988
## 
## Rotation (n x k) = (7 x 7):
##                         PC1         PC2         PC3         PC4
## Div_Yld          0.27159946 -0.57909866  0.04572501 -0.52895604
## Eps              0.42040708 -0.15008243 -0.02476659  0.33737265
## Mkt_Cap_12M_Usd  0.52386846  0.34323935  0.17228893  0.06249528
## Mom_11M_Usd      0.04723846  0.05771359 -0.89715955  0.24101481
## Ocf              0.53294744  0.19588990  0.18503939  0.23437100
## Pb               0.15241340  0.58080620 -0.22104807 -0.68213576
## Vol1Y_Usd       -0.40688963  0.38113933  0.28216181  0.15541056
##                         PC5          PC6          PC7
## Div_Yld         -0.22662581 -0.506566090  0.032011635
## Eps              0.77137719 -0.301883295  0.011965041
## Mkt_Cap_12M_Usd -0.25278113 -0.002987057  0.714319417
## Mom_11M_Usd     -0.25055884 -0.258476580  0.043178747
## Ocf             -0.35759553 -0.049015486 -0.676866120
## Pb               0.30866476 -0.038674594 -0.168799297
## Vol1Y_Usd       -0.06157461 -0.762587677  0.008632062
\end{verbatim}

\normalsize

The rotation gives the matrix \(\textbf{P}\): it's the tool that changes
the base. The first row of the output indicates the standard deviation
of each new factor (column). Each factor is indicated via a PC index
(principal component). Often, the first PC loads positively on all
initial columns: a convex weighted average of all features is expected
to carry a lot of information. In the above example, it is almost the
case, with the exception of volatility, which has a negative coefficient
in the first PC.

Sometimes, it can be useful to visualize the way the principal
components are builts. We show one popular representation that is used
for two factors (usually the first two).

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(factoextra)                      }\CommentTok{# Package for PCA visualization}
\KeywordTok{fviz_pca_var}\NormalTok{(pca,                        }\CommentTok{# Source of PCA decomposition}
             \DataTypeTok{col.var=}\StringTok{"contrib"}\NormalTok{,          }
             \DataTypeTok{gradient.cols =} \KeywordTok{c}\NormalTok{(}\StringTok{"#00AFBB"}\NormalTok{, }\StringTok{"#E7B800"}\NormalTok{, }\StringTok{"#FC4E07"}\NormalTok{),}
             \DataTypeTok{repel =} \OtherTok{TRUE}                \CommentTok{# Avoid text overlapping}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=330px,height=200px]{ML_factor_files/figure-latex/pca2-1} \end{center}

\normalsize

The plot shows that no initial factor has negative signs for the first
two principal components. Volatility is negative for the first one and
earnings per share and dividend yield are negative for the second. The
numbers indicated along the axes are the proportion of explained
variance of each PC. Compared to the figures in the first line of the
output, the numbers are squared and then divided by the total sum of
squares.

Once the rotation is known, it is possible to select a subsample of the
transformed data. From the original 7 features, it is easy to pick just
4.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{training_sample }\OperatorTok{%>%}\StringTok{                                  }\CommentTok{# Start from large sample}
\StringTok{    }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(features_short) }\OperatorTok{%>%}\StringTok{                }\CommentTok{# Keep only 7 features}
\StringTok{    }\KeywordTok{as.matrix}\NormalTok{() }\OperatorTok{%>%}\StringTok{                                  }\CommentTok{# Transform in matrix}
\StringTok{    }\KeywordTok{multiply_by_matrix}\NormalTok{(pca}\OperatorTok{$}\NormalTok{rotation[,}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{]) }\OperatorTok{%>%}\StringTok{       }\CommentTok{# Rotate via PCA (first 4 columns of P)}
\StringTok{    `}\DataTypeTok{colnames<-}\StringTok{`}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"PC1"}\NormalTok{, }\StringTok{"PC2"}\NormalTok{, }\StringTok{"PC3"}\NormalTok{, }\StringTok{"PC4"}\NormalTok{)) }\OperatorTok{%>%}\StringTok{  }\CommentTok{# Change column names}
\StringTok{    }\KeywordTok{head}\NormalTok{()                                           }\CommentTok{# Show first 6 lines}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            PC1       PC2         PC3       PC4
## [1,] 0.3989674 0.7578132 -0.13915223 0.3132578
## [2,] 0.4284697 0.7587274 -0.40164338 0.3745255
## [3,] 0.5215295 0.5679119 -0.10533870 0.2574949
## [4,] 0.5445359 0.5335619 -0.08833864 0.2281793
## [5,] 0.5672644 0.5339749 -0.06092424 0.2320938
## [6,] 0.5871306 0.6420126 -0.44566482 0.3075399
\end{verbatim}

\normalsize

These 4 factors can then be used as orthogonal features in any ML
engine. The fact that the features are uncorrelated is undoubtedly an
asset. But the price of this convenience is high: the features are no
longer immediately interpretable. De-correlating the predictors adds yet
another layer of ``\emph{blackbox-ing}'' in the algorithm.

\hypertarget{ae}{%
\subsection{Autoencoders}\label{ae}}

In a PCA, the coding from \(\textbf{X}\) to \(\textbf{Y}\) is
straightfoward, linear and works both ways:
\[\textbf{Y}=\textbf{X}\textbf{P} \quad \text{and} \quad \textbf{X}=\textbf{YP}',\]
so that we recover \(\textbf{X}\) from \(\textbf{Y}\). This can be
writen differently: \begin{equation}
\label{eq:pcascheme}
\textbf{X} \quad \overset{\text{encode via }\textbf{P}}{\longrightarrow} \quad \textbf{Y} \quad \overset{\text{decode via } \textbf{P}'}{\longrightarrow} \quad \textbf{X}
\end{equation}

If we take the truncated version and seek a smaller output (with only
\(K'\) columns), this gives:

\begin{equation}
\label{eq:pcaschem2}
\textbf{X}, \ (I\times K) \quad \overset{\text{encode via }\textbf{P}_{K'}}{\longrightarrow} \quad \tilde{\textbf{X}}, \ (I \times K') \quad \overset{\text{decode via } \textbf{P}'_{K'}}{\longrightarrow} \quad \breve{\textbf{X}},\ (I \times K),
\end{equation}

where \(\textbf{P}_{K'}\) is the restriction of \(\textbf{P}\) to the
\(K'\) columns that correspond to the factors with the largest
variances. The dimensions of matrices are indicated inside the brackets.
In this case, the recoding cannot recover \(\textbf{P}\) exactly but
only an approximation, which we write \(\breve{\textbf{X}}\). This
approximation is coded with less information, hence this new data
\(\breve{\textbf{X}}\) is compressed and provides a parcimonious
representation of the original sample \(\textbf{X}\).

An autoencodeur generalizes this concept to nonlinear coding functions.
Simple linear autoencoders are linked to latent factor models (see
Proposition 1 in \cite{gu2019autoencoder} for the case of single layer
autoencoders.) The scheme is the following \begin{equation}
\label{eq:aescheme2}
\textbf{X},\ (I\times K) \quad \overset{\text{encode via } N} {\longrightarrow} \quad \tilde{\textbf{X}}=N(\textbf{X}), \ (I \times K') \quad \overset{\text{decode via } N'}{\longrightarrow} \quad \breve{\textbf{X}}=N'(\tilde{\textbf{X}}), \ (I \times K),
\end{equation}

where the encoding and decoding functions \(N\) and \(N'\) are often
taken to be neural networks. The term \textbf{auto-encoder} comes from
the fact that the target output, which we often write \(\textbf{Y}\) is
the original sample \(\textbf{X}\). Thus, the algorithm seek to
determine the function \(N\) that minimizes the distance (to be defined)
between \(\textbf{X}\) and the output value \(\breve{\textbf{X}}\). The
encoder generates an alternative representation of \(\textbf{X}\) while
the decoder tries to recode it back to its original values. Naturally,
the intermediate (coded) version \(\tilde{\textbf{X}}\) is targeted to
have a smaller dimension compared to \(\textbf{X}\).

\hypertarget{application}{%
\subsection{Application}\label{application}}

Auto-encoders are easy to code in Keras. To underline the power of the
framework, we resort to another way of coding a NN: the so-called
functional API. For simplicity, we work with the small number of
predictors (7). The structure of the network consists of two symmetric
networks with only one intermediate layer containing 32 units. The
activation function is sigmoid: this makes sense since the input have
values in the unit interval.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{input_layer <-}\StringTok{ }\KeywordTok{layer_input}\NormalTok{(}\DataTypeTok{shape =} \KeywordTok{c}\NormalTok{(}\DecValTok{7}\NormalTok{))    }\CommentTok{# features_short has 7 columns }

\NormalTok{encoder <-}\StringTok{ }\NormalTok{input_layer }\OperatorTok{%>%}\StringTok{       }\CommentTok{# First, encode}
\StringTok{    }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{32}\NormalTok{, }\DataTypeTok{activation =} \StringTok{"sigmoid"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{4}\NormalTok{)       }\CommentTok{# 4 dimensions for the output layer (same as PCA example)}

\NormalTok{decoder <-}\StringTok{ }\NormalTok{encoder }\OperatorTok{%>%}\StringTok{           }\CommentTok{# Then, from encoder, decode}
\StringTok{    }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{32}\NormalTok{, }\DataTypeTok{activation =} \StringTok{"sigmoid"}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{layer_dense}\NormalTok{(}\DataTypeTok{units =} \DecValTok{7}\NormalTok{)       }\CommentTok{# the original sample has 7 features}
\end{Highlighting}
\end{Shaded}

\normalsize

In the training part, we optimize the MSE and use an Adam update of the
weights (see Section \ref{backprop}).

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ae_model <-}\StringTok{ }\KeywordTok{keras_model}\NormalTok{(}\DataTypeTok{inputs =}\NormalTok{ input_layer, }\DataTypeTok{outputs =}\NormalTok{ decoder) }\CommentTok{# Builds the model}

\NormalTok{ae_model }\OperatorTok{%>%}\StringTok{ }\KeywordTok{compile}\NormalTok{(                }\CommentTok{# Learning parameters}
    \DataTypeTok{loss =} \StringTok{'mean_squared_error'}\NormalTok{,}
    \DataTypeTok{optimizer =} \StringTok{'adam'}\NormalTok{,}
    \DataTypeTok{metrics =} \KeywordTok{c}\NormalTok{(}\StringTok{'mean_absolute_error'}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\normalsize

Finally, we are ready to train the data onto itself!

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit_ae <-}\StringTok{ }\NormalTok{ae_model }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{fit}\NormalTok{(training_sample }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(features_short) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(),  }\CommentTok{# Input}
\NormalTok{        training_sample }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(features_short) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(),  }\CommentTok{# Output}
        \DataTypeTok{epochs =} \DecValTok{15}\NormalTok{, }\DataTypeTok{batch_size =} \DecValTok{512}\NormalTok{,}
        \DataTypeTok{validation_data =} \KeywordTok{list}\NormalTok{(testing_sample }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(features_short) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(), }
\NormalTok{                               testing_sample }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(features_short) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{())}
\NormalTok{    )}
\KeywordTok{plot}\NormalTok{(fit_ae) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_grey}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=400px]{ML_factor_files/figure-latex/aekeras3-1} 

}

\caption{Output from the training of an auto-encoder.}\label{fig:aekeras3}
\end{figure}

\normalsize

In order to get the details of all weights and biases, the syntax is the
following.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ae_weights <-}\StringTok{ }\NormalTok{ae_model }\OperatorTok{%>%}\StringTok{ }\KeywordTok{get_weights}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\normalsize

Retrieving the encoder and processing the data into the compressed
format is just a matter of matrix manipulation (see exercise below).

\hypertarget{clustering-via-k-means}{%
\section{Clustering via k-means}\label{clustering-via-k-means}}

The second family of unsupervised tools pertains to clustering. Features
are grouped into homogeneous families of predictors. It is then possible
to single out one among the group (or to create a synthetic average of
all of them). Mechanically, the number of predictors is reduced.

The principle is simple: among a group of variables (the reasoning would
be the same for observations in the other dimension)
\(\textbf{x}_{\{1 \le j \le J\}}\), find the combination of \(k<J\)
groups that minimize \begin{equation}
\label{eq:kmeans}
\sum_{i=1}^k\sum_{\textbf{x}\in S_i}||\textbf{x}-\textbf{m}_i||^2,
\end{equation} where \(||\cdot ||\) is some norm which is usually taken
to be the Euclidean \(l^2\)-norm. The \(S_i\) are the groups and the
minimization is run on the whole set of groups \(\textbf{S}\). The
\(\textbf{m}_i\) are the group means (also called centroids or
barycenters):
\(\textbf{m}_i=(\text{card}(S_i))^{-1}\sum_{\textbf{x}\in S_i}\textbf{x}\).

In order to ensure optimality, all possible arrangements must be tested,
which is prohibitively long when \(k\) and \(J\) are large. Therefore,
the problem is usually solved with greedy algorithms that seek (and
find) solutions that are not optimal but `good enough'.

One heuristic way to proceed is the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{-1}
\tightlist
\item
  Start with a (possibly random) partition of \(k\) clusters.\\
\item
  For each cluster, compute the optimal mean values \(\textbf{m}_i^*\)
  that minimizes expression \eqref{eq:kmeans}. This is a simple quadratic
  program.\\
\item
  Given the optimal centers \(\textbf{m}_i^*\), reassign the points
  \(\textbf{x}_i\) so that they are all the closest to their center.\\
\item
  Repeat steps 1. and 2. until the points do not change cluster at step
  2.
\end{enumerate}

Below, we illustrate this process with an example. From all 93 features,
we build 10 clusters.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)                               }\CommentTok{# Setting the random seed (the optimization is random)}
\NormalTok{k_means <-}\StringTok{ }\NormalTok{training_sample }\OperatorTok{%>%}\StringTok{             }\CommentTok{# Performs the k-means clustering}
\StringTok{    }\KeywordTok{select}\NormalTok{(features) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{as.matrix}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{t}\NormalTok{() }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{kmeans}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\NormalTok{clusters <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{factor =} \KeywordTok{names}\NormalTok{(k_means}\OperatorTok{$}\NormalTok{cluster),   }\CommentTok{# Organize the cluster data}
                   \DataTypeTok{cluster =}\NormalTok{ k_means}\OperatorTok{$}\NormalTok{cluster) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{arrange}\NormalTok{(cluster)}
\NormalTok{clusters }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(cluster }\OperatorTok{==}\StringTok{ }\DecValTok{4}\NormalTok{)                     }\CommentTok{# Shows one particular group}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 20 x 2
##    factor                         cluster
##    <chr>                            <int>
##  1 Asset_Turnover                       4
##  2 Bb_Yld                               4
##  3 Ebit_Bv                              4
##  4 Ebit_Noa                             4
##  5 Ebit_Oa                              4
##  6 Ebit_Ta                              4
##  7 Eps                                  4
##  8 Eps_Basic                            4
##  9 Eps_Basic_Gr                         4
## 10 Eps_Contin_Oper                      4
## 11 Eps_Dil                              4
## 12 Ni_Oa                                4
## 13 Op_Prt_Margin                        4
## 14 Pb                                   4
## 15 Recurring_Earning_Total_Assets       4
## 16 Return_On_Capital                    4
## 17 Roa                                  4
## 18 Roc                                  4
## 19 Roce                                 4
## 20 Roe                                  4
\end{verbatim}

\normalsize

We single out the fourth cluster which is composed mainly of accounting
ratios related to the profitability of firms. Given these 10 clusters,
we can build a much smaller group of features that can then be fed to
the predictive engines described in Chapters \ref{lasso} to \ref{bayes}.
The representative of a cluster can be the member that is closest to the
center, or simply the center itself. This pre-processing step can
nonetheless cause problems in the forecasting phase. Typically, it
requires that the training data be also clustered.

\hypertarget{nearest-neighbors}{%
\section{Nearest neighbors}\label{nearest-neighbors}}

To the best of our knowledge, nearest neighbors are not used in large
scale portfolio choice applications. The reason is simple: computational
cost. Nonetheless, the concept of neighbors is widespread in
unsupervised learning and can be used locally in complement to
interpretability tools.

In what follows, we seek to find neighbors of one particular instance
\(\textbf{x}_i\) (a \(K\)-dimensional row vector). Note that there is a
major difference with the previous section: the clustering is intended
at the observation level (row) and not at the predictor level (column).

Given a dataset with the same (corresponding) columns
\(\textbf{X}_{i,k}\), the neighbors are defined via a similarity measure
(or distance) \begin{equation}
\label{eq:D}
D(\textbf{x}_j,\textbf{x}_i)=\sum_{k=1}^Kc_k d_k(x_{j,k},x_{i,k}),
\end{equation} where the distances functions \(d_k\) can operate on
various data types (numerical, categorical, etc.). For numerical values,
\(d_k(x_{j,k},x_{i,k})=(x_{j,k}-x_{i,k})^2\) or
\(d_k(x_{j,k},x_{i,k})=|x_{j,k}-x_{i,k}|\). For categorical values, we
refer to the exhaustive survey by \citet{boriah2008similarity} which
lists 14 possible measures. Finally the \(c_k\) in Equation \eqref{eq:D}
allow some flexbility by weighting features. This is useful because both
raw values (\(x_{i,k}\) versus \(x_{i,k'}\)) or measure outputs (\(d_k\)
versus \(d_{k'}\)) can have different scales.

Once the distances are computed over the whole sample, they are ranked
using indices \(l_1^i, \dots, l_I^i\):
\[D\left(\textbf{x}_{l_1^i},\textbf{x}_i\right) \le D\left(\textbf{x}_{l_2^i},\textbf{x}_i\right) \le \dots, \le D\left(\textbf{x}_{l_I^i},\textbf{x}_i\right)\]

The nearest neighbors are those indexed by \(l_m^i\) for
\(m=1,\dots,k\). We leave out the case when there are problematic
equalities of the type
\(D\left(\textbf{x}_{l_m^i},\textbf{x}_i\right)=D\left(\textbf{x}_{l_{m+1}^i},\textbf{x}_i\right)\)
for the sake of simplicity and because they rarely occur in practice as
long as there are sufficiently many numerical predictors.

Given these neighbors, it is now possible to build a prediction for the
label side \(y_i\). The rationale is straightforward: if
\(\textbf{x}_i\) is close to other instances \(\textbf{x}_j\) then the
label value \(y_i\) should also be close to \(y_j\) (under the
assumption that the features carry some predictive information over the
label \(y\)).

An intuitive prediction for \(y_i\) is the following weighted average:
\[\hat{y}_i=\frac{\sum_{j\neq i} h(D(\textbf{x}_j,\textbf{x}_i)) y_j}{\sum_{j\neq i} h(D(\textbf{x}_j,\textbf{x}_i))},\]
where \(h\) is a decreasing function. Thus, the further \(\textbf{x}_j\)
is from \(\textbf{x}_i\), the smaller the weight in the average. A
typical choice for \(h\) is \(h(z)=e^{-az}\) for some parameter \(a>0\)
that determines how penalizing the distance
\(D(\textbf{x}_j,\textbf{x}_i)\) is. Of course, the average can be taken
in the set of \(k\) nearest neighbors, in which case the \(h\) is equal
to zero beyond a particular distance threshold:
\[\hat{y}_i=\frac{\sum_{j \text{ neighbor}} h(D(\textbf{x}_j,\textbf{x}_i)) y_j}{\sum_{j \text{ neighbor}} h(D(\textbf{x}_j,\textbf{x}_i))}.\]

A more agnostic rule is to take \(h:=1\) over the set of neighbors and
in this case, all neighbors have the same weight (see the old discussion
by \citet{bailey1978note} in the case of classification). For
classification tasks, the procedure involves a voting rule whereby the
class with the most votes wins the contest, with possible tie-breaking
methods. The interested reader can have a look at the short survey
\citet{bhatia2010survey}.

For the choice of optimal \(k\), several complicated techniques and
criteria exists (see, e.g., \citet{ghosh2006optimum} and
\citet{hall2008choice}). Heuristic values often do the job pretty well.
A rule of thumb is that \(k=\sqrt{I}\) (\(I\) being the total number of
instances) is not too far from the optimal value, unless \(I\) is
exceedingly large.

Below, we illustrate this concept. We pick one date (31th of December
2006) and single out one asset (with stock\_id equal to 13). We then
seek to find the \(k=30\) stocks that are the closest to this asset at
this particular date. We resort to the \emph{FNN} package that propose
an efficient computation of Euclidean distances (and their ordering).

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(FNN)     }\CommentTok{# Package for Fast Nearest Neighbors detection}
\NormalTok{knn_data <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(data_ml, date }\OperatorTok{==}\StringTok{ "2006-12-31"}\NormalTok{)    }\CommentTok{# Dataset for k-NN exercise}
\NormalTok{knn_target <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(knn_data, stock_id }\OperatorTok{==}\StringTok{ }\DecValTok{13}\NormalTok{) }\OperatorTok{%>%}\StringTok{   }\CommentTok{# Target observation}
\StringTok{              }\KeywordTok{select}\NormalTok{(features)}
\NormalTok{knn_sample <-}\StringTok{ }\KeywordTok{filter}\NormalTok{(knn_data, stock_id }\OperatorTok{!=}\StringTok{ }\DecValTok{13}\NormalTok{) }\OperatorTok{%>%}\StringTok{   }\CommentTok{# All other observations}
\StringTok{              }\KeywordTok{select}\NormalTok{(features)}
\NormalTok{neighbors <-}\StringTok{ }\KeywordTok{get.knnx}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ knn_sample, }\DataTypeTok{query =}\NormalTok{ knn_target, }\DataTypeTok{k =} \DecValTok{30}\NormalTok{) }
\NormalTok{neighbors}\OperatorTok{$}\NormalTok{nn.index                                   }\CommentTok{# Indices of the k nearest neighbors}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
## [1,]  905  876  730  548 1036  501  335  117  789    54   618   130   342
##      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24]
## [1,]   360   673   153   265   858   830   286  1150   166   946   192
##      [,25] [,26] [,27] [,28] [,29] [,30]
## [1,]   340   162   951   376   785     2
\end{verbatim}

\normalsize

Once the neighbors and distances are known, we can compute a prediction
for the return of the target stock. We use the function \(h(z)=e^{-z}\)
for the weighting of instances (via the distances).

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knn_labels <-}\StringTok{ }\NormalTok{knn_data[neighbors}\OperatorTok{$}\NormalTok{nn.index,] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(R1M_Usd)           }\CommentTok{# y values for the neighbors}
\KeywordTok{sum}\NormalTok{(knn_labels }\OperatorTok{*}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\NormalTok{neighbors}\OperatorTok{$}\NormalTok{nn.dist) }\OperatorTok{/}\StringTok{ }\KeywordTok{sum}\NormalTok{(}\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\NormalTok{neighbors}\OperatorTok{$}\NormalTok{nn.dist)))  }\CommentTok{# Prediction for k(z)=exp(-z)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.003042282
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{filter}\NormalTok{(knn_data, stock_id }\OperatorTok{==}\StringTok{ }\DecValTok{13}\NormalTok{) }\OperatorTok{%>%}\StringTok{                                      }\CommentTok{# True y (to check the error)}
\StringTok{              }\KeywordTok{select}\NormalTok{(R1M_Usd)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   R1M_Usd
##     <dbl>
## 1   0.089
\end{verbatim}

\normalsize

The prediction is neither very good, nor very bad (the sign is
correct!). However, note that this example cannot be used for predictive
purposes because we use data from 2006-12-31 to predict a return at the
same date. In order to avoid the forward looking bias, the knn\_sample
variable should be chosen from a prior point in time.

The above computations are fast (a handful of seconds at most), but hold
for only one asset. In a \(k\)-NN exercise each stock gets a customed
prediction and the set of neighbors must be re-assessed each time. This
is particularly costly in a backtest, especially when several parameters
can be tested (\(k\), \(a\) in the weighting function \(h(z)=e^{-az}\)).

\hypertarget{coding-exercise}{%
\section{Coding exercise}\label{coding-exercise}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Code the compressed version of the data via the autoencoder.
\end{enumerate}

\hypertarget{RL}{%
\chapter{Reinforcement learning}\label{RL}}

Due to its increasing popularity within the Machine Learing community,
we dedicate a chapter to reinforcement learning (RL). In 2019 only, more
than 25 papers dedicated to RL have been submitted to (or updated on)
arXiv under the \textbf{q:fin} (quantitative finance) classification.
Moreover, an early survey of RL-based portfolios is compiled in
\citet{sato2019model} and general financial applications are discussed
in \citet{kolm2019modern} and \citet{meng2019reinforcement}. This shows
that RL has recently gained traction among the quantitative finance
community.\footnote{Like neural networks, reinforcement learning methods
  have also been recently developed for derivatives pricing and hedging,
  see for instance \citet{kolm2019dynamic}.}

XXXXXXXXX

While RL is a framework more than a particular algorithm, its efficient
application in portfolio management is not straightforward, as we will
show.

\hypertarget{theoretical-layout}{%
\section{Theoretical layout}\label{theoretical-layout}}

\hypertarget{general-framework}{%
\subsection{General framework}\label{general-framework}}

In this section, we introduce the core concepts of RL and follow
relatively closely the notations (and layout) of
\citet{sutton2018reinforcement}, which is widely considered as a solid
reference in the field, along with \citet{bertsekas2017dynamic}. One
central tool in the field is called the Markov Decision Process (MDP,
see Chapter 3 in \citet{sutton2018reinforcement}).

MDPs involve the interaction between an agent (e.g., a trader or
portfolio manager) and an environment (e.g., a financial market). The
agent performs actions that may alter the state of environment and gets
a reward (possibly negative) for each action. This short sequence can be
repeated an arbitrary number of times, as is shown in Figure
\ref{fig:mdpscheme}.

\begin{figure}[b]

{\centering \includegraphics[width=500px]{images/MDP_scheme} 

}

\caption{Scheme of Markov Decision Process. R, S and A stand for reward, state and action, respectively.}\label{fig:mdpscheme}
\end{figure}

Given initialized values for the state of the environement (\(S_0\)) and
reward (usually \(R_0=0\)), the agent performs an action (e.g., invests
in some assets). This generates future rewards \(R_1\) (e.g., returns,
profits, Sharpe ratio) and also a future state of the environment
(\(S_1\)). Based on that, the agent performs and new action and the
sequence continues. When the sets of states, actions and rewards are
finite, the MDP is logically called \emph{finite}. In a financial
framework, this is somewhat unrealistic and we discuss this issue later
on. It nevertheless is not hard to think of simplified and discretized
financial problems. For instance, the reward can be binary: win money
versus lose money. In the case of only one asset, the action can also be
dual: investing versus not investing. When the number of assets is
sufficiently small, it is possible to set fixed proportions that lead to
a reasonable number of combinations of portfolio choices, etc.

We pursue our exposé with finite MDPs (they are the most common in the
literature and their formal treatment is simpler). As is often the case
with markovian objects, the key notion is that of transition
probability:

\begin{equation}
\label{eq:transprob}
p(s',r|s,a)=\mathbb{P}\left[S_t=s',R_t=r | S_{t-1}=s,A_{t-1}=a \right],
\end{equation}

which is the probability of reaching state \(s'\) and reward \(r\) at
time \(t\), conditionally on being in state \(s\) and performing action
\(a\) at time \(t-1\). The finite sets of states and actions will be
denoted with \(\mathcal{S}\) and \(\mathcal{A}\) henceforth. Sometimes,
this probability is averaged over the set of rewards which gives the
following decomposition: \begin{align}  \label{eq:transprob2}
\sum_r rp(s',r|s,a)&=\mathcal{P}_{ss'}^a \mathcal{R}_{ss'}^a, \quad \text{ where } \\
\mathcal{P}_{ss'}^a &=\mathbb{P}\left[S_t=s' | S_{t-1}=s,A_{t-1}=a \right],  \quad \text{ and } \nonumber \\
 \mathcal{R}_{ss'}^a &= \mathbb{E}\left[R_t | S_{t-1}=s,S_t=s', A_{t-1}=a \right]. \nonumber
\end{align}

The goal of the agent is to maximize some function of the stream of
rewards. This gain is usually defined as \begin{align}
G_t&=\sum_{k=0}^T\gamma^kR_{t+k+1} \nonumber \\   \label{eq:gain6}
&=R_{t+1} +\gamma G_{t+1},
\end{align}

i.e., it is a discounted version of the reward, where the discount
factor is \(\gamma \in (0,1]\). The horizon \(T\) may be infinite, which
is why \(\gamma\) was originally introduced. Assuming the rewards are
bounded, the infinite sum diverges for \(\gamma=1\), but may converge
when \(\gamma <1\). When \(T\) is finite, the task is called
\emph{episodic} and otherwise, it is said to be \emph{continuous}.

In RL, the focal unknown is the \emph{policy} \(\pi\), which drives the
actions of the agent. More precisely,
\(\pi(a,s)=\mathbb{P}[A_t=a|S_t=s]\), that is, \(\pi\) equals the
probability of taking action \(a\) if the state of the environment is
\(s\). This means that actions are subject to randomness, just like for
mixed strategies in game theory. While this may seem disappointing
because an investor would want to be sure to take \emph{the} best
action, but it is also a good reminder that the best way to face random
outcomes may well be to randomize actions as well.

Finally, in order to try to determine the \emph{best} policy, one key
indicator is the so-called value function: \begin{equation}
\label{eq:RLvalue}
v_\pi(s)=\mathbb{E}_\pi\left[ G_t | S_t=s \right],
\end{equation}

where the time index \(t\) is not very relevant and omitted in the
notation of the function. The index \(\pi\) under the expectation
operator \(\mathbb{E}[\cdot]\) simply indicates that the average is
taken when the policy \(\pi\) is enforced. The value function is simply
equal to the average gain conditionally on the state being equal to
\(s\). In financial terms, this is equivalent to the average profit if
the agents takes action \(a\) when the market environment is \(s\). More
generally, it is also possible to condition not only on the state, but
also on the action taken. We thus introduce the \(q_\pi\) action-value
function: \begin{equation}
\label{eq:RLQ}
q_\pi(s,a)=\mathbb{E}_\pi\left[ G_t | S_t=s, \ A_t=a \right].
\end{equation}

The \(q_\pi\) function is highly important because it gives the average
gain when the state and action are fixed. Hence, if the current state is
known, then one obvious choice is to select the action for which
\(q_\pi(s,\cdot)\) is the highest. Of course, this is the best solution
if the optimal value of \(q_\pi\) is known, which is not always the case
in practice. The value function can easily be accessed via \(q_\pi\):
\(v_\pi(s)=\sum_a \pi(a,s)q_\pi(s,a)\).

The optimal \(v_\pi\) and \(q_\pi\) are straightforwardly defined as
\[v_*(s)=\underset{\pi}{\max} \, v_\pi(s), \ \forall s\in \mathcal{S}, \quad \text{ and } \quad q_*(s,a) =\underset{\pi}{\max} \, q_\pi(s,a), \ \forall (s,a)\in \mathcal{S}\times \mathcal{A}.\]

If only \(v_*(s)\) is known, then the agent must span the set of actions
and find those that yield the maximum value for any given state \(s\).

Finding these optimal values is a very complicated task and many
articles are dedicated to solving this challenge. One reason why finding
the best \(q_\pi(s,a)\) is difficult is because it depends on two
elements (\(s\) and \(a\)) on one side and \(\pi\) on the other.
Usually, for a fixed policy \(\pi\), it can be time consuming to
evaluate \(q_\pi(s,a)\) for a given stream of actions, states and
rewards. Once \(q_\pi(s,a)\) is estimated, then a new policy \(\pi'\)
must be tested and evaluated to determine if it is better than the
original one. Thus, this iterative search for a good policy can take
long. For more details on policy improvement and value function
updating, we recommend Chapter 4 of \citet{sutton2018reinforcement}
which is dedicated to dynamic programming.

\hypertarget{q-learning}{%
\subsection{Q-learning}\label{q-learning}}

An interesting shortcut to the problem of finding \(v_*(s)\) and
\(q_*(s,a)\) is to remove the dependence on the policy. Consequently,
there is then of course no need to iteratively improve the policy. The
central relationship that is required to do this is the so-called
Bellman equation that is satisfies by \(q_\pi(s,a)\). We detail its
derivation below. First of all, we recall that \begin{align*}
q_\pi(s,a) &= \mathbb{E}_\pi[G_t|S_t=s,A_t=a] \\
&= \mathbb{E}_\pi[R_{t+1}+ \gamma G_{t+1}|S_t=s,A_t=a],
\end{align*} where the second equality stems from \eqref{eq:gain6}. The
expression \(\mathbb{E}_\pi[R_{t+1}|S_t=s,A_t=a]\) can be further
decomposed. Since the expectation runs over \(\pi\), we need to sum over
all possible actions \(a'\) and states \(s'\) and resort to
\(\pi(a',s')\). In addition, the sum on the \(s'\) and \(r\) arguments
of the probability
\(p(s',r|s,a)=\mathbb{P}\left[S_{t+1}=s',R_{t+1}=r | S_t=s,A_t=a \right]\)
gives access to the distribution of the random couple
\((S_{t+1},R_{t+1})\) so that in the end
\(\mathbb{E}_\pi[R_{t+1}|S_t=s,A_t=a]=\sum_{a', r,s'}\pi(a',s')p(s',r|s,a) r\).
A similar reasoning applies to the second portion of \(q_\pi\) and:
\begin{align}
q_\pi(s,a) &=\sum_{a',r, s'}\pi(a',s')p(s',r|s,a) \left[ r+\gamma \mathbb{E}_\pi[ G_{t+1}|S_t=s',A_t=a']\right] \nonumber \\  \label{eq:bellman}
&=\sum_{a',r,s'}\pi(a',s')p(s',r|s,a) \left[ r+\gamma q_\pi(s',a')\right].
\end{align}

This equation links \(q_\pi(s,a)\) to the future \(q_\pi(s',a')\) from
the states and actions \((s',a')\) that are accessible from \((s,a)\).

Notably, Equation \eqref{eq:bellman} is also true for the optimal
action-value function \(q_*=\underset{\pi}{\max} \, q_\pi(s,a)\):

\begin{align}
q_*(s,a) &=\underset{a'}{\max} \sum_{r,s'}p(s',r|s,a) \left[ r+\gamma q_*(s',a')\right], \nonumber \\ \label{eq:bellmanq}
&= \mathbb{E}_{\pi^*}[r|s,a]+ \gamma \, \sum_{r,s'}p(s',r|s,a)\left(  \underset{a'}{\max} q_*(s',a') \right) 
\end{align}

because one optimal policy is one that maximizes \(q_\pi(s,a)\), for a
given state \(s\) and over all possible actions \(a\). This expression
is central to a cornerstone algorithm in reinforcement learning called
\(Q\)-learning (the formal proof of convergence is outlined in
\citet{watkins1992q}). In \(Q\)-learning, the state-action function does
no longer depend on policy and is written with capital \(Q\). The
process is the following:

Initialize values \(Q(s,a)\) for all states \(s\) and actions \(a\). For
each episode:\\

\begin{tabular}{l}
0. Initialize state $S_0$ and for each iteration $i$ until the end of the episode:   \\
1. observe state $s_i$;    \\
2. perform action $a_i$ (depending on $Q$);   \\
3. receive reward $r_{i+1}$ and observe state $s_{i+1}$  \\
4. Update $Q$ as follows: 
\end{tabular}

\begin{equation}
\label{eq:QLupdate}
Q_{i+1}(s_i,a_i) \longleftarrow Q_i(s_i,a_i) + \eta  \left(\underbrace{r_{i+1}+\gamma \, \underset{a}{\max} \, Q_i(s_{i+1},a)}_{\text{echo of } (\ref{eq:bellmanq})}-Q_i(s_i,a_i) \right)
\end{equation}

The underlying reason why this update rule works can be linked to fixed
point theorems of contraction mappings. If a function \(f\) satisfies
\(|f(x)-f(y)|< \delta |x-y|\) (Lipshitz continuity), then a fixed point
\(z\) satisfying \(f(z)=z\) can be iteratively obtained via
\(z \leftarrow f(z)\). This updating rule converges to the fixed point.
Equation \eqref{eq:bellmanq} can be solved using a similar principle,
except that a learning rate \(\eta\) slows the learning process (but
also technically ensures convergence under technical assumptions).

More generally, \eqref{eq:QLupdate} has a form that is widespread in
reinforcement learning that is summarized in Equation (2.4) of
\citet{sutton2018reinforcement}: \begin{equation}
\label{eq:RLeq}
\text{New estimate} \leftarrow \text{Old estimate + Step size (}i.e., \text{ learning rate)} \times (\text{Target - Old estimate}),
\end{equation}

where the last part can be viewed as an error term. Starting from the
old estimate, the new estimate therefore goes in the `right' (or sought)
direction, modulo a discount term that makes sure that the magnitude of
this direction is not too large. The update rule in \eqref{eq:QLupdate} is
often referred to as `\emph{temporal difference}' learning because it is
driven by the improvement yielded by estimates that are known at time
\(t+1\) (target) versus those known at time \(t\).

One important step of the \emph{Q}-learning sequence is the second one
where the action \(a_i\) is picked. In RL, the best algorithms combine
two features: exploitation and exploration. Exploitation is when the
machine uses the currrent information at its disposal to choose the next
action. In this case, for a given state \(s_i\), it chooses the action
\(a_i\) that maximizes the expected reward \(Q_i(s_i,a_i)\). While
obvious, this choice is not optimal if the current function \(Q_i\) is
relatively far from the \emph{true} \(Q\). Indeed: decisions (actions)
are bound to be suboptimal and repeated many times.

In order to gather new information stemming from actions that have not
been tested much (but that can potentially generate higher rewards),
exploration is needed. This is when an action \(a_i\) is chosen
randomly. The most common way to combine these two concepts is called
\(\epsilon\)-greedy exploration. The action \(a_i\) is assigned
according to:

\begin{equation}
\label{eq:egreedy}
a_i=\left\{ \begin{array}{c l}
\underset{a}{\text{argmax}} \ Q_i(s_i,a) & \text{ with probability } 1-\epsilon \\
\text{randomly (uniformly) over } \mathcal{A} & \text{ with probability } \epsilon
\end{array}\right. .
\end{equation}

Thus, with probability \(\epsilon\), the algorithm explores and with
probability \(1-\epsilon\), it exploits the current knowledge of the
expected reward and picks the best action. Because all actions have a
non-zero probability of being chosen, the policy is called ``soft''.
Indeed, then best action has a probability of selection equal to
\(1-\epsilon(1-\text{card}(\mathcal{A})^{-1})\) while all other actions
are picked with probability \(\epsilon/\text{card}(\mathcal{A})\).

\hypertarget{sarsa}{%
\subsection{SARSA}\label{sarsa}}

In \(Q\)-learning, the algorithm seeks to find the action-value function
of the optimal policy. Thus, the policy that is followed to pick actions
is different from the one that is learned (via \(Q\)). Such algorithms
are called \emph{off-policy}. \emph{On-policy} algorithms seek to
improve the estmation of the action-value function \(q_\pi\) by
continuously acting according to the policy \(\pi\). One canonical
example of on-policy learning is the SARSA method which requires two
consecutive states and actions \textbf{SA}R\textbf{SA}. The way the
quintuple \((S_t,A_t,R_{t+1}, S_{t+1}, A_{t+1})\) is processed is
presented below.

The main difference between \(Q\) learning and SARSA is the update rule.
In SARSA, it is given by \begin{equation}
\label{eq:SARSAupdate}
Q_{i+1}(s_i,a_i) \longleftarrow Q_i(s_i,a_i) + \eta  \left(r_{i+1}+\gamma \, Q_i(s_{i+1},a_{i+1})-Q_i(s_i,a_i) \right)
\end{equation}

The improvement comes only from the local point \(Q_i(s_{i+1},a_{i+1})\)
that is based on the new states and actions (\(s_{i+1},a_{i+1}\)),
whereas in \(Q\)-learning, it comes from all possible actions of which
only the best is retained \(\underset{a}{\max} \, Q_i(s_{i+1},a)\).

A more robust (but computationally demanding) version of SARSA is
\emph{expected} SARSA in which the target \(Q\) function is averaged
over all actions: \begin{equation}
\label{eq:exSARSAupdate}
Q_{i+1}(s_i,a_i) \longleftarrow Q_i(s_i,a_i) + \eta  \left(r_{i+1}+\gamma \, \sum_a \pi(a,s_{i+1}) Q_i(s_{i+1},a) -Q_i(s_i,a_i) \right)
\end{equation}

Expected SARSA is less volatile than SARSA because the latter is
strongly impacted by the random choice of \(a_{i+1}\). In expected
SARSA, the average smoothes the learning process.

\hypertarget{issues-and-potential-solutions}{%
\section{Issues and potential
solutions}\label{issues-and-potential-solutions}}

\hypertarget{the-curse-of-dimensionality}{%
\subsection{The curse of
dimensionality}\label{the-curse-of-dimensionality}}

Reinforcement learning is a framework and not a particular algorithm. In
fact, different tools can very well co-exist in a RL task (AlphaGo
combined both tree methods and neural networks, see
\citet{silver2016mastering}). Nonetheless, any RL attempt will always
rely on the three key concepts that are: the states, the actions and the
rewards. In factor investing, they fairly easy to identify, though there
is always room for interpretation. Actions are evidently defined by
portfolio compositions. The states can be viewed as the current values
that describe the economy: as a first order approximation, it can be
assumed that the feature levels fulfill this role (possibly conditioned
or complemented with macro-economic data). The rewards are even more
straightforward. Returns or any relevant performance metric (e.g.,
Sharpe ratio which is for instance used in \citet{moody1998performance},
\citet{bertoluzzo2012testing} and \citet{aboussalah2020continuous} or
drawdown-based ratios, as in \citet{almahdi2017adaptive}) can account
for rewards.

A major problem lies in the dimensionality of both states and actions.
Assuming an absence of leverage (no negative weights), the actions take
values on the simplex \begin{equation}
\label{eq:simplex}
\mathbb{S}_N=\left\{ \mathbf{x} \in \mathbb{R}^N\left|\sum_{n=1}^Nx_n=1, \ x_n\ge 0, \ \forall n=1,\dots,N \right.\right\}
\end{equation} and assuming that all features have been uniformized,
their space is \([0,1]^{NK}\). Needless to say, the dimensions of both
spaces are numerically impractical.

A simple solution to this problem is discretization: each space is
divided into a small number of categories. Some authors do take this
route. In \citet{yang2018investor}, the state space is discretized into
three values depending on volatility, and actions are also split into
three categories. \citet{bertoluzzo2012testing} and
\citet{xiong2018practical} also choose three possible actions (buy,
hold, sell). In \citet{almahdi2019constrained}, the learner is expected
to yield binary signals for buying or shorting.
\citet{garcia2019continuous} consider a larger state space (8 elements)
but restrict the action set to 3 options.\footnote{Some recent papers
  consider arbitrary weights (e.g., \citet{jiang2017deep} and
  \citet{yu2019model}) for a limited number of assets.} In terms of the
state space, all articles assume that the state of the economy is
determined by prices (or returns).

One strong limitation of these approaches is the marked simplification
they imply. Realistic discretizations are numerically intractable when
investing in multiple assets. Indeed, splitting the unit interval in
\(h\) points yields \(h^{NK}\) possibilities for feature values. The
number of options for weight combinations is exponentially increasing
\(N\). As an example: just 10 possible values for 10 features of 10
stocks yield \(10^{100}\) permutations.

The problems mentioned above are of course not restricted to portfolio
construction. Many solutions have been proposed to solve Markov Decision
Processes in continuous spaces. We refer for instance to Section 4 in
\citet{powell2011review} for a review of early methods (outside
finance).

This curse of dimensionality is accompanied by fundamental question of
training data. Two options are conceivable: market data or simulations.
Under a given controlled generator of samples, it is hard to imagine
that the algorithm will beat the solution that maximizes a given utility
function (if anything, it should converge towards the static optimal
solution). This leaves market data as a preferred solution but even with
large datasets, there is little chance to cover all the (actions,
states) combinations mentioned above. Datasets have depths that run
through a few decades of monthly data, which means several hundreds of
time-stamps at most. This is by far too limited to allow for a reliable
learning process. It is always possible to generate synthetic data (as
in \citet{yu2019model}), but it is unclear that this will solidly
improve the performance of the algorithm.

\hypertarget{policy-gradient}{%
\subsection{Policy gradient}\label{policy-gradient}}

Beyond the discretization of action and state spaces, a powerful trick
is \textbf{parametrization}. When \(a\) and \(s\) can take discrete
values, action-value functions must be computed for all pairs \((a,s)\),
which can be prohibitively cumbersome. An elegant way to circumvent this
problem is to assume that the policy is driven by a relatively modest
number of parameters. The learning process is then focused on optimizing
this set of parameters \(\boldsymbol{\theta}\). We then write
\(\pi_{\boldsymbol{\theta}}(a,s)\) for the probability of choosing
action \(a\) in state \(s\). One intuitive way to define
\(\pi_{\boldsymbol{\theta}}(a,s)\) is to resort to a soft-max form:
\begin{equation}
\label{eq:policyex}
\pi_{\boldsymbol{\theta}}(a,s) = \frac{e^{\boldsymbol{\theta}'\textbf{h}(a,s)}}{\sum_{b}e^{\boldsymbol{\theta}'\textbf{h}(b,s)}},
\end{equation} where the output of function \(\textbf{h}(a,s)\), which
has the same dimension as \(\boldsymbol{\theta}\) is called a feature
vector representing the pair \((a,s)\). Typically, \(\textbf{h}\) can
very well be a simple neural network with two input units and an output
dimension equal to the length of \(\boldsymbol{\theta}\).

One desired property for \(\pi_{\boldsymbol{\theta}}\) is that it be
differentiable with respect to \(\boldsymbol{\theta}\) so that
\(\boldsymbol{\theta}\) can be improved via some gradient method. The
most simple and intuitive results about policy gradients are known in
the case of episodic tasks (finite horizon) for which it is sought to
maximize the average gain \(\mathbb{E}_{\boldsymbol{\theta}}[G_t]\)
where the gain is defined in Equation \eqref{eq:gain6}. The expectation is
computed according to a particular policy that depends on
\(\boldsymbol{\theta}\), this is why we use a simple subscript. One
central result is the so-called policy gradient theorem which states
that

\begin{equation}
\label{eq:PGT}
\nabla \mathbb{E}_{\boldsymbol{\theta}}[G_t]=\mathbb{E}_{\boldsymbol{\theta}} \left[G_t\frac{\nabla \pi_{\boldsymbol{\theta}}}{\pi_{\boldsymbol{\theta}}} \right].
\end{equation}

This result can then be used for gradient ascent: when seeking to
maximize a quantity, the parameter change must go in the upward
direction:

\begin{equation}
\label{eq:ascent}
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} + \eta \nabla \mathbb{E}_{\boldsymbol{\theta}}[G_t].
\end{equation}

This simple update rule is known as the REINFORCE algorithm. Many
extensions of this simple idea exist (adding baseline, actor-critic
methods, etc) and we refer to Chapter 13 of
\citet{sutton2018reinforcement} for a detailed account on this topic.

One interesting application of parametric policies is outlined in
\citet{aboussalah2020continuous}. In their article, the authors define a
trading policy that is based on a recurrent neural network. Thus, the
parameter \(\boldsymbol{\theta}\) in this case encompasses all weights
and biases in the network.

Another favorable feature of parametric policies is that they are
compatible with continuous sets of actions. Beyond the form
\eqref{eq:policyex}, there are other ways to shape
\(\pi_{\boldsymbol{\theta}}\). If \(\mathcal{A}\) is a subset of
\(\mathbb{R}\), and \(f_{\boldsymbol{\Omega}}\) is a density function
with parameters \(\boldsymbol{\Omega}\), then a candidate form for
\(\pi_{\boldsymbol{\theta}}\) is

\begin{equation}
\label{eq:parpol}
\pi_{\boldsymbol{\theta}} = f_{\boldsymbol{\Omega}(s,\boldsymbol{\theta})}(a),
\end{equation} in which the parameters \(\boldsymbol{\Omega}\) are in
turn functions of the states and of the underlying (second order)
parameters \(\boldsymbol{\theta}\).

While the Gaussian distribution (see Section 13.7 in
\citet{sutton2018reinforcement}) are often a preferred choice, they
would require some processing to lie inside the unit interval. One easy
way to obtain such values is to apply the normal cumulative distribution
function to the output. In \citet{wang2019continuous}, the multivariate
Gaussian policy is theoretically explored, but it assumes no constraint
on weights.

Some natural parametric distributions emerge as alternatives. If only
one asset is traded, then the Bernoulli distribution can be used to
determine whether or not to buy the asset. If a riskless asset is
available, the beta distribution offers more flexibility because the
values for the proportion invested in the risky asset span the whole
interval; the remainder can be invested into the safe asset. When many
asset are traded, things become more complicated because of the budget
constraint. One ideal candidate is the Dirichlet distribution because it
is defined on a simplex (see Equation (\eqref{eq:simplex})):
\[f_{\boldsymbol{\alpha}}(x_1,\dots,x_n)=\frac{1}{B(\boldsymbol{\alpha})}\prod_{i=1}^nx_i^{\alpha_i-1},\]
where \(B(\boldsymbol{\alpha})\) is a normalizing constant depending on
\(n\) strictly positive parameters \(\alpha_1, \dots, \alpha_n\).

\hypertarget{simple-examples}{%
\section{Simple examples}\label{simple-examples}}

\hypertarget{q-learning-with-simulations}{%
\subsection{Q-learning with
simulations}\label{q-learning-with-simulations}}

To illustrate the gist of the problems mentioned above, we propose two
implementations of \(Q\)-learning. For simplicity, the first one is
based on simulations. This helps understand the learning process in a
simplified framework. We consider two assets: one risky and one
riskless, with return equal to zero. The returns for the risky process
follow an autoregressive model of order one (AR(1)):
\(r_{t+1}=a+\rho r_t+\epsilon_{t+1}\) with \(|\rho|<1\) and \(\epsilon\)
following a standard white noise with variance \(\sigma^2\).

The environment consists only in observing the past return \(r_t\).
Since we seek to estimate the \(Q\) function, we need to discretize this
state variable. The simplest choice is to resort to a binary variable:
equal to -1 (negative) if \(r_t<0\) and to +1 (positive) if
\(r_t\ge 0\). The actions are summarized by the quantity invested in the
risky asset. It can take 5 values: 0 (risk-free portfolio), 0.25, 0.5,
0.75 and 1 (fully invested in the risky asset). This is for instance the
same choice as in \citet{pendharkar2018trading}.

The landscape of R libraries for RL is surprisingly sparse. We resort to
the package \emph{ReinforcementLearning} which has an intuitive
implementation of \(Q\)-learning (another option would be the
\emph{reinforcelearn} package). It requires a dataset with the usual
inputs: state, action, reward and subsequent state. We start by
simulating the returns: they drive the states and the rewards (portfolio
returns). The actions are sampled randomly. Technically, the main
function of the package requires that states and actions be of character
type. The data is built in the chunk below.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ReinforcementLearning)                              }\CommentTok{# Package for RL}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)                                                }\CommentTok{# Fixing the random seed}
\NormalTok{n_sample <-}\StringTok{ }\DecValTok{10}\OperatorTok{^}\DecValTok{5}                                            \CommentTok{# Number of samples to be generated}
\NormalTok{rho <-}\StringTok{ }\FloatTok{0.3}                                                  \CommentTok{# Autoregressive parameter}
\NormalTok{sd <-}\StringTok{ }\FloatTok{0.4}                                                   \CommentTok{# Std. dev. of noise}
\NormalTok{a <-}\StringTok{ }\FloatTok{0.06} \OperatorTok{*}\StringTok{ }\NormalTok{rho                                             }\CommentTok{# Scaled mean of returns}
\NormalTok{data_RL <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}\DataTypeTok{returns =}\NormalTok{ a}\OperatorTok{/}\NormalTok{rho }\OperatorTok{+}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\DataTypeTok{n =}\NormalTok{ n_sample, }\CommentTok{# Returns via AR(1) simulation}
                                      \KeywordTok{list}\NormalTok{(}\DataTypeTok{ar =}\NormalTok{ rho),       }
                                      \DataTypeTok{sd =}\NormalTok{ sd),}
                  \DataTypeTok{action =} \KeywordTok{round}\NormalTok{(}\KeywordTok{runif}\NormalTok{(n_sample)}\OperatorTok{*}\DecValTok{4}\NormalTok{)}\OperatorTok{/}\DecValTok{4}\NormalTok{) }\OperatorTok{%>%}\StringTok{  }\CommentTok{# Random action (portfolio)}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{state =} \KeywordTok{if_else}\NormalTok{(returns }\OperatorTok{<}\StringTok{ }\DecValTok{0}\NormalTok{, }\StringTok{"neg"}\NormalTok{, }\StringTok{"pos"}\NormalTok{),      }\CommentTok{# Coding of state}
           \DataTypeTok{reward =}\NormalTok{ returns }\OperatorTok{*}\StringTok{ }\NormalTok{action,                       }\CommentTok{# Reward = portfolio return}
           \DataTypeTok{new_state =} \KeywordTok{lead}\NormalTok{(state),                         }\CommentTok{# Next state}
           \DataTypeTok{action =} \KeywordTok{as.character}\NormalTok{(action)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{    }\KeywordTok{na.omit}\NormalTok{()                                               }\CommentTok{# Remove one missing state}
\NormalTok{data_RL }\OperatorTok{%>%}\StringTok{ }\KeywordTok{head}\NormalTok{()                                          }\CommentTok{# Show first lines}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 5
##   returns action state reward new_state
##     <dbl> <chr>  <chr>  <dbl> <chr>    
## 1   0.674 0      pos   0      pos      
## 2   0.206 0.25   pos   0.0516 pos      
## 3   0.911 0.25   pos   0.228  pos      
## 4   0.290 0      pos   0      pos      
## 5   0.651 0.25   pos   0.163  pos      
## 6   1.15  0.25   pos   0.288  neg
\end{verbatim}

\normalsize

There are 3 parameters in the implementation of the \emph{Q}-learning
algorithm:

\begin{itemize}
\tightlist
\item
  \(\gamma\), which is the learning rate in the updating Equation
  \eqref{eq:QLupdate}. In \emph{ReinforcementLearning}, this is coded as
  \emph{alpha};\\
\item
  \(\gamma\), the discounting rate for the rewards (also shown in
  Equation \eqref{eq:QLupdate});
\item
  and \(\epsilon\), which controls the rate of exploration versus
  exploitation (see Equation \eqref{eq:egreedy}).
\end{itemize}

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{control <-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{0.1}\NormalTok{,                       }\CommentTok{# Learning rate}
                \DataTypeTok{gamma =} \FloatTok{0.7}\NormalTok{,                       }\CommentTok{# Discount factor for rewards}
                \DataTypeTok{epsilon =} \FloatTok{0.1}\NormalTok{)                     }\CommentTok{# Exploration rate}

\NormalTok{fit_RL <-}\StringTok{ }\KeywordTok{ReinforcementLearning}\NormalTok{(data_RL,           }\CommentTok{# Main RL function}
                               \DataTypeTok{s =} \StringTok{"state"}\NormalTok{, }
                               \DataTypeTok{a =} \StringTok{"action"}\NormalTok{, }
                               \DataTypeTok{r =} \StringTok{"reward"}\NormalTok{, }
                               \DataTypeTok{s_new =} \StringTok{"new_state"}\NormalTok{, }
                               \DataTypeTok{control =}\NormalTok{ control)}
\KeywordTok{print}\NormalTok{(fit_RL)   }\CommentTok{# Show the output}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## State-Action function Q
##          0.25         0         1      0.75       0.5
## neg 0.4704760 0.4945049 0.2073319 0.3247837 0.3099006
## pos 0.6814258 0.5432345 1.0100722 0.9102886 0.7287897
## 
## Policy
## neg pos 
## "0" "1" 
## 
## Reward (last iteration)
## [1] 2877.216
\end{verbatim}

\normalsize

The output shows the \emph{Q} function, which depends naturally both on
states and actions. When the state is negative, large risky positions
(action equal to 0.75 or 1.00) are associated with negative average
rewards whereas small positions yield positive average rewards. When the
state is positive, the average rewards are the highest for the largest
allocations. Thus, the recommendation of the algorithm (i.e., the
policy) is to be fully invested in a postive state and to refrain from
investing in a negative state. Given the positive autocorrelation of the
underlying process, this does make sense.

Basically, the algorithm has simply learned that positive (\emph{resp.}
negative) returns are more likely to follow positive (\emph{resp}.
negative) returns. While this is somewhat reassuring, it is by no means
impressive and much simpler tools would yield similar conclusions and
guidance.

\hypertarget{q-learning-with-market-data}{%
\subsection{Q-learning with market
data}\label{q-learning-with-market-data}}

The second application is based on the financial dataset. To reduce the
dimensionality of the problem, we will assume:\\
- that only one feature (price-to-book ratio) captures the state of the
environment;\\
- that actions take values over a discrete set consisting of three
positions: +1 (buy the market), -1 (sell the market) and 0 (hold no
risky positions);\\
- that only two assets are traded: those with stock\_id equal to 1 and
3.

The construction of the dataset is unelegantly coded below.

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{return_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\NormalTok{data_ml }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(stock_id }\OperatorTok{==}\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{pull}\NormalTok{(R1M_Usd)      }\CommentTok{# Return of asset 1}
\NormalTok{return_}\DecValTok{3}\NormalTok{ <-}\StringTok{ }\NormalTok{data_ml }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(stock_id }\OperatorTok{==}\DecValTok{3}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{pull}\NormalTok{(R1M_Usd)      }\CommentTok{# Return of asset 3}
\NormalTok{pb_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\NormalTok{data_ml }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(stock_id }\OperatorTok{==}\DecValTok{1}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{pull}\NormalTok{(Pb)               }\CommentTok{# P/B ratio of asset 1}
\NormalTok{pb_}\DecValTok{3}\NormalTok{ <-}\StringTok{ }\NormalTok{data_ml }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(stock_id }\OperatorTok{==}\DecValTok{3}\NormalTok{) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{pull}\NormalTok{(Pb)               }\CommentTok{# P/B ratio of asset 3}
\NormalTok{action_}\DecValTok{1}\NormalTok{ <-}\StringTok{ }\KeywordTok{floor}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\KeywordTok{length}\NormalTok{(pb_}\DecValTok{1}\NormalTok{))}\OperatorTok{*}\DecValTok{3}\NormalTok{) }\OperatorTok{-}\StringTok{ }\DecValTok{1}                        \CommentTok{# Action for asset 1 (random)}
\NormalTok{action_}\DecValTok{3}\NormalTok{ <-}\StringTok{ }\KeywordTok{floor}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\KeywordTok{length}\NormalTok{(pb_}\DecValTok{1}\NormalTok{))}\OperatorTok{*}\DecValTok{3}\NormalTok{) }\OperatorTok{-}\StringTok{ }\DecValTok{1}                        \CommentTok{# Action for asset 3 (random)}
\NormalTok{RL_data <-}\StringTok{ }\KeywordTok{tibble}\NormalTok{(return_}\DecValTok{1}\NormalTok{, return_}\DecValTok{3}\NormalTok{,                               }\CommentTok{# Building the dataset}
\NormalTok{                  pb_}\DecValTok{1}\NormalTok{, pb_}\DecValTok{3}\NormalTok{,}
\NormalTok{                  action_}\DecValTok{1}\NormalTok{, action_}\DecValTok{3}\NormalTok{) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{action =} \KeywordTok{paste}\NormalTok{(action_}\DecValTok{1}\NormalTok{, action_}\DecValTok{3}\NormalTok{),                      }\CommentTok{# Uniting actions}
           \DataTypeTok{pb_1 =} \KeywordTok{round}\NormalTok{(}\DecValTok{5} \OperatorTok{*}\StringTok{ }\NormalTok{pb_}\DecValTok{1}\NormalTok{),                                  }\CommentTok{# Simplifying states}
           \DataTypeTok{pb_3 =} \KeywordTok{round}\NormalTok{(}\DecValTok{5} \OperatorTok{*}\StringTok{ }\NormalTok{pb_}\DecValTok{3}\NormalTok{),                                  }\CommentTok{# Simplifying states}
           \DataTypeTok{state =} \KeywordTok{paste}\NormalTok{(pb_}\DecValTok{1}\NormalTok{, pb_}\DecValTok{3}\NormalTok{),                               }\CommentTok{# Uniting states}
           \DataTypeTok{reward =}\NormalTok{ action_}\DecValTok{1}\OperatorTok{*}\NormalTok{return_}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{action_}\DecValTok{3}\OperatorTok{*}\NormalTok{return_}\DecValTok{3}\NormalTok{,          }\CommentTok{# Computing rewards}
           \DataTypeTok{new_state =} \KeywordTok{lead}\NormalTok{(state)) }\OperatorTok{%>%}\StringTok{                             }\CommentTok{# Infer new state}
\StringTok{    }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{pb_}\DecValTok{1}\NormalTok{, }\OperatorTok{-}\NormalTok{pb_}\DecValTok{3}\NormalTok{, }\OperatorTok{-}\NormalTok{action_}\DecValTok{1}\NormalTok{, }\OperatorTok{-}\NormalTok{action_}\DecValTok{3}\NormalTok{, }\OperatorTok{-}\NormalTok{return_}\DecValTok{1}\NormalTok{, }\OperatorTok{-}\NormalTok{return_}\DecValTok{3}\NormalTok{) }\CommentTok{# Remove superfluous vars.}
\KeywordTok{head}\NormalTok{(RL_data)                                                       }\CommentTok{# Showing the result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 4
##   action state reward new_state
##   <chr>  <chr>  <dbl> <chr>    
## 1 0 -1   1 1   -0.077 1 1      
## 2 -1 -1  1 1   -0.239 1 1      
## 3 0 0    1 1    0     1 1      
## 4 0 -1   1 1   -0.027 1 1      
## 5 -1 -1  1 1    0.047 1 1      
## 6 1 1    1 1   -0.149 1 1
\end{verbatim}

\normalsize

Actions and states have to be merged to yield all possible combinations.
To simplify the states, we round 5 times the price-to-book ratios.

We keep the same hyperparameters as in the previous example. Columns
below stand for actions: the first (\(resp.\) second) number notes the
position in the first (\(resp.\) second) asset. The rows correspond to
states. The scaled P/R ratios are separated by a point (e.g., ``X2.3''
means that the first (\(resp.\) second) asset has a scale P/B of 2
(\(resp.\) 3).

\footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit_RL2 <-}\StringTok{ }\KeywordTok{ReinforcementLearning}\NormalTok{(RL_data,           }\CommentTok{# Main RL function}
                               \DataTypeTok{s =} \StringTok{"state"}\NormalTok{, }
                               \DataTypeTok{a =} \StringTok{"action"}\NormalTok{, }
                               \DataTypeTok{r =} \StringTok{"reward"}\NormalTok{, }
                               \DataTypeTok{s_new =} \StringTok{"new_state"}\NormalTok{, }
                               \DataTypeTok{control =}\NormalTok{ control)}
\NormalTok{fit_RL2}\OperatorTok{$}\NormalTok{Q <-}\StringTok{ }\KeywordTok{round}\NormalTok{(fit_RL2}\OperatorTok{$}\NormalTok{Q, }\DecValTok{3}\NormalTok{) }\CommentTok{# Round the Q-matrix}
\KeywordTok{print}\NormalTok{(fit_RL2)                   }\CommentTok{# Show the output}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## State-Action function Q
##        0 0    0 1   0 -1  -1 -1   -1 0   -1 1   1 -1    1 0    1 1
## X0.0 0.000  0.000  0.000 -0.026  0.000  0.000  0.017  0.000  0.000
## X0.1 0.000  0.000 -0.058  0.000  0.009  0.000  0.000  0.000  0.000
## X4.1 0.000  0.002 -0.005  0.008  0.000  0.000  0.000 -0.006 -0.008
## X3.1 0.021 -0.005  0.017  0.017  0.003  0.049 -0.013  0.030 -0.007
## X3.2 0.001  0.020  0.013  0.010  0.041  0.015 -0.002  0.000  0.000
## X2.0 0.000  0.000 -0.017  0.000  0.000 -0.013  0.000  0.000  0.000
## X2.1 0.004 -0.019 -0.002 -0.037 -0.017 -0.024  0.020  0.002 -0.011
## X2.2 0.002 -0.002  0.005  0.034  0.010 -0.006  0.020  0.000 -0.020
## X2.3 0.000  0.000  0.005  0.000  0.000  0.000 -0.007  0.013  0.000
## X1.1 0.004 -0.003 -0.012 -0.033  0.015 -0.006  0.017  0.037  0.014
## X1.2 0.000  0.009 -0.008  0.000 -0.016  0.000  0.021  0.015  0.021
## 
## Policy
##    X0.0    X0.1    X4.1    X3.1    X3.2    X2.0    X2.1    X2.2    X2.3 
##  "1 -1"  "-1 0" "-1 -1"  "-1 1"  "-1 0"   "0 0"  "1 -1" "-1 -1"   "1 0" 
##    X1.1    X1.2 
##   "1 0"  "1 -1" 
## 
## Reward (last iteration)
## [1] -1.362
\end{verbatim}

\normalsize

The output show that there are many combinations of states and actions
that are not spanned by the data. Some states seem to be more often
represented (``X1.1'', ``X2.1'' and X3.1``). It is hard to make any
sense of the recommendations. The first two states (''X0.0" and
``X0.1'') are close but the decisions related to them are very different
(buy and short versus short and hold). Moreover, there is no coherence
and no monotonicity in actions with respect to individual state values:
low values of states can be associated to very different actions. The
only seemingly robust pattern is the advice to sell the first asset when
its scaled P/B ratio is high (values of 3 or 4).

One reason why these conclusion do not appear trustworthy pertains to
the data size. With only 200+ time points and 99 state-action pairs (11
times 9), this yields on average only two data points to compute the
\(Q\) function. This could be improved by testing more random actions,
but the limits of the sample size would eventually (rapidly) be reached
anyway.

\hypertarget{concluding-remarks}{%
\section{Concluding remarks}\label{concluding-remarks}}

Reinforcement learning has been applied to financial problems for a long
time. Early contributions in the late 1990s include
\citet{neuneier1996optimal}, \citet{moody1997optimization},
\citet{moody1998performance} and \citet{neuneier1998enhancing}. Since
then, many researchers in the computer science field have sought to
apply RL techniques to portfolio problems. The advent of massive
datasets and the increase in dimensionality make it hard for RL tools to
adapt well to very rich environments that are encountered in factor
investing.

Recently, some approaches seek to adapt RL to continuous action spaces
(\citet{wang2019continuous}, \citet{aboussalah2020continuous}) but not
to high-dimensional state spaces. These spaces are those required in
factor investing because all firms yields hundreds of data points
characterizing their economic situation. In addition, applications of RL
in financial frameworks have a particularity compared to many typical RL
tasks: in financial markets, actions of agents have no impact on the
environment (unless the agent is able to perform massive trades, which
is rare and ill-advised because it pushes prices in the wrong
direction). This lack of impact of actions may possibly mitigate the
efficiency of traditional RL approaches.

Those are challenges that will need to be solved in order for RL to
become competitive with alternative (supervised) methods. We end this
chapter by underlining that reinforcement learning has also been used to
estimate complex theoretical models (\citet{halperin2018market},
\citet{garcia2019continuous}). The research in the field is incredibly
diversified and is orientated towards many directions. It is likely that
captivating work will be published in the near future.

\hypertarget{exercises}{%
\section{Exercises}\label{exercises}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Test what happens if the process for generating returns has a negative
  autocorrelation. What is the impact on the \(Q\) function and the
  policy?
\item
  Increase the size of RL\_data by testing all possible action
  combination for each original data point. Re-run the \(Q\)-learning
  function and see what happens.
\end{enumerate}

\hypertarget{NLP}{%
\chapter{Natural Language Processing}\label{NLP}}

\citet{loughran2016textual}

\citet{gentzkow2019text}

\citet{cong2019analyzing}

\citet{cong2019textual}

\hypertarget{conclusion}{%
\chapter{Conclusion}\label{conclusion}}

\hypertarget{appendix-appendix}{%
\appendix \addcontentsline{toc}{chapter}{\appendixname}}


\hypertarget{data-description}{%
\chapter{Data Description}\label{data-description}}

\begin{table}
\begin{center}
\begin{tabular}{l l}
\textbf{Column Name}    &   \textbf{Short Description}  \\ \hline
stock\_id   &   security id \\
date    &   date of the data    \\
Advt\_12M\_Usd  &   average daily volume in amount in USD over 12 months    \\
Advt\_3M\_Usd   &   average daily volume in amount in USD over 3 months \\
Advt\_6M\_Usd   &   average daily volume in amount in USD over 6 months \\
Asset\_Turnover &   total sales on average assets   \\
Bb\_Yld &   buyback yield   \\
Bv  &   book value  \\
Capex\_Ps\_Cf   &   capital expenditure on price to sale cash flow  \\
Capex\_Sales    &   capital expenditure on sales    \\
Cash\_Div\_Cf   &   cash dividends cash flow    \\
Cash\_Per\_Share    &   cash per share  \\
Cf\_Sales   &   cash flow per share \\
Debtequity  &   debt to equity  \\
Div\_Yld    &   dividend yield  \\
Dps &   dividend per share  \\
Ebit\_Bv    &   EBIT on book value  \\
Ebit\_Noa   &   EBIT on non operating asset \\
Ebit\_Oa    &   EBIT on operating asset \\
Ebit\_Ta    &   EBIT on total asset \\
Ebitda\_Margin  &   EBITDA margin   \\
Eps &   earnings per share  \\
Eps\_Basic  &   earnings per share basic    \\
Eps\_Basic\_Gr  &   earnings per share growth   \\
Eps\_Contin\_Oper   &   earnings per share continuing operations    \\
Eps\_Dil    &   earnings per share diluted  \\
Ev  &   enterprise value    \\
Ev\_Ebitda  &   enterprise value on EBITDA  \\
Fa\_Ci  &   fixed assets on common equity   \\
Fcf &   free cash flow  \\
Fcf\_Bv &   free cash flow on book value    \\
Fcf\_Ce &   free cash flow on capital employed  \\ \hline
\end{tabular}
\end{center}
\caption{Description of features in the dataset. \label{tab:appendix1}}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{l l}
\textbf{Column Name}    &   \textbf{Short Description}  \\ \hline
Fcf\_Margin &   free cash flow margin   \\
Fcf\_Noa    &   free cash flow on net operating assets  \\
Fcf\_Oa &   free cash flow on operating assets  \\
Fcf\_Ta &   free cash flow on total assets  \\
Fcf\_Tbv    &   free cash flow on tangible book value   \\
Fcf\_Toa    &   free cash flow on total operating assets    \\
Fcf\_Yld    &   free cash flow yield    \\
Free\_Ps\_Cf    &   free cash flow on price sales   \\
Int\_Rev    &   intangibles on revenues \\
Interest\_Expense   &   interest expense coverage   \\
Mkt\_Cap\_12M\_Usd  &   average market capitalisation over 12 months in USD \\
Mkt\_Cap\_3M\_Usd   &   average market capitalisation over 3 months in USD  \\
Mkt\_Cap\_6M\_Usd   &   average market capitalisation over 6 months in USD  \\
Mom\_11M\_Usd   &   price momentum  12 - 1 months in USD    \\
Mom\_5M\_Usd    &   price momentum  6 - 1 months in USD \\
Mom\_Sharp\_11M\_Usd    &   price momentum  12 - 1 months in USD / divided by volatility    \\ 
Mom\_Sharp\_5M\_Usd &   price momentum  6 - 1 months in USD / divided by volatility \\
Nd\_Ebitda  &   net debt on EBITDA  \\
Net\_Debt   &   net debt    \\
Net\_Debt\_Cf   &   net debt on cash flow   \\
Net\_Margin &   net margin  \\
Netdebtyield    &   net debt yield  \\
Ni  &   net income  \\
Ni\_Avail\_Margin   &   net income available margin \\
Ni\_Oa  &   net income on operating asset   \\
Ni\_Toa &   net income on total operating asset \\
Noa &   net operating asset \\
Oa  &   operating asset \\ \hline
\end{tabular}
\end{center}
\caption{Description of features in the dataset (continued). \label{tab:appendix2}}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{l l}
\textbf{Column Name}    &   \textbf{Short Description}  \\ \hline
Ocf &   operating cash flow \\
Ocf\_Bv &   operating cash flow on book value   \\
Ocf\_Ce &   operating cash flow on capital employed \\
Ocf\_Margin &   operating cash flow margin  \\
Ocf\_Noa    &   operating cash flow on net operating assets \\
Ocf\_Oa &   operating cash flow on operating assets \\
Ocf\_Ta &   operating cash flow on total assets \\
Ocf\_Tbv    &   operating cash flow on tangible book value  \\
Ocf\_Toa    &   operating cash flow on total operating assets   \\
Op\_Margin  &   operating margin    \\
Op\_Prt\_Margin &   net marging 1Y growth   \\
Oper\_Ps\_Net\_Cf   &   cash flow from operations per share net \\
Pb  &   price to book   \\
Pe  &   price earnings  \\
Ptx\_Mgn    &   margin pretax   \\
Recurring\_Earning\_Total\_Assets   &   reccuring earnings on total assets  \\
Return\_On\_Capital &   return on capital   \\
Rev &   revenue \\
Roa &   return on assets    \\
Roc &   return on capital   \\
Roce    &   return on capital employed  \\
Roe &   return on equity    \\
Sales\_Ps   &   price to sales  \\
Share\_Turn\_12M    &   average share turnover 12 months    \\
Share\_Turn\_3M &   average share turnover 3 months \\
Share\_Turn\_6M &   average share turnover 6 months \\
Ta  &   total assets    \\
Tev\_Less\_Mktcap   &   total enterprise value less market capitalisation   \\
Tot\_Debt\_Rev  &   total debt on revenue   \\
Total\_Capital  &   total capital   \\
Total\_Debt &   total debt on revenue   \\
Total\_Debt\_Capital    &   total debt on capital   \\
Total\_Liabilities\_Total\_Assets   &   total liabilities on total assets   \\
Vol1Y\_Usd  &   volatility of returns one year  \\
Vol3Y\_Usd  &   volatility of returns 3 years   \\
R1M\_Usd    &   return forward 1 month  \\
R3M\_Usd    &   return forward 3 months \\
R6M\_Usd    &   return forward 6 months \\
R12M\_Usd   &   return forward 12 months    \\ \hline
\end{tabular}
\end{center}
\caption{Description of features and labels in the dataset. \label{tab:appendix3}}
\end{table}

\hypertarget{solution-to-exercises}{%
\chapter{Solution to exercises}\label{solution-to-exercises}}

\hypertarget{chapter-4}{%
\section{Chapter 4}\label{chapter-4}}

For annual values: \footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data_ml }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(date) }\OperatorTok{%>%}\StringTok{                                            }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{growth =}\NormalTok{ Pb }\OperatorTok{>}\StringTok{ }\KeywordTok{median}\NormalTok{(Pb)) }\OperatorTok{%>%}\StringTok{            }\CommentTok{# Creates the sort}
\StringTok{    }\KeywordTok{ungroup}\NormalTok{() }\OperatorTok{%>%}\StringTok{                                   }\CommentTok{# Ungroup}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{year =}\NormalTok{ lubridate}\OperatorTok{::}\KeywordTok{year}\NormalTok{(date)) }\OperatorTok{%>%}\StringTok{        }\CommentTok{# Creates a year variable}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(year, growth) }\OperatorTok{%>%}\StringTok{                      }\CommentTok{# Analyze by year & sort}
\StringTok{    }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{ret =} \KeywordTok{mean}\NormalTok{(R1M_Usd)) }\OperatorTok{%>%}\StringTok{              }\CommentTok{# Compute average return}
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ year, }\DataTypeTok{y =}\NormalTok{ ret, }\DataTypeTok{fill =}\NormalTok{ growth)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\CommentTok{# Plot!}
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.7}\NormalTok{, }\FloatTok{0.8}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=400px,height=150px]{ML_factor_files/figure-latex/ex41b-1} 

}

\caption{The value factor: annual returns}\label{fig:ex41b}
\end{figure}

\normalsize

For monthly values: \footnotesize

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{returns <-}\StringTok{ }\NormalTok{data_ml }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(date) }\OperatorTok{%>%}\StringTok{                                            }
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{growth =}\NormalTok{ Pb }\OperatorTok{>}\StringTok{ }\KeywordTok{median}\NormalTok{(Pb)) }\OperatorTok{%>%}\StringTok{                           }\CommentTok{# Creates the sort}
\StringTok{    }\KeywordTok{group_by}\NormalTok{(date, growth) }\OperatorTok{%>%}\StringTok{                                     }\CommentTok{# Analyze by date & sort}
\StringTok{    }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{ret =} \KeywordTok{mean}\NormalTok{(R1M_Usd)) }\OperatorTok{%>%}\StringTok{                             }\CommentTok{# Compute average return}
\StringTok{    }\KeywordTok{spread}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ growth, }\DataTypeTok{value =}\NormalTok{ ret) }\OperatorTok{%>%}\StringTok{                          }\CommentTok{# Pivot to wide matrix format}
\StringTok{    }\KeywordTok{ungroup}\NormalTok{()}
\KeywordTok{colnames}\NormalTok{(returns)[}\DecValTok{2}\OperatorTok{:}\DecValTok{3}\NormalTok{] <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"value"}\NormalTok{, }\StringTok{"growth"}\NormalTok{)                     }\CommentTok{# Changing column names}
\NormalTok{returns }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{value =} \KeywordTok{cumprod}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{value),                             }\CommentTok{# From returns to portf. values}
           \DataTypeTok{growth =} \KeywordTok{cumprod}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{growth)) }\OperatorTok{%>%}
\StringTok{    }\KeywordTok{gather}\NormalTok{(}\DataTypeTok{key =}\NormalTok{ portfolio, }\DataTypeTok{value =}\NormalTok{ value, }\OperatorTok{-}\NormalTok{date) }\OperatorTok{%>%}\StringTok{              }\CommentTok{# Back in tidy format}
\StringTok{    }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ date, }\DataTypeTok{y =}\NormalTok{ value, }\DataTypeTok{color =}\NormalTok{ portfolio)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}\StringTok{     }\CommentTok{# Plot!  }
\StringTok{    }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \KeywordTok{c}\NormalTok{(}\FloatTok{0.7}\NormalTok{, }\FloatTok{0.8}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics[width=400px,height=150px]{ML_factor_files/figure-latex/ex41-1} 

}

\caption{The value factor: portfolio values.}\label{fig:ex41}
\end{figure}

\normalsize

\hypertarget{chapter-5}{%
\section{Chapter 5}\label{chapter-5}}

\hypertarget{chapter-6}{%
\section{Chapter 6}\label{chapter-6}}

\hypertarget{chapter-7}{%
\section{Chapter 7}\label{chapter-7}}

\hypertarget{chapter-8}{%
\section{Chapter 8}\label{chapter-8}}

\hypertarget{chapter-9}{%
\section{Chapter 9}\label{chapter-9}}

\hypertarget{chapter-12}{%
\section{Chapter 12}\label{chapter-12}}

\hypertarget{chapter-13}{%
\section{Chapter 13}\label{chapter-13}}

\hypertarget{functional-programming-in-the-backtest}{%
\subsection{Functional programming in the
backtest}\label{functional-programming-in-the-backtest}}

\hypertarget{advanced-weighting-function}{%
\subsection{Advanced weighting
function}\label{advanced-weighting-function}}

\hypertarget{references}{%
\chapter{References}\label{references}}

\bibliography{bib.bib}

\backmatter
\printindex

\end{document}
